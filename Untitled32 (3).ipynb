{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27636fca3a8c461787baff1a349af865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_077922fa4810484e8defa176e64342f9",
              "IPY_MODEL_d80bc3a6f888467bb324f0ec49873368",
              "IPY_MODEL_3fef3da9e5d1429bb1b83dc0d644a1df"
            ],
            "layout": "IPY_MODEL_7b6f536541dc4aeca8739a8a9c20f591"
          }
        },
        "077922fa4810484e8defa176e64342f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e342024e50534b55a3214b4207edcf57",
            "placeholder": "​",
            "style": "IPY_MODEL_40bebe3c8e784541a465d0750e760cf1",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "d80bc3a6f888467bb324f0ec49873368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9fd7844cbed4400bc8b1f076fd4a58d",
            "max": 662513657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6cde0dfcd0843d3905a7e9495817da5",
            "value": 662513657
          }
        },
        "3fef3da9e5d1429bb1b83dc0d644a1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_492df7f033904f20b38d06c4c585b97a",
            "placeholder": "​",
            "style": "IPY_MODEL_6ada7b011ccd49e2a3b6f6e17a074095",
            "value": " 663M/663M [00:00&lt;00:00, 225MB/s]"
          }
        },
        "7b6f536541dc4aeca8739a8a9c20f591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e342024e50534b55a3214b4207edcf57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40bebe3c8e784541a465d0750e760cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9fd7844cbed4400bc8b1f076fd4a58d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6cde0dfcd0843d3905a7e9495817da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "492df7f033904f20b38d06c4c585b97a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ada7b011ccd49e2a3b6f6e17a074095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1311d909060444b8a8a73c0d234b06dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c8e7b06275c4fd9a8cbdc6521264359",
              "IPY_MODEL_247eb04689f145849b61281011ee10d6",
              "IPY_MODEL_681ae69919854e8cb6c94f6bae04af22"
            ],
            "layout": "IPY_MODEL_b8e5d36ceb6a4ae487d0f2707c6544fe"
          }
        },
        "5c8e7b06275c4fd9a8cbdc6521264359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37a07bdf1a534f70a9a7fef0659cf880",
            "placeholder": "​",
            "style": "IPY_MODEL_4c709f707db449059f713b123e262d22",
            "value": "model.safetensors: 100%"
          }
        },
        "247eb04689f145849b61281011ee10d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c22b429f86e468a96e31abdf1f1f6c1",
            "max": 662435448,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a73d0fa1972c4ac8bc9541d4bf5f767f",
            "value": 662435448
          }
        },
        "681ae69919854e8cb6c94f6bae04af22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_215163703bad4934a7a88ea6b4c3d779",
            "placeholder": "​",
            "style": "IPY_MODEL_46c35cfe1fca4e51ab5b95c1a51917f9",
            "value": " 662M/662M [00:04&lt;00:00, 202MB/s]"
          }
        },
        "b8e5d36ceb6a4ae487d0f2707c6544fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37a07bdf1a534f70a9a7fef0659cf880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c709f707db449059f713b123e262d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c22b429f86e468a96e31abdf1f1f6c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a73d0fa1972c4ac8bc9541d4bf5f767f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "215163703bad4934a7a88ea6b4c3d779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46c35cfe1fca4e51ab5b95c1a51917f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72408a7d7d2249af9750dba116cf988b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_101ef3d549554056b207930d46b4dc12",
              "IPY_MODEL_71f9032787014916908a32144a5c4886",
              "IPY_MODEL_d3c3d75af4624fa58f1b33b06c0e7962"
            ],
            "layout": "IPY_MODEL_bf40faae9ec045819040acce8bde91ce"
          }
        },
        "101ef3d549554056b207930d46b4dc12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d20c4d3235d4a22a74f97cf9dcbed05",
            "placeholder": "​",
            "style": "IPY_MODEL_7b6c0ad98d1644f4b92519bf4b8f8ce6",
            "value": "generation_config.json: 100%"
          }
        },
        "71f9032787014916908a32144a5c4886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8d79b0c8aaf4f07a86d54300066ef7c",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3002aa9d8664787b3569fcc2c081ccb",
            "value": 137
          }
        },
        "d3c3d75af4624fa58f1b33b06c0e7962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae0b8cfed0b442e0b4c6c06068892d23",
            "placeholder": "​",
            "style": "IPY_MODEL_6216311ad10d4b3f92fdf4f2ff027f60",
            "value": " 137/137 [00:00&lt;00:00, 9.60kB/s]"
          }
        },
        "bf40faae9ec045819040acce8bde91ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d20c4d3235d4a22a74f97cf9dcbed05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6c0ad98d1644f4b92519bf4b8f8ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8d79b0c8aaf4f07a86d54300066ef7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3002aa9d8664787b3569fcc2c081ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae0b8cfed0b442e0b4c6c06068892d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6216311ad10d4b3f92fdf4f2ff027f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77f471e5fd7740dbb24be36fc5454f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcfda591b71f4bbdbe1b5b42c7249f4e",
              "IPY_MODEL_7029c1cf5f334a23858b3c327ec6d092",
              "IPY_MODEL_dbc088d308f3490fae85fb36f300caf7"
            ],
            "layout": "IPY_MODEL_00a74ba2c8cf4023b39648e9c1c4d8fb"
          }
        },
        "bcfda591b71f4bbdbe1b5b42c7249f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6843d3f27ea84ad8bc484bfb692c57f1",
            "placeholder": "​",
            "style": "IPY_MODEL_5af922a87bf1446499f6ca851c961c6e",
            "value": "config.json: 100%"
          }
        },
        "7029c1cf5f334a23858b3c327ec6d092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03f04dc8f6874d83917465cc54fe5340",
            "max": 693,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aece4236eb144e00a8dbefc08ce40cf4",
            "value": 693
          }
        },
        "dbc088d308f3490fae85fb36f300caf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69fbc99b54914aad83634b4ddc3dcc98",
            "placeholder": "​",
            "style": "IPY_MODEL_5b1343ad068947809b857848aaf9818e",
            "value": " 693/693 [00:00&lt;00:00, 18.9kB/s]"
          }
        },
        "00a74ba2c8cf4023b39648e9c1c4d8fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6843d3f27ea84ad8bc484bfb692c57f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5af922a87bf1446499f6ca851c961c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03f04dc8f6874d83917465cc54fe5340": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aece4236eb144e00a8dbefc08ce40cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69fbc99b54914aad83634b4ddc3dcc98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1343ad068947809b857848aaf9818e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb95701d0d8146ada10dbb3de7cda519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ceeadcf920eb4c51861d8c865affdb60",
              "IPY_MODEL_de294bfbba604d68a739db3b72e73379",
              "IPY_MODEL_14b6964c771646f38f00915a1164df17"
            ],
            "layout": "IPY_MODEL_205bb7b50e0f4c27998a3cbaf4ec1ef8"
          }
        },
        "ceeadcf920eb4c51861d8c865affdb60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_230fc155266440df97d51b63d7e6432e",
            "placeholder": "​",
            "style": "IPY_MODEL_82a6d34c456d4a04b5513f627b4e4e9c",
            "value": "model.safetensors: 100%"
          }
        },
        "de294bfbba604d68a739db3b72e73379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_929090e773f8432ab9af59420235c08b",
            "max": 1118459525,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4221dd8b0bed410ba8c88123d25b0dc5",
            "value": 1118459525
          }
        },
        "14b6964c771646f38f00915a1164df17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f4fbf0e14d54867bda9f49ef8cc2d2b",
            "placeholder": "​",
            "style": "IPY_MODEL_d490dc4551684169beced0f6fbfebe97",
            "value": " 1.12G/1.12G [00:05&lt;00:00, 246MB/s]"
          }
        },
        "205bb7b50e0f4c27998a3cbaf4ec1ef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "230fc155266440df97d51b63d7e6432e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82a6d34c456d4a04b5513f627b4e4e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "929090e773f8432ab9af59420235c08b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4221dd8b0bed410ba8c88123d25b0dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f4fbf0e14d54867bda9f49ef8cc2d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d490dc4551684169beced0f6fbfebe97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afc08189e4904e6eb82480ac14bb26b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5a272c9c0434460a31d3a954b78c926",
              "IPY_MODEL_1baad00fa283443dbb7999f8b8796784",
              "IPY_MODEL_4ebd8c0f24cb4837b54ecde14baf2c6f"
            ],
            "layout": "IPY_MODEL_79e763ca59bb42f2b55a16076b2e60ae"
          }
        },
        "e5a272c9c0434460a31d3a954b78c926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f51a040331c4485ca93aebf998ba3a97",
            "placeholder": "​",
            "style": "IPY_MODEL_bd96d89c4145420da99a154c6c3a1bda",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1baad00fa283443dbb7999f8b8796784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13c457d7efd64aad81bbadd1bb1e9199",
            "max": 746,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbb5c90f8c1549cabd5f14d8700840e5",
            "value": 746
          }
        },
        "4ebd8c0f24cb4837b54ecde14baf2c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b669a91a4a543e1a2de3292ea1bcf26",
            "placeholder": "​",
            "style": "IPY_MODEL_32d81b3df6cb4b529564b710b8ab9935",
            "value": " 746/746 [00:00&lt;00:00, 42.2kB/s]"
          }
        },
        "79e763ca59bb42f2b55a16076b2e60ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f51a040331c4485ca93aebf998ba3a97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd96d89c4145420da99a154c6c3a1bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13c457d7efd64aad81bbadd1bb1e9199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbb5c90f8c1549cabd5f14d8700840e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b669a91a4a543e1a2de3292ea1bcf26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32d81b3df6cb4b529564b710b8ab9935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a19dfed298040a4a2af082fbd369d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb1488be0ed54e3ead3153b7ae9e13f7",
              "IPY_MODEL_2837c17c40894a4e85300d06c1b86f2f",
              "IPY_MODEL_61c824fe12f045099d28440112213175"
            ],
            "layout": "IPY_MODEL_8a741a14bbd548939f748e725ad74941"
          }
        },
        "fb1488be0ed54e3ead3153b7ae9e13f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c502e0ad277041f784602b65cffdab52",
            "placeholder": "​",
            "style": "IPY_MODEL_ffcbaf5b39a44867ad92e22dd4283cb4",
            "value": "tokenizer.model: 100%"
          }
        },
        "2837c17c40894a4e85300d06c1b86f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45e0c8a21601414f8c99a8684f9a6a89",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d1540c67a1b486f93647fb8b3631ca8",
            "value": 499723
          }
        },
        "61c824fe12f045099d28440112213175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef8cbc35e78343528538be1e3c7b7e3a",
            "placeholder": "​",
            "style": "IPY_MODEL_d3ee237a93704a13bd7e50c7aa2e485d",
            "value": " 500k/500k [00:00&lt;00:00, 14.7MB/s]"
          }
        },
        "8a741a14bbd548939f748e725ad74941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c502e0ad277041f784602b65cffdab52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffcbaf5b39a44867ad92e22dd4283cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45e0c8a21601414f8c99a8684f9a6a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d1540c67a1b486f93647fb8b3631ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef8cbc35e78343528538be1e3c7b7e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3ee237a93704a13bd7e50c7aa2e485d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e13d2d63d984fbea8e5e3dce5186b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48015d00bc794fabb1a758c832df4773",
              "IPY_MODEL_a5ac73c2a4044d2cb9d04db55c53e346",
              "IPY_MODEL_4cf3596f40ab483d970e4b192ce6e374"
            ],
            "layout": "IPY_MODEL_b63f74ed4b7c40c285b76bf452c295fd"
          }
        },
        "48015d00bc794fabb1a758c832df4773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59b287036dd04171b87e5d88073850a9",
            "placeholder": "​",
            "style": "IPY_MODEL_6e5d3a6eaec442f7afe407bc70600cde",
            "value": "tokenizer.json: 100%"
          }
        },
        "a5ac73c2a4044d2cb9d04db55c53e346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f79345d097134fab95ac0bf98be3b728",
            "max": 1842764,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2e0fdc9e7844ed0a32af4ba17473de2",
            "value": 1842764
          }
        },
        "4cf3596f40ab483d970e4b192ce6e374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e2e1e0c4e6f4a5fadca406413918650",
            "placeholder": "​",
            "style": "IPY_MODEL_2e313602b5a34674bd019e6f34829904",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 11.6MB/s]"
          }
        },
        "b63f74ed4b7c40c285b76bf452c295fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59b287036dd04171b87e5d88073850a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e5d3a6eaec442f7afe407bc70600cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f79345d097134fab95ac0bf98be3b728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2e0fdc9e7844ed0a32af4ba17473de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e2e1e0c4e6f4a5fadca406413918650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e313602b5a34674bd019e6f34829904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21b94fa823e645f49f9af64f70ebf4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f51ffffcc83944518310f94bbfb27781",
              "IPY_MODEL_3a0cdb50137a400085e7cde7ad5b8acf",
              "IPY_MODEL_10b5bb4a56eb4f15a5bdbcb4c336c015"
            ],
            "layout": "IPY_MODEL_0aec4b5a6f6b4c159ae337bd4fceb224"
          }
        },
        "f51ffffcc83944518310f94bbfb27781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_812f21e5c1ce41b485c7ba0c26254663",
            "placeholder": "​",
            "style": "IPY_MODEL_d35315dac2ff4066a4325f2ffbce1646",
            "value": "added_tokens.json: 100%"
          }
        },
        "3a0cdb50137a400085e7cde7ad5b8acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63b2dbf07b294bc99bc48748cf754290",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d9c1da89c074d6a999b74b7ec653084",
            "value": 21
          }
        },
        "10b5bb4a56eb4f15a5bdbcb4c336c015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6ce7b3bd4e4031aba0aafcc41d8bbe",
            "placeholder": "​",
            "style": "IPY_MODEL_92cf9e86450a4fe3bd4e8bc46a3537fe",
            "value": " 21.0/21.0 [00:00&lt;00:00, 1.21kB/s]"
          }
        },
        "0aec4b5a6f6b4c159ae337bd4fceb224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "812f21e5c1ce41b485c7ba0c26254663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d35315dac2ff4066a4325f2ffbce1646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63b2dbf07b294bc99bc48748cf754290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9c1da89c074d6a999b74b7ec653084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e6ce7b3bd4e4031aba0aafcc41d8bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92cf9e86450a4fe3bd4e8bc46a3537fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e636cd4ef8b94ea9b2690945b242a016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a3a6058c0d847048ddaba625274f8d2",
              "IPY_MODEL_c88af462d6974988b59dc86420abc414",
              "IPY_MODEL_dca1f2afdb854ff0ae0f4dc5f290e7b6"
            ],
            "layout": "IPY_MODEL_a3687a9268894df9a8d5a4b95e39c8c8"
          }
        },
        "5a3a6058c0d847048ddaba625274f8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67362ba128214f3a817f05de200f7d25",
            "placeholder": "​",
            "style": "IPY_MODEL_f39237b55ac34c26a4c2e0c3932b11a0",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "c88af462d6974988b59dc86420abc414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cc8677593f84768a8a2906506269656",
            "max": 435,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1583f122a07349c293acef8eefce8b5b",
            "value": 435
          }
        },
        "dca1f2afdb854ff0ae0f4dc5f290e7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c0be8257f44ebab74edd27e57e1c5d",
            "placeholder": "​",
            "style": "IPY_MODEL_1079b5b793b34c36b5208dbae1ff7ee8",
            "value": " 435/435 [00:00&lt;00:00, 36.0kB/s]"
          }
        },
        "a3687a9268894df9a8d5a4b95e39c8c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67362ba128214f3a817f05de200f7d25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f39237b55ac34c26a4c2e0c3932b11a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cc8677593f84768a8a2906506269656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1583f122a07349c293acef8eefce8b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1c0be8257f44ebab74edd27e57e1c5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1079b5b793b34c36b5208dbae1ff7ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0ae4939977341399a76fa780f7e63fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a30b25786814e68a7c51913b9902e3e",
              "IPY_MODEL_bc2e466a076f417f860c43f76b4d7f26",
              "IPY_MODEL_3d27d9ddb3d848d48fc2cbc303eb15a0"
            ],
            "layout": "IPY_MODEL_9c084413c9b748d08228e0237ac1b778"
          }
        },
        "0a30b25786814e68a7c51913b9902e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7dc2e6de0ab4052b29882bb28b6785c",
            "placeholder": "​",
            "style": "IPY_MODEL_c9f77932a9464f93b4c98fcbc90d3476",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "bc2e466a076f417f860c43f76b4d7f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7490c67cb55f4195a9803f90dd48b0ca",
            "max": 55351,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab7d7cd06c0a4e13ba0de7fdca5983b2",
            "value": 55351
          }
        },
        "3d27d9ddb3d848d48fc2cbc303eb15a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2daceb510b7b4acc80cbbbd96c399589",
            "placeholder": "​",
            "style": "IPY_MODEL_5bef16207e064604b746295514cb6cbd",
            "value": " 55.4k/55.4k [00:00&lt;00:00, 943kB/s]"
          }
        },
        "9c084413c9b748d08228e0237ac1b778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7dc2e6de0ab4052b29882bb28b6785c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9f77932a9464f93b4c98fcbc90d3476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7490c67cb55f4195a9803f90dd48b0ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab7d7cd06c0a4e13ba0de7fdca5983b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2daceb510b7b4acc80cbbbd96c399589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bef16207e064604b746295514cb6cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25871037f82a40deb0f3027763dea018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36b71313f9bb49d7928782f966458998",
              "IPY_MODEL_a2c901403f7a4b5b80847e6a9361ab87",
              "IPY_MODEL_b22b081e62a94f68942a75f51e33ac67"
            ],
            "layout": "IPY_MODEL_5ede96c8b82a4792b80e4cd086d7ceaf"
          }
        },
        "36b71313f9bb49d7928782f966458998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ada8aef79fd49f7acf2665bef60dc5d",
            "placeholder": "​",
            "style": "IPY_MODEL_0eb4b17f1b94483faabdb27d593d5a08",
            "value": "tokenizer.json: 100%"
          }
        },
        "a2c901403f7a4b5b80847e6a9361ab87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d0d3e3871254c9192f759884800f82c",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca81f1edf526409caa8765aed4afeecd",
            "value": 9085657
          }
        },
        "b22b081e62a94f68942a75f51e33ac67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11b294389b464d3b84f43f0f1db244f0",
            "placeholder": "​",
            "style": "IPY_MODEL_c259b6bf45c640a1a913af9c18d75d68",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 32.4MB/s]"
          }
        },
        "5ede96c8b82a4792b80e4cd086d7ceaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ada8aef79fd49f7acf2665bef60dc5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eb4b17f1b94483faabdb27d593d5a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d0d3e3871254c9192f759884800f82c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca81f1edf526409caa8765aed4afeecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11b294389b464d3b84f43f0f1db244f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c259b6bf45c640a1a913af9c18d75d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c82b89744e24e28826cf4f30948b240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1f8544698634c57aab76209e64b6bcf",
              "IPY_MODEL_66122e84e67f4edfbd61384c7eee3e0c",
              "IPY_MODEL_e2fa66e2621a40f8a41ba95cfe7b807a"
            ],
            "layout": "IPY_MODEL_afe03352a56e4d8f96f0d1fc3a1bae58"
          }
        },
        "c1f8544698634c57aab76209e64b6bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1511121b50d346898499244ed54ae996",
            "placeholder": "​",
            "style": "IPY_MODEL_2237752bf5ff46fb862042bfb6eb14cb",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "66122e84e67f4edfbd61384c7eee3e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ef5541fed014f66882c76948e37c619",
            "max": 296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6594e7d234e48adb293945588558c26",
            "value": 296
          }
        },
        "e2fa66e2621a40f8a41ba95cfe7b807a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a6cb283eb6d4877ad950f9c3441a30f",
            "placeholder": "​",
            "style": "IPY_MODEL_dfbbec7018ed47388bb9e464dc318a91",
            "value": " 296/296 [00:00&lt;00:00, 28.1kB/s]"
          }
        },
        "afe03352a56e4d8f96f0d1fc3a1bae58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1511121b50d346898499244ed54ae996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2237752bf5ff46fb862042bfb6eb14cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ef5541fed014f66882c76948e37c619": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6594e7d234e48adb293945588558c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a6cb283eb6d4877ad950f9c3441a30f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfbbec7018ed47388bb9e464dc318a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dba159d4bee482abb85df06543ab9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d244f4a618d0467ea9e5f2c08f3a7a21",
              "IPY_MODEL_42019ee54418479bbac6afaed04c3574",
              "IPY_MODEL_af2c989ba8f74ff7a14e3d61be626a3a"
            ],
            "layout": "IPY_MODEL_4ae5055c3bb7482195319c32b4528d2a"
          }
        },
        "d244f4a618d0467ea9e5f2c08f3a7a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0b525755b17445bb66bd47692f986a5",
            "placeholder": "​",
            "style": "IPY_MODEL_e4fdee063c8c4af6a5f864faa1778ad3",
            "value": "config.json: 100%"
          }
        },
        "42019ee54418479bbac6afaed04c3574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b69384152dd4be997aec9799348ec46",
            "max": 855,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2af5c08f6b08419994bc612258b1e90e",
            "value": 855
          }
        },
        "af2c989ba8f74ff7a14e3d61be626a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e5f5ca15fec4d24853943af5e953d90",
            "placeholder": "​",
            "style": "IPY_MODEL_6c91e96b5dd5407db32582cf663c79e2",
            "value": " 855/855 [00:00&lt;00:00, 27.8kB/s]"
          }
        },
        "4ae5055c3bb7482195319c32b4528d2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b525755b17445bb66bd47692f986a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4fdee063c8c4af6a5f864faa1778ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b69384152dd4be997aec9799348ec46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2af5c08f6b08419994bc612258b1e90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e5f5ca15fec4d24853943af5e953d90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c91e96b5dd5407db32582cf663c79e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c2b29af0bbe406abd1154c7ba696d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0619c282554043a69c89c72cd13e5bf2",
              "IPY_MODEL_7d28cc00f4a34d1c9e750cee37240cc3",
              "IPY_MODEL_190d722850f04a06af016d1237c6d5be"
            ],
            "layout": "IPY_MODEL_d0e0673cac9c471b8bbbc9c53694733c"
          }
        },
        "0619c282554043a69c89c72cd13e5bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26c1091978424bc2b48007a7b2c131d4",
            "placeholder": "​",
            "style": "IPY_MODEL_c5a2f0cd42ef4cbb9c39bf526f91fcf3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7d28cc00f4a34d1c9e750cee37240cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7046a9e3e64445c084a6d3f03ca3be4e",
            "max": 776,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b088c755560643148e921906c2879902",
            "value": 776
          }
        },
        "190d722850f04a06af016d1237c6d5be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a79df47eef04296bb64547053a76e08",
            "placeholder": "​",
            "style": "IPY_MODEL_2e95bd4b76244acc9ed63ed6de3a5d01",
            "value": " 776/776 [00:00&lt;00:00, 57.6kB/s]"
          }
        },
        "d0e0673cac9c471b8bbbc9c53694733c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26c1091978424bc2b48007a7b2c131d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5a2f0cd42ef4cbb9c39bf526f91fcf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7046a9e3e64445c084a6d3f03ca3be4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b088c755560643148e921906c2879902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a79df47eef04296bb64547053a76e08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e95bd4b76244acc9ed63ed6de3a5d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bff724cf473448b8d0f188ee1e6b852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f8f336c4a5545c19d633c6c093647c3",
              "IPY_MODEL_3d2313bb7228465f959e6f7eb1705e7c",
              "IPY_MODEL_57bd868ae47c4012861169ec3756ae49"
            ],
            "layout": "IPY_MODEL_0455e2474ff749ea903cca13831e883c"
          }
        },
        "7f8f336c4a5545c19d633c6c093647c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37263b3ca8a94edead998064de9106f1",
            "placeholder": "​",
            "style": "IPY_MODEL_5163f5e16dea490098fb2979c008b1d2",
            "value": "tokenizer.model: 100%"
          }
        },
        "3d2313bb7228465f959e6f7eb1705e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4dd2fb1b6ec4598912391c015ec4d42",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c8088be0927470ab9ba0034527e0d0b",
            "value": 499723
          }
        },
        "57bd868ae47c4012861169ec3756ae49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a8de725dce54d7fbfb71fc3c99426fa",
            "placeholder": "​",
            "style": "IPY_MODEL_9f9c645349b449c09d51bc5e3f548a36",
            "value": " 500k/500k [00:00&lt;00:00, 10.9MB/s]"
          }
        },
        "0455e2474ff749ea903cca13831e883c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37263b3ca8a94edead998064de9106f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5163f5e16dea490098fb2979c008b1d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4dd2fb1b6ec4598912391c015ec4d42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c8088be0927470ab9ba0034527e0d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a8de725dce54d7fbfb71fc3c99426fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f9c645349b449c09d51bc5e3f548a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74f59c85892e448cb381be50fbb7c69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ffb6aeb32e1495d8f83f64095b8e3e4",
              "IPY_MODEL_2f447a5b1d2c48cf8bddf2df46fef2c3",
              "IPY_MODEL_2509783a69ca448eaccda5088ba70c7a"
            ],
            "layout": "IPY_MODEL_7ef18a76ab9348f5999832bb7c861969"
          }
        },
        "6ffb6aeb32e1495d8f83f64095b8e3e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4b15c95036c430b949b176a83a0a8d6",
            "placeholder": "​",
            "style": "IPY_MODEL_15108c870974487a9b5613eafd45f12b",
            "value": "tokenizer.json: 100%"
          }
        },
        "2f447a5b1d2c48cf8bddf2df46fef2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b082c57e41ee4c35b9e2cb23912ebb9a",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61a7f15089d047dd93a982aa5fb82576",
            "value": 1842767
          }
        },
        "2509783a69ca448eaccda5088ba70c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e7f781dca7745f3a7bc0f95c5c9cbbc",
            "placeholder": "​",
            "style": "IPY_MODEL_025cacee72624352b391e046ab01f6b2",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 30.0MB/s]"
          }
        },
        "7ef18a76ab9348f5999832bb7c861969": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4b15c95036c430b949b176a83a0a8d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15108c870974487a9b5613eafd45f12b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b082c57e41ee4c35b9e2cb23912ebb9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a7f15089d047dd93a982aa5fb82576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e7f781dca7745f3a7bc0f95c5c9cbbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "025cacee72624352b391e046ab01f6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a709be84d3541708f45859a4f1067fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ecf8518fc7424dba9f260c261064ebab",
              "IPY_MODEL_cf2ec075a4e54093af6a942022ec1fc8",
              "IPY_MODEL_d5be8d7aa1004c6f9ab60513a26aed97"
            ],
            "layout": "IPY_MODEL_ca4b6c447aa84aedabcafcf0159195c2"
          }
        },
        "ecf8518fc7424dba9f260c261064ebab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59bcfee2cda048759fcb152238a01f9a",
            "placeholder": "​",
            "style": "IPY_MODEL_bded4ea3b8754bbe849df94a8caea119",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "cf2ec075a4e54093af6a942022ec1fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebc493be87534b22bdb050739571bf54",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6ab8ff16e844d059710a64c76e35858",
            "value": 414
          }
        },
        "d5be8d7aa1004c6f9ab60513a26aed97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbc37aa39f92408dbcca6df34d497fb9",
            "placeholder": "​",
            "style": "IPY_MODEL_d1fe2a3aee2a429c9698d301ac4293ad",
            "value": " 414/414 [00:00&lt;00:00, 27.6kB/s]"
          }
        },
        "ca4b6c447aa84aedabcafcf0159195c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59bcfee2cda048759fcb152238a01f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bded4ea3b8754bbe849df94a8caea119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebc493be87534b22bdb050739571bf54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ab8ff16e844d059710a64c76e35858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbc37aa39f92408dbcca6df34d497fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1fe2a3aee2a429c9698d301ac4293ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f78bc8f3bf8e434688efe84e056639ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6258b67436544967883bea91fe612356",
              "IPY_MODEL_0282a725238f4a67bdca9ae1eb7cd9d3",
              "IPY_MODEL_561561e64d4b4a0cb956af94a70ff7bb"
            ],
            "layout": "IPY_MODEL_d4ad8a775b7f44cebe110014e62b5265"
          }
        },
        "6258b67436544967883bea91fe612356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4bdb2a78c894e3380ef8400c95d7f5f",
            "placeholder": "​",
            "style": "IPY_MODEL_b191c88940174991a99556d0475b1cdf",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0282a725238f4a67bdca9ae1eb7cd9d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec3eb88506bd4171b16373da875106d4",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfa8fb9432da4a138054edcc86f98549",
            "value": 4
          }
        },
        "561561e64d4b4a0cb956af94a70ff7bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c26485818d54873b588b2e88455cfb1",
            "placeholder": "​",
            "style": "IPY_MODEL_9d46469d91154cc6ab6aa39ea81d21b4",
            "value": " 4/4 [00:22&lt;00:00, 22.90s/it]"
          }
        },
        "d4ad8a775b7f44cebe110014e62b5265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4bdb2a78c894e3380ef8400c95d7f5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b191c88940174991a99556d0475b1cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec3eb88506bd4171b16373da875106d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfa8fb9432da4a138054edcc86f98549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c26485818d54873b588b2e88455cfb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d46469d91154cc6ab6aa39ea81d21b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19a4ea748ba4480e9d96101142f120bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2d13a5571454df8954e41483d792fe1",
              "IPY_MODEL_941f0f710bb74552a3876399dcdb881a",
              "IPY_MODEL_14d2583ecf274b8e9f937ad009643319"
            ],
            "layout": "IPY_MODEL_ffeb5febca824bd68e88b2980a5a9b6b"
          }
        },
        "e2d13a5571454df8954e41483d792fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55d119c388f64054bffdc6a47c5cae72",
            "placeholder": "​",
            "style": "IPY_MODEL_b03b382414844b5e996478d68e4b33df",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "941f0f710bb74552a3876399dcdb881a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd7d6075252343a2ba03c84838c6ac60",
            "max": 50566,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_020e99660c1047a9a1cc3f6a50a59856",
            "value": 50566
          }
        },
        "14d2583ecf274b8e9f937ad009643319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0db9e6b181649cc902b0d0bc05b98ea",
            "placeholder": "​",
            "style": "IPY_MODEL_32cd9f6230274baca19ee790450d06d1",
            "value": " 50.6k/50.6k [00:00&lt;00:00, 4.07MB/s]"
          }
        },
        "ffeb5febca824bd68e88b2980a5a9b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55d119c388f64054bffdc6a47c5cae72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b03b382414844b5e996478d68e4b33df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd7d6075252343a2ba03c84838c6ac60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "020e99660c1047a9a1cc3f6a50a59856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0db9e6b181649cc902b0d0bc05b98ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32cd9f6230274baca19ee790450d06d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a9f789421b447b8b514fc3605bdd4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a49bf73d1866489cabb9e0f4e69633d5",
              "IPY_MODEL_759fb6b6ba64467ea80a80ffdd8a9fd7",
              "IPY_MODEL_fe10ba710263432b9957ca9c26d13245"
            ],
            "layout": "IPY_MODEL_ef075ad9d9c14c0fba53400e554fed03"
          }
        },
        "a49bf73d1866489cabb9e0f4e69633d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3984b4b8a01045c29015a63c2eb923dc",
            "placeholder": "​",
            "style": "IPY_MODEL_0d020d846e5b4a28897a372e8e05a70f",
            "value": "tokenizer.json: 100%"
          }
        },
        "759fb6b6ba64467ea80a80ffdd8a9fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccbe4fa5b787491faca0f12a602b285e",
            "max": 9085698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbc70a2f85fb479ca6c8ddf0852f9a2f",
            "value": 9085698
          }
        },
        "fe10ba710263432b9957ca9c26d13245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf1df91d88c04e1ba9f220df57ac8799",
            "placeholder": "​",
            "style": "IPY_MODEL_36688d015aad49d18182097af6a2522e",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 11.7MB/s]"
          }
        },
        "ef075ad9d9c14c0fba53400e554fed03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3984b4b8a01045c29015a63c2eb923dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d020d846e5b4a28897a372e8e05a70f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccbe4fa5b787491faca0f12a602b285e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbc70a2f85fb479ca6c8ddf0852f9a2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf1df91d88c04e1ba9f220df57ac8799": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36688d015aad49d18182097af6a2522e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d39b61c62014969b5c2515432085634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a99f6298fcea4bfb8200824cd93ea467",
              "IPY_MODEL_7cfda4aa70a047faabc69189cbccf30a",
              "IPY_MODEL_23689ba215c1418e8a979f853e2a642a"
            ],
            "layout": "IPY_MODEL_ee97b247323045ce8f2920fbcf57d0fc"
          }
        },
        "a99f6298fcea4bfb8200824cd93ea467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43f4f74467b241e689419e9493e75677",
            "placeholder": "​",
            "style": "IPY_MODEL_1787535f26da4bb5a01b58663ab56c49",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "7cfda4aa70a047faabc69189cbccf30a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc6e165f8ea747d58b72ca496e8786c5",
            "max": 73,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8752b93754994b75bb1c3e10c8310242",
            "value": 73
          }
        },
        "23689ba215c1418e8a979f853e2a642a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5339318ac4ae45c1b6c83151f97901ee",
            "placeholder": "​",
            "style": "IPY_MODEL_9571577850164f8f93b15b38a5a458e2",
            "value": " 73.0/73.0 [00:00&lt;00:00, 6.84kB/s]"
          }
        },
        "ee97b247323045ce8f2920fbcf57d0fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43f4f74467b241e689419e9493e75677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1787535f26da4bb5a01b58663ab56c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc6e165f8ea747d58b72ca496e8786c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8752b93754994b75bb1c3e10c8310242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5339318ac4ae45c1b6c83151f97901ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9571577850164f8f93b15b38a5a458e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70d39c5cff9a4f7ab92cf03846327a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_510ef7c1afc145c98bbcab559f5611ec",
              "IPY_MODEL_c6e83bf013aa432bb022c071902230d2",
              "IPY_MODEL_555d527dcbfd4e62870840e87b7a1c8a"
            ],
            "layout": "IPY_MODEL_c980e0032ab94db1b965c85643a6e899"
          }
        },
        "510ef7c1afc145c98bbcab559f5611ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4485b0b33c9a40a3b97dcbc6554b1f77",
            "placeholder": "​",
            "style": "IPY_MODEL_cab6a8c99d744a82aef195bc67710135",
            "value": "config.json: 100%"
          }
        },
        "c6e83bf013aa432bb022c071902230d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_826d9a76dc594576b1414ecdf228c1d4",
            "max": 843,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_830f908d816a4abc9521741cbf74c0ab",
            "value": 843
          }
        },
        "555d527dcbfd4e62870840e87b7a1c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ff14329f3c742d8ada737a3614d1733",
            "placeholder": "​",
            "style": "IPY_MODEL_2f25c11565c24ef78a9dcdf55abf85a8",
            "value": " 843/843 [00:00&lt;00:00, 25.1kB/s]"
          }
        },
        "c980e0032ab94db1b965c85643a6e899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4485b0b33c9a40a3b97dcbc6554b1f77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cab6a8c99d744a82aef195bc67710135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "826d9a76dc594576b1414ecdf228c1d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "830f908d816a4abc9521741cbf74c0ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ff14329f3c742d8ada737a3614d1733": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f25c11565c24ef78a9dcdf55abf85a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9260de2b6825483481b685111af9ff0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d91bee53faf4ca080db25cd8744de6a",
              "IPY_MODEL_17de6bd7cc674986ab151f366393b5f3",
              "IPY_MODEL_0a5990bfdeb348e6b267e68873d166c0"
            ],
            "layout": "IPY_MODEL_f3fc47c0672342588cc07af08eb246c1"
          }
        },
        "2d91bee53faf4ca080db25cd8744de6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f267c06863e42b99ba3e0a17fe3e069",
            "placeholder": "​",
            "style": "IPY_MODEL_acc389daa4244fb9b19ce4d3a79d3b52",
            "value": "model.safetensors: 100%"
          }
        },
        "17de6bd7cc674986ab151f366393b5f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81c24150a0d48e4924a5fb8ba4a57d0",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ffa1d8731644f35881877d89d9b8b35",
            "value": 2471645608
          }
        },
        "0a5990bfdeb348e6b267e68873d166c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_088c7947448d4971890834785d8795d2",
            "placeholder": "​",
            "style": "IPY_MODEL_65a36b580544433989037bf93b378735",
            "value": " 2.47G/2.47G [00:21&lt;00:00, 51.2MB/s]"
          }
        },
        "f3fc47c0672342588cc07af08eb246c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f267c06863e42b99ba3e0a17fe3e069": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acc389daa4244fb9b19ce4d3a79d3b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f81c24150a0d48e4924a5fb8ba4a57d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ffa1d8731644f35881877d89d9b8b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "088c7947448d4971890834785d8795d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a36b580544433989037bf93b378735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd5b32ff45fe4b369231429312e5a615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_450ae8ba85bc480bbabb84f45fe95c3c",
              "IPY_MODEL_004b3c33f4ca4fd68d90adb0bee52094",
              "IPY_MODEL_1c82addfd11b4de3adbabd73c2c04aa8"
            ],
            "layout": "IPY_MODEL_9887397bdb7f407081a57f39f82acf45"
          }
        },
        "450ae8ba85bc480bbabb84f45fe95c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_225ec7ffb01c479da4404e66d96c29bd",
            "placeholder": "​",
            "style": "IPY_MODEL_567454cc2eb0415a95bbbb08c572a39d",
            "value": "generation_config.json: 100%"
          }
        },
        "004b3c33f4ca4fd68d90adb0bee52094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_071eb0739d4640409b47c5fe30b435fa",
            "max": 185,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_757b695ce5b341d1896529412bb27a58",
            "value": 185
          }
        },
        "1c82addfd11b4de3adbabd73c2c04aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89636cdabb8b458780bec5acab35c9ca",
            "placeholder": "​",
            "style": "IPY_MODEL_94f76743b59348848b522742b14e118e",
            "value": " 185/185 [00:00&lt;00:00, 16.2kB/s]"
          }
        },
        "9887397bdb7f407081a57f39f82acf45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "225ec7ffb01c479da4404e66d96c29bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "567454cc2eb0415a95bbbb08c572a39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "071eb0739d4640409b47c5fe30b435fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "757b695ce5b341d1896529412bb27a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89636cdabb8b458780bec5acab35c9ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f76743b59348848b522742b14e118e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bc8cf61167a45bdb99e65fa4279e9ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b63b28b0dcdc46d1909eb565e828f03f",
              "IPY_MODEL_bf7240a336994fa4b2f6cb76a50ef3e5",
              "IPY_MODEL_b19d3f09063e4c7f9c2291bf9a87d29e"
            ],
            "layout": "IPY_MODEL_fbffa5a074184d0a83f1c45ed680b305"
          }
        },
        "b63b28b0dcdc46d1909eb565e828f03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db206453f2be475ba78f25e116c446ef",
            "placeholder": "​",
            "style": "IPY_MODEL_6d542517b9b94ccd983446a63a7c4f4f",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "bf7240a336994fa4b2f6cb76a50ef3e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b98bb429cd3418f8174d15c831cb9ef",
            "max": 50566,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f521189747649e4b7ce82fccd719038",
            "value": 50566
          }
        },
        "b19d3f09063e4c7f9c2291bf9a87d29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e9e56d68ade4a81a4263035372a10c5",
            "placeholder": "​",
            "style": "IPY_MODEL_777f9f79f15a4c789f8054252e89a143",
            "value": " 50.6k/50.6k [00:00&lt;00:00, 3.99MB/s]"
          }
        },
        "fbffa5a074184d0a83f1c45ed680b305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db206453f2be475ba78f25e116c446ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d542517b9b94ccd983446a63a7c4f4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b98bb429cd3418f8174d15c831cb9ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f521189747649e4b7ce82fccd719038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e9e56d68ade4a81a4263035372a10c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "777f9f79f15a4c789f8054252e89a143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33da43ea81bf4d349d4382cc0980d3e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53f7d3bea6f94ef2a3cda1c709d2b330",
              "IPY_MODEL_932120a80e5a4b14818b19c7537bacf9",
              "IPY_MODEL_f39b9fd9b03c415c9f465a90f2049e95"
            ],
            "layout": "IPY_MODEL_49aa81e38f8c4b3f9aacab75313bd706"
          }
        },
        "53f7d3bea6f94ef2a3cda1c709d2b330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b26cf1a43f74a7d893166cb07568aba",
            "placeholder": "​",
            "style": "IPY_MODEL_9d6e332ff1fe462b97d9c65ba0da478b",
            "value": "tokenizer.json: 100%"
          }
        },
        "932120a80e5a4b14818b19c7537bacf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_797bcb2a1c4e45079ab0227eb7af538f",
            "max": 9085698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e885702aa0842c985c5ee4ae3abb745",
            "value": 9085698
          }
        },
        "f39b9fd9b03c415c9f465a90f2049e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_650fed1371fd4e389179dad939f0bc1d",
            "placeholder": "​",
            "style": "IPY_MODEL_d9048d46cf9347f19431f8606fecea6d",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 36.6MB/s]"
          }
        },
        "49aa81e38f8c4b3f9aacab75313bd706": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b26cf1a43f74a7d893166cb07568aba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d6e332ff1fe462b97d9c65ba0da478b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "797bcb2a1c4e45079ab0227eb7af538f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e885702aa0842c985c5ee4ae3abb745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "650fed1371fd4e389179dad939f0bc1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9048d46cf9347f19431f8606fecea6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80adcdc172ad46b3912b447cd6b8b7dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba942683e3a245a5b99ed2bed5b24d64",
              "IPY_MODEL_a25e06b054474ca9bfe20ea5e9d1c793",
              "IPY_MODEL_826607a977b94c1aa7ee71e9b6ceadfc"
            ],
            "layout": "IPY_MODEL_904c5d47507e45bcb77a641c1097f2cd"
          }
        },
        "ba942683e3a245a5b99ed2bed5b24d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3f6635f80be4427b25eda3b6e440702",
            "placeholder": "​",
            "style": "IPY_MODEL_eef8cea8b679400c83156e0bfe8795f3",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "a25e06b054474ca9bfe20ea5e9d1c793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42b1147402804bcaba2949997390f5f7",
            "max": 73,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ce893531ea141e4ae21d9e3cbb90925",
            "value": 73
          }
        },
        "826607a977b94c1aa7ee71e9b6ceadfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4f62b6af92b4c80a09e92f0129a50ce",
            "placeholder": "​",
            "style": "IPY_MODEL_138c420982714c73a43c44e061d4b815",
            "value": " 73.0/73.0 [00:00&lt;00:00, 5.66kB/s]"
          }
        },
        "904c5d47507e45bcb77a641c1097f2cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3f6635f80be4427b25eda3b6e440702": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eef8cea8b679400c83156e0bfe8795f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42b1147402804bcaba2949997390f5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ce893531ea141e4ae21d9e3cbb90925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4f62b6af92b4c80a09e92f0129a50ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "138c420982714c73a43c44e061d4b815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_09HfTqWrrA0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0b5x7l_prw1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "czlcOszJrw4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tve517v9rw69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ttj/blt"
      ],
      "metadata": {
        "id": "q32ls6OCtOAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# Load BLT-1B model\n",
        "model_weights = load_file('safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model\n",
        "entropy_weights = load_file('safetensors/entropy_model/consolidated.safetensors')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "bjvQSp8Trw9V",
        "outputId": "182fa071-7ea7-4886-c8d5-bebafe410f53"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No such file or directory: \"safetensors/blt_1b/consolidated.safetensors\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3beb9621934a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load BLT-1B model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'safetensors/blt_1b/consolidated.safetensors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load entropy model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/safetensors/torch.py\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \"\"\"\n\u001b[1;32m    312\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msafe_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: \"safetensors/blt_1b/consolidated.safetensors\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X0F097eVrxjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# Load BLT-1B model\n",
        "model_weights = load_file('ttj/blt')\n",
        "\n",
        "# Load entropy model\n",
        "entropy_weights = load_file('ttj/blt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "fBf9X4V3sXuO",
        "outputId": "24ee96dc-8986-42fc-b335-f42d6f9545d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No such file or directory: \"ttj/blt\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6dd8b7c053b5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load BLT-1B model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ttj/blt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load entropy model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/safetensors/torch.py\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \"\"\"\n\u001b[1;32m    312\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msafe_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: \"ttj/blt\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/safetensors/ blt_1b\n",
        "\n",
        "\n",
        "!wget https://huggingface.co/ttj/blt/resolve/main/safetensors/blt_1b/consolidated.safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so3MSs3Osbkv",
        "outputId": "23e5f44e-f760-4dbf-a0e9-2cd84acbd459"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/safetensors/ blt_1b\n",
            "--2025-04-24 21:28:03--  https://huggingface.co/ttj/blt/resolve/main/safetensors/blt_1b/consolidated.safetensors\n",
            "Resolving huggingface.co (huggingface.co)... 3.167.112.96, 3.167.112.45, 3.167.112.38, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.167.112.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/c4/d6/c4d6ccaf6771cc82a464702e8fb58c952ab0960b39b8a81be086d594c3ff5b3f/3738bef71ca45c1ac58783c93c6eee8da6feefc0672113bcccd0a44cfd3cfe1f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27consolidated.safetensors%3B+filename%3D%22consolidated.safetensors%22%3B&Expires=1745533683&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTUzMzY4M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2M0L2Q2L2M0ZDZjY2FmNjc3MWNjODJhNDY0NzAyZThmYjU4Yzk1MmFiMDk2MGIzOWI4YTgxYmUwODZkNTk0YzNmZjViM2YvMzczOGJlZjcxY2E0NWMxYWM1ODc4M2M5M2M2ZWVlOGRhNmZlZWZjMDY3MjExM2JjY2NkMGE0NGNmZDNjZmUxZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=lvGaAy4rsLhd6YYzdcg-iqAeAikrj2XSNQEuwFpmTVannQrSU4lv%7EAI7sil9Xopewr9GVM0wLbnAZejKCfbnhe7p2s3PSK2Mbkgy5shVha1qFR4hxUqQ1OvdnxPJBAjpv1cDFzDRo6meZPfjEC%7EOQjOYVG6LbMuStPmaixGdLGagPE1bXMGumRjvAsv22vcxD0cAgkrQAKAJsxA1Vb8incMH7bubQwzj3Oj7RxEkOQI2q5wFwopZKeUZsOMTcaGxtzOrVx39FWGewEE2U%7E4zwC3bf3HJRU5rxVqnYgENfiyZI9-B0ZX-49Wg0Ve%7EEGKEsuFL47XVNI4JjOlM90w02w__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-24 21:28:03--  https://cdn-lfs-us-1.hf.co/repos/c4/d6/c4d6ccaf6771cc82a464702e8fb58c952ab0960b39b8a81be086d594c3ff5b3f/3738bef71ca45c1ac58783c93c6eee8da6feefc0672113bcccd0a44cfd3cfe1f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27consolidated.safetensors%3B+filename%3D%22consolidated.safetensors%22%3B&Expires=1745533683&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTUzMzY4M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2M0L2Q2L2M0ZDZjY2FmNjc3MWNjODJhNDY0NzAyZThmYjU4Yzk1MmFiMDk2MGIzOWI4YTgxYmUwODZkNTk0YzNmZjViM2YvMzczOGJlZjcxY2E0NWMxYWM1ODc4M2M5M2M2ZWVlOGRhNmZlZWZjMDY3MjExM2JjY2NkMGE0NGNmZDNjZmUxZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=lvGaAy4rsLhd6YYzdcg-iqAeAikrj2XSNQEuwFpmTVannQrSU4lv%7EAI7sil9Xopewr9GVM0wLbnAZejKCfbnhe7p2s3PSK2Mbkgy5shVha1qFR4hxUqQ1OvdnxPJBAjpv1cDFzDRo6meZPfjEC%7EOQjOYVG6LbMuStPmaixGdLGagPE1bXMGumRjvAsv22vcxD0cAgkrQAKAJsxA1Vb8incMH7bubQwzj3Oj7RxEkOQI2q5wFwopZKeUZsOMTcaGxtzOrVx39FWGewEE2U%7E4zwC3bf3HJRU5rxVqnYgENfiyZI9-B0ZX-49Wg0Ve%7EEGKEsuFL47XVNI4JjOlM90w02w__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.168.73.68, 3.168.73.31, 3.168.73.67, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.168.73.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9067807688 (8.4G) [binary/octet-stream]\n",
            "Saving to: ‘consolidated.safetensors’\n",
            "\n",
            "consolidated.safete 100%[===================>]   8.44G  44.3MB/s    in 3m 6s   \n",
            "\n",
            "2025-04-24 21:31:09 (46.4 MB/s) - ‘consolidated.safetensors’ saved [9067807688/9067807688]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/safetensors/entropy_model\n",
        "!wget https://huggingface.co/ttj/blt/resolve/main/safetensors/entropy_model/consolidated.safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4jHl4E5s0vX",
        "outputId": "e669a885-6ed1-4813-9b09-f1fe0fa01a11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/safetensors/entropy_model\n",
            "--2025-04-24 21:32:15--  https://huggingface.co/ttj/blt/resolve/main/safetensors/entropy_model/consolidated.safetensors\n",
            "Resolving huggingface.co (huggingface.co)... 3.168.73.106, 3.168.73.111, 3.168.73.38, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.168.73.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/c4/d6/c4d6ccaf6771cc82a464702e8fb58c952ab0960b39b8a81be086d594c3ff5b3f/aa45c5ffae9f528329d146a91356d1d06056b71653beddada39781bfcadb521d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27consolidated.safetensors%3B+filename%3D%22consolidated.safetensors%22%3B&Expires=1745533935&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTUzMzkzNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2M0L2Q2L2M0ZDZjY2FmNjc3MWNjODJhNDY0NzAyZThmYjU4Yzk1MmFiMDk2MGIzOWI4YTgxYmUwODZkNTk0YzNmZjViM2YvYWE0NWM1ZmZhZTlmNTI4MzI5ZDE0NmE5MTM1NmQxZDA2MDU2YjcxNjUzYmVkZGFkYTM5NzgxYmZjYWRiNTIxZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=urQ3EPZdtqBKXOE6YVZBfNbvPWTvMTIUElrrfsFDuLYVLZWHJl5OqLXw7qN9QlVd6vGII7sNH5fi8kL-8VFWDjuOI56-3IPHloyvdRnGByJCTEHRjbiza98DNlzAQQlXw9Ez-eUJGOLIVgKD1WJuP3ht7aDENLpT57uH5lwsrXWQsXRvYdrK59C5ZnT3I61B4UUNENAZ5Jh1CpIVZEbWiJfcsx3rQkJmpSyvIY88O2sJARJtB8GSIpME4BQFBDyuND8TXGDnAdNMeLKTIrfRriVehJSPRFTg1pcyHAAiKftdinnw9l9VTwWcYLiWMyfmkMqrxv2H5ZbyW5IXLhSvmw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-24 21:32:15--  https://cdn-lfs-us-1.hf.co/repos/c4/d6/c4d6ccaf6771cc82a464702e8fb58c952ab0960b39b8a81be086d594c3ff5b3f/aa45c5ffae9f528329d146a91356d1d06056b71653beddada39781bfcadb521d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27consolidated.safetensors%3B+filename%3D%22consolidated.safetensors%22%3B&Expires=1745533935&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTUzMzkzNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2M0L2Q2L2M0ZDZjY2FmNjc3MWNjODJhNDY0NzAyZThmYjU4Yzk1MmFiMDk2MGIzOWI4YTgxYmUwODZkNTk0YzNmZjViM2YvYWE0NWM1ZmZhZTlmNTI4MzI5ZDE0NmE5MTM1NmQxZDA2MDU2YjcxNjUzYmVkZGFkYTM5NzgxYmZjYWRiNTIxZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=urQ3EPZdtqBKXOE6YVZBfNbvPWTvMTIUElrrfsFDuLYVLZWHJl5OqLXw7qN9QlVd6vGII7sNH5fi8kL-8VFWDjuOI56-3IPHloyvdRnGByJCTEHRjbiza98DNlzAQQlXw9Ez-eUJGOLIVgKD1WJuP3ht7aDENLpT57uH5lwsrXWQsXRvYdrK59C5ZnT3I61B4UUNENAZ5Jh1CpIVZEbWiJfcsx3rQkJmpSyvIY88O2sJARJtB8GSIpME4BQFBDyuND8TXGDnAdNMeLKTIrfRriVehJSPRFTg1pcyHAAiKftdinnw9l9VTwWcYLiWMyfmkMqrxv2H5ZbyW5IXLhSvmw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 108.138.128.54, 108.138.128.37, 108.138.128.53, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|108.138.128.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 199037304 (190M) [binary/octet-stream]\n",
            "Saving to: ‘consolidated.safetensors’\n",
            "\n",
            "consolidated.safete 100%[===================>] 189.82M  41.7MB/s    in 4.6s    \n",
            "\n",
            "2025-04-24 21:32:20 (41.0 MB/s) - ‘consolidated.safetensors’ saved [199037304/199037304]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w8JrtRAUty4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ksa8HQRUuAru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "# Load the weights from the safetensors files\n",
        "model.load_state_dict(model_weights)\n",
        "# (Optional) Load the entropy model weights if needed\n",
        "# entropy_model.load_state_dict(entropy_weights)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "u3IicIeXuBBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "input_text = \"Your input text here\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    output = model.generate(input_ids)\n",
        "\n",
        "# Decode the output\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(decoded_output)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8btdg7cPuCYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# Load BLT-1B model\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "q_nqpqYguG51",
        "outputId": "e2f619c2-5580-42ad-e71f-be7fc181be6a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No such file or directory: \"/content/safetensors/blt_1b/consolidated.safetensors\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-61808ccfa397>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load BLT-1B model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/safetensors/blt_1b/consolidated.safetensors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load entropy model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/safetensors/torch.py\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \"\"\"\n\u001b[1;32m    312\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msafe_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: \"/content/safetensors/blt_1b/consolidated.safetensors\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/safetensors"
      ],
      "metadata": {
        "id": "hIfrwCCWuM0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# Load BLT-1B model\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n"
      ],
      "metadata": {
        "id": "oD1UdoHYuZfV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# Load BLT-1B model\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "model.load_state_dict(model_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "FAPTxklIuvPm",
        "outputId": "269bb493-5028-4041-bc14-62792e7a0cbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b488e1c30aa0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mentropy_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/safetensors/entropy_model/consolidated.safetensors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "St_CLiiVuvhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load BLT-1B model\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Define and initialize the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "# Now load the weights\n",
        "model.load_state_dict(model_weights)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758,
          "referenced_widgets": [
            "27636fca3a8c461787baff1a349af865",
            "077922fa4810484e8defa176e64342f9",
            "d80bc3a6f888467bb324f0ec49873368",
            "3fef3da9e5d1429bb1b83dc0d644a1df",
            "7b6f536541dc4aeca8739a8a9c20f591",
            "e342024e50534b55a3214b4207edcf57",
            "40bebe3c8e784541a465d0750e760cf1",
            "d9fd7844cbed4400bc8b1f076fd4a58d",
            "a6cde0dfcd0843d3905a7e9495817da5",
            "492df7f033904f20b38d06c4c585b97a",
            "6ada7b011ccd49e2a3b6f6e17a074095",
            "1311d909060444b8a8a73c0d234b06dd",
            "5c8e7b06275c4fd9a8cbdc6521264359",
            "247eb04689f145849b61281011ee10d6",
            "681ae69919854e8cb6c94f6bae04af22",
            "b8e5d36ceb6a4ae487d0f2707c6544fe",
            "37a07bdf1a534f70a9a7fef0659cf880",
            "4c709f707db449059f713b123e262d22",
            "0c22b429f86e468a96e31abdf1f1f6c1",
            "a73d0fa1972c4ac8bc9541d4bf5f767f",
            "215163703bad4934a7a88ea6b4c3d779",
            "46c35cfe1fca4e51ab5b95c1a51917f9",
            "72408a7d7d2249af9750dba116cf988b",
            "101ef3d549554056b207930d46b4dc12",
            "71f9032787014916908a32144a5c4886",
            "d3c3d75af4624fa58f1b33b06c0e7962",
            "bf40faae9ec045819040acce8bde91ce",
            "0d20c4d3235d4a22a74f97cf9dcbed05",
            "7b6c0ad98d1644f4b92519bf4b8f8ce6",
            "d8d79b0c8aaf4f07a86d54300066ef7c",
            "d3002aa9d8664787b3569fcc2c081ccb",
            "ae0b8cfed0b442e0b4c6c06068892d23",
            "6216311ad10d4b3f92fdf4f2ff027f60"
          ]
        },
        "id": "mu01MD_Fu-fn",
        "outputId": "f7af800f-033c-4024-aa4d-ec3dbd6fd7c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:  79%|#######9  | 524M/663M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27636fca3a8c461787baff1a349af865"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/662M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1311d909060444b8a8a73c0d234b06dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72408a7d7d2249af9750dba116cf988b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for OPTForCausalLM:\n\tMissing key(s) in state_dict: \"model.decoder.embed_tokens.weight\", \"model.decoder.embed_positions.weight\", \"model.decoder.project_out.weight\", \"model.decoder.project_in.weight\", \"model.decoder.layers.0.self_attn.k_proj.weight\", \"model.decoder.layers.0.self_attn.k_proj.bias\", \"model.decoder.layers.0.self_attn.v_proj.weight\", \"model.decoder.layers.0.self_attn.v_proj.bias\", \"model.decoder.layers.0.self_attn.q_proj.weight\", \"model.decoder.layers.0.self_attn.q_proj.bias\", \"model.decoder.layers.0.self_attn.out_proj.weight\", \"model.decoder.layers.0.self_attn.out_proj.bias\", \"model.decoder.layers.0.self_attn_layer_norm.weight\", \"model.decoder.layers.0.self_attn_layer_norm.bias\", \"model.decoder.layers.0.fc1.weight\", \"model.decoder.layers.0.fc1.bias\", \"model.decoder.layers.0.fc2.weight\", \"model.decoder.layers.0.fc2.bias\", \"model.decoder.layers.0.final_layer_norm.weight\", \"model.decoder.layers.0.final_layer_norm.bias\", \"model.decoder.layers.1.self_attn.k_proj.weight\", \"model.decoder.layers.1.self_attn.k_proj.bias\", \"model.decoder.layers.1.self_attn.v_proj.weight\", \"model.decoder.layers.1.self_attn.v_proj.bias\", \"model.decoder.layers.1.self_attn.q_proj.weight\", \"model.decoder.layers.1.self_attn.q_proj.bias\", \"model.decoder.layers.1.self_attn.out_proj.weight\", \"model.decoder.layers.1.self_attn.out_proj.bias\", \"model.decoder.layers.1.self_attn_layer_norm.weight\", \"model.decoder.layers.1.self_attn_layer_norm.bias\", \"model.decoder.layers.1.fc1.weight\", \"model.decoder.layers.1.fc1.bias\", \"model.decoder.layers.1.fc2.weight\", \"model.decoder.layers.1.fc2.bias\", \"model.decoder.layers.1.final_layer_norm.weight\", \"model.decoder.layers.1.final_layer_norm.bias\", \"model.decoder.layers.2.self_attn.k_proj.weight\", \"model.decoder.layers.2.self_attn.k_proj.bias\", \"model.decoder.layers.2.self_attn.v_proj.weight\", \"model.decoder.layers.2.self_attn.v_proj.bias\", \"model.decoder.layers.2.self_attn.q_proj.weight\", \"model.decoder.layers.2.self_attn.q_proj.bias\", \"model.decoder.layers.2.self_attn.out_proj.weight\", \"model.decoder.layers.2.self_attn.out_proj.bias\", \"model.decoder.layers.2.self_attn_layer_norm.weight\", \"model.decoder.layers.2.self_attn_layer_norm.bias\", \"model.decoder.layers.2.fc1.weight\", \"model.decoder.layers.2.fc1.bias\", \"model.decoder.layers.2.fc2.weight\", \"model.decoder.layers.2.fc2.bias\", \"model.decoder.layers.2.final_layer_norm.weight\", \"model.decoder.layers.2.final_layer_norm.bias\", \"model.decoder.layers.3.self_attn.k_proj.weight\", \"model.decoder.layers.3.self_attn.k_proj.bias\", \"model.decoder.layers.3.self_attn.v_proj.weight\", \"model.decoder.layers.3.self_attn.v_proj.bias\", \"model.decoder.layers.3.self_attn.q_proj.weight\", \"model.decoder.layers.3.self_attn.q_proj.bias\", \"model.decoder.layers.3.self_attn.out_proj.weight\", \"model.decoder.layers.3.self_attn.out_proj.bias\", \"model.decoder.layers.3.self_attn_layer_norm.weight\", \"model.decoder.layers.3.self_attn_layer_norm.bias\", \"model.decoder.layers.3.fc1.weight\", \"model.decoder.layers.3.fc1.bias\", \"model.decoder.layers.3.fc2.weight\", \"model.decoder.layers.3.fc2.bias\", \"model.decoder.layers.3.final_layer_norm.weight\", \"model.decoder.layers.3.final_layer_norm.bias\", \"model.decoder.layers.4.self_attn.k_proj.weight\", \"model.decoder.layers.4.self_attn.k_proj.bias\", \"model.decoder.layers.4.self_attn.v_proj.weight\", \"model.decoder.layers.4.self_attn.v_proj.bias\", \"model.decoder.layers.4.self_attn.q_proj.weight\", \"model.decoder.layers.4.self_attn.q_proj.bias\", \"model.decoder.layers.4.self_attn.out_proj.weight\", \"model.decoder.layers.4.self_attn.out_proj.bias\", \"model.decoder.layers.4.self_attn_layer_norm.weight\", \"model.decoder.layers.4.self_attn_layer_norm.bias\", \"model.decoder.layers.4.fc1.weight\", \"model.decoder.layers.4.fc1.bias\", \"model.decoder.layers.4.fc2.weight\", \"model.decoder.layers.4.fc2.bias\", \"model.decoder.layers.4.final_layer_norm.weight\", \"model.decoder.layers.4.final_layer_norm.bias\", \"model.decoder.layers.5.self_attn.k_proj.weight\", \"model.decoder.layers.5.self_attn.k_proj.bias\", \"model.decoder.layers.5.self_attn.v_proj.weight\", \"model.decoder.layers.5.self_attn.v_proj.bias\", \"model.decoder.layers.5.self_attn.q_proj.weight\", \"model.decoder.layers.5.self_attn.q_proj.bias\", \"model.decoder.layers.5.self_attn.out_proj.weight\", \"model.decoder.layers.5.self_attn.out_proj.bias\", \"model.decoder.layers.5.self_attn_layer_norm.weight\", \"model.decoder.layers.5.self_attn_layer_norm.bias\", \"model.decoder.layers.5.fc1.weight\", \"model.decoder.layers.5.fc1.bias\", \"model.decoder.layers.5.fc2.weight\", \"model.decoder.layers.5.fc2.bias\", \"model.decoder.layers.5.final_layer_norm.weight\", \"model.decoder.layers.5.final_layer_norm.bias\", \"model.decoder.layers.6.self_attn.k_proj.weight\", \"model.decoder.layers.6.self_attn.k_proj.bias\", \"model.decoder.layers.6.self_attn.v_proj.weight\", \"model.decoder.layers.6.self_attn.v_proj.bias\", \"model.decoder.layers.6.self_attn.q_proj.weight\", \"model.decoder.layers.6.self_attn.q_proj.bias\", \"model.decoder.layers.6.self_attn.out_proj.weight\", \"model.decoder.layers.6.self_attn.out_proj.bias\", \"model.decoder.layers.6.self_attn_layer_norm.weight\", \"model.decoder.layers.6.self_attn_layer_norm.bias\", \"model.decoder.layers.6.fc1.weight\", \"model.decoder.layers.6.fc1.bias\", \"model.decoder.layers.6.fc2.weight\", \"model.decoder.layers.6.fc2.bias\", \"model.decoder.layers.6.final_layer_norm.weight\", \"model.decoder.layers.6.final_layer_norm.bias\", \"model.decoder.layers.7.self_attn.k_proj.weight\", \"model.decoder.layers.7.self_attn.k_proj.bias\", \"model.decoder.layers.7.self_attn.v_proj.weight\", \"model.decoder.layers.7.self_attn.v_proj.bias\", \"model.decoder.layers.7.self_attn.q_proj.weight\", \"model.decoder.layers.7.self_attn.q_proj.bias\", \"model.decoder.layers.7.self_attn.out_proj.weight\", \"model.decoder.layers.7.self_attn.out_proj.bias\", \"model.decoder.layers.7.self_attn_layer_norm.weight\", \"model.decoder.layers.7.self_attn_layer_norm.bias\", \"model.decoder.layers.7.fc1.weight\", \"model.decoder.layers.7.fc1.bias\", \"model.decoder.layers.7.fc2.weight\", \"model.decoder.layers.7.fc2.bias\", \"model.decoder.layers.7.final_layer_norm.weight\", \"model.decoder.layers.7.final_layer_norm.bias\", \"model.decoder.layers.8.self_attn.k_proj.weight\", \"model.decoder.layers.8.self_attn.k_proj.bias\", \"model.decoder.layers.8.self_attn.v_proj.weight\", \"model.decoder.layers.8.self_attn.v_proj.bias\", \"model.decoder.layers.8.self_attn.q_proj.weight\", \"model.decoder.layers.8.self_attn.q_proj.bias\", \"model.decoder.layers.8.self_attn.out_proj.weight\", \"model.decoder.layers.8.self_attn.out_proj.bias\", \"model.decoder.layers.8.self_attn_layer_norm.weight\", \"model.decoder.layers.8.self_attn_layer_norm.bias\", \"model.decoder.layers.8.fc1.weight\", \"model.decoder.layers.8.fc1.bias\", \"model.decoder.layers.8.fc2.weight\", \"model.decoder.layers.8.fc2.bias\", \"model.decoder.layers.8.final_layer_norm.weight\", \"model.decoder.layers.8.final_layer_norm.bias\", \"model.decoder.layers.9.self_attn.k_proj.weight\", \"model.decoder.layers.9.self_attn.k_proj.bias\", \"model.decoder.layers.9.self_attn.v_proj.weight\", \"model.decoder.layers.9.self_attn.v_proj.bias\", \"model.decoder.layers.9.self_attn.q_proj.weight\", \"model.decoder.layers.9.self_attn.q_proj.bias\", \"model.decoder.layers.9.self_attn.out_proj.weight\", \"model.decoder.layers.9.self_attn.out_proj.bias\", \"model.decoder.layers.9.self_attn_layer_norm.weight\", \"model.decoder.layers.9.self_attn_layer_norm.bias\", \"model.decoder.layers.9.fc1.weight\", \"model.decoder.layers.9.fc1.bias\", \"model.decoder.layers.9.fc2.weight\", \"model.decoder.layers.9.fc2.bias\", \"model.decoder.layers.9.final_layer_norm.weight\", \"model.decoder.layers.9.final_layer_norm.bias\", \"model.decoder.layers.10.self_attn.k_proj.weight\", \"model.decoder.layers.10.self_attn.k_proj.bias\", \"model.decoder.layers.10.self_attn.v_proj.weight\", \"model.decoder.layers.10.self_attn.v_proj.bias\", \"model.decoder.layers.10.self_attn.q_proj.weight\", \"model.decoder.layers.10.self_attn.q_proj.bias\", \"model.decoder.layers.10.self_attn.out_proj.weight\", \"model.decoder.layers.10.self_attn.out_proj.bias\", \"model.decoder.layers.10.self_attn_layer_norm.weight\", \"model.decoder.layers.10.self_attn_layer_norm.bias\", \"model.decoder.layers.10.fc1.weight\", \"model.decoder.layers.10.fc1.bias\", \"model.decoder.layers.10.fc2.weight\", \"model.decoder.layers.10.fc2.bias\", \"model.decoder.layers.10.final_layer_norm.weight\", \"model.decoder.layers.10.final_layer_norm.bias\", \"model.decoder.layers.11.self_attn.k_proj.weight\", \"model.decoder.layers.11.self_attn.k_proj.bias\", \"model.decoder.layers.11.self_attn.v_proj.weight\", \"model.decoder.layers.11.self_attn.v_proj.bias\", \"model.decoder.layers.11.self_attn.q_proj.weight\", \"model.decoder.layers.11.self_attn.q_proj.bias\", \"model.decoder.layers.11.self_attn.out_proj.weight\", \"model.decoder.layers.11.self_attn.out_proj.bias\", \"model.decoder.layers.11.self_attn_layer_norm.weight\", \"model.decoder.layers.11.self_attn_layer_norm.bias\", \"model.decoder.layers.11.fc1.weight\", \"model.decoder.layers.11.fc1.bias\", \"model.decoder.layers.11.fc2.weight\", \"model.decoder.layers.11.fc2.bias\", \"model.decoder.layers.11.final_layer_norm.weight\", \"model.decoder.layers.11.final_layer_norm.bias\", \"model.decoder.layers.12.self_attn.k_proj.weight\", \"model.decoder.layers.12.self_attn.k_proj.bias\", \"model.decoder.layers.12.self_attn.v_proj.weight\", \"model.decoder.layers.12.self_attn.v_proj.bias\", \"model.decoder.layers.12.self_attn.q_proj.weight\", \"model.decoder.layers.12.self_attn.q_proj.bias\", \"model.decoder.layers.12.self_attn.out_proj.weight\", \"model.decoder.layers.12.self_attn.out_proj.bias\", \"model.decoder.layers.12.self_attn_layer_norm.weight\", \"model.decoder.layers.12.self_attn_layer_norm.bias\", \"model.decoder.layers.12.fc1.weight\", \"model.decoder.layers.12.fc1.bias\", \"model.decoder.layers.12.fc2.weight\", \"model.decoder.layers.12.fc2.bias\", \"model.decoder.layers.12.final_layer_norm.weight\", \"model.decoder.layers.12.final_layer_norm.bias\", \"model.decoder.layers.13.self_attn.k_proj.weight\", \"model.decoder.layers.13.self_attn.k_proj.bias\", \"model.decoder.layers.13.self_attn.v_proj.weight\", \"model.decoder.layers.13.self_attn.v_proj.bias\", \"model.decoder.layers.13.self_attn.q_proj.weight\", \"model.decoder.layers.13.self_attn.q_proj.bias\", \"model.decoder.layers.13.self_attn.out_proj.weight\", \"model.decoder.layers.13.self_attn.out_proj.bias\", \"model.decoder.layers.13.self_attn_layer_norm.weight\", \"model.decoder.layers.13.self_attn_layer_norm.bias\", \"model.decoder.layers.13.fc1.weight\", \"model.decoder.layers.13.fc1.bias\", \"model.decoder.layers.13.fc2.weight\", \"model.decoder.layers.13.fc2.bias\", \"model.decoder.layers.13.final_layer_norm.weight\", \"model.decoder.layers.13.final_layer_norm.bias\", \"model.decoder.layers.14.self_attn.k_proj.weight\", \"model.decoder.layers.14.self_attn.k_proj.bias\", \"model.decoder.layers.14.self_attn.v_proj.weight\", \"model.decoder.layers.14.self_attn.v_proj.bias\", \"model.decoder.layers.14.self_attn.q_proj.weight\", \"model.decoder.layers.14.self_attn.q_proj.bias\", \"model.decoder.layers.14.self_attn.out_proj.weight\", \"model.decoder.layers.14.self_attn.out_proj.bias\", \"model.decoder.layers.14.self_attn_layer_norm.weight\", \"model.decoder.layers.14.self_attn_layer_norm.bias\", \"model.decoder.layers.14.fc1.weight\", \"model.decoder.layers.14.fc1.bias\", \"model.decoder.layers.14.fc2.weight\", \"model.decoder.layers.14.fc2.bias\", \"model.decoder.layers.14.final_layer_norm.weight\", \"model.decoder.layers.14.final_layer_norm.bias\", \"model.decoder.layers.15.self_attn.k_proj.weight\", \"model.decoder.layers.15.self_attn.k_proj.bias\", \"model.decoder.layers.15.self_attn.v_proj.weight\", \"model.decoder.layers.15.self_attn.v_proj.bias\", \"model.decoder.layers.15.self_attn.q_proj.weight\", \"model.decoder.layers.15.self_attn.q_proj.bias\", \"model.decoder.layers.15.self_attn.out_proj.weight\", \"model.decoder.layers.15.self_attn.out_proj.bias\", \"model.decoder.layers.15.self_attn_layer_norm.weight\", \"model.decoder.layers.15.self_attn_layer_norm.bias\", \"model.decoder.layers.15.fc1.weight\", \"model.decoder.layers.15.fc1.bias\", \"model.decoder.layers.15.fc2.weight\", \"model.decoder.layers.15.fc2.bias\", \"model.decoder.layers.15.final_layer_norm.weight\", \"model.decoder.layers.15.final_layer_norm.bias\", \"model.decoder.layers.16.self_attn.k_proj.weight\", \"model.decoder.layers.16.self_attn.k_proj.bias\", \"model.decoder.layers.16.self_attn.v_proj.weight\", \"model.decoder.layers.16.self_attn.v_proj.bias\", \"model.decoder.layers.16.self_attn.q_proj.weight\", \"model.decoder.layers.16.self_attn.q_proj.bias\", \"model.decoder.layers.16.self_attn.out_proj.weight\", \"model.decoder.layers.16.self_attn.out_proj.bias\", \"model.decoder.layers.16.self_attn_layer_norm.weight\", \"model.decoder.layers.16.self_attn_layer_norm.bias\", \"model.decoder.layers.16.fc1.weight\", \"model.decoder.layers.16.fc1.bias\", \"model.decoder.layers.16.fc2.weight\", \"model.decoder.layers.16.fc2.bias\", \"model.decoder.layers.16.final_layer_norm.weight\", \"model.decoder.layers.16.final_layer_norm.bias\", \"model.decoder.layers.17.self_attn.k_proj.weight\", \"model.decoder.layers.17.self_attn.k_proj.bias\", \"model.decoder.layers.17.self_attn.v_proj.weight\", \"model.decoder.layers.17.self_attn.v_proj.bias\", \"model.decoder.layers.17.self_attn.q_proj.weight\", \"model.decoder.layers.17.self_attn.q_proj.bias\", \"model.decoder.layers.17.self_attn.out_proj.weight\", \"model.decoder.layers.17.self_attn.out_proj.bias\", \"model.decoder.layers.17.self_attn_layer_norm.weight\", \"model.decoder.layers.17.self_attn_layer_norm.bias\", \"model.decoder.layers.17.fc1.weight\", \"model.decoder.layers.17.fc1.bias\", \"model.decoder.layers.17.fc2.weight\", \"model.decoder.layers.17.fc2.bias\", \"model.decoder.layers.17.final_layer_norm.weight\", \"model.decoder.layers.17.final_layer_norm.bias\", \"model.decoder.layers.18.self_attn.k_proj.weight\", \"model.decoder.layers.18.self_attn.k_proj.bias\", \"model.decoder.layers.18.self_attn.v_proj.weight\", \"model.decoder.layers.18.self_attn.v_proj.bias\", \"model.decoder.layers.18.self_attn.q_proj.weight\", \"model.decoder.layers.18.self_attn.q_proj.bias\", \"model.decoder.layers.18.self_attn.out_proj.weight\", \"model.decoder.layers.18.self_attn.out_proj.bias\", \"model.decoder.layers.18.self_attn_layer_norm.weight\", \"model.decoder.layers.18.self_attn_layer_norm.bias\", \"model.decoder.layers.18.fc1.weight\", \"model.decoder.layers.18.fc1.bias\", \"model.decoder.layers.18.fc2.weight\", \"model.decoder.layers.18.fc2.bias\", \"model.decoder.layers.18.final_layer_norm.weight\", \"model.decoder.layers.18.final_layer_norm.bias\", \"model.decoder.layers.19.self_attn.k_proj.weight\", \"model.decoder.layers.19.self_attn.k_proj.bias\", \"model.decoder.layers.19.self_attn.v_proj.weight\", \"model.decoder.layers.19.self_attn.v_proj.bias\", \"model.decoder.layers.19.self_attn.q_proj.weight\", \"model.decoder.layers.19.self_attn.q_proj.bias\", \"model.decoder.layers.19.self_attn.out_proj.weight\", \"model.decoder.layers.19.self_attn.out_proj.bias\", \"model.decoder.layers.19.self_attn_layer_norm.weight\", \"model.decoder.layers.19.self_attn_layer_norm.bias\", \"model.decoder.layers.19.fc1.weight\", \"model.decoder.layers.19.fc1.bias\", \"model.decoder.layers.19.fc2.weight\", \"model.decoder.layers.19.fc2.bias\", \"model.decoder.layers.19.final_layer_norm.weight\", \"model.decoder.layers.19.final_layer_norm.bias\", \"model.decoder.layers.20.self_attn.k_proj.weight\", \"model.decoder.layers.20.self_attn.k_proj.bias\", \"model.decoder.layers.20.self_attn.v_proj.weight\", \"model.decoder.layers.20.self_attn.v_proj.bias\", \"model.decoder.layers.20.self_attn.q_proj.weight\", \"model.decoder.layers.20.self_attn.q_proj.bias\", \"model.decoder.layers.20.self_attn.out_proj.weight\", \"model.decoder.layers.20.self_attn.out_proj.bias\", \"model.decoder.layers.20.self_attn_layer_norm.weight\", \"model.decoder.layers.20.self_attn_layer_norm.bias\", \"model.decoder.layers.20.fc1.weight\", \"model.decoder.layers.20.fc1.bias\", \"model.decoder.layers.20.fc2.weight\", \"model.decoder.layers.20.fc2.bias\", \"model.decoder.layers.20.final_layer_norm.weight\", \"model.decoder.layers.20.final_layer_norm.bias\", \"model.decoder.layers.21.self_attn.k_proj.weight\", \"model.decoder.layers.21.self_attn.k_proj.bias\", \"model.decoder.layers.21.self_attn.v_proj.weight\", \"model.decoder.layers.21.self_attn.v_proj.bias\", \"model.decoder.layers.21.self_attn.q_proj.weight\", \"model.decoder.layers.21.self_attn.q_proj.bias\", \"model.decoder.layers.21.self_attn.out_proj.weight\", \"model.decoder.layers.21.self_attn.out_proj.bias\", \"model.decoder.layers.21.self_attn_layer_norm.weight\", \"model.decoder.layers.21.self_attn_layer_norm.bias\", \"model.decoder.layers.21.fc1.weight\", \"model.decoder.layers.21.fc1.bias\", \"model.decoder.layers.21.fc2.weight\", \"model.decoder.layers.21.fc2.bias\", \"model.decoder.layers.21.final_layer_norm.weight\", \"model.decoder.layers.21.final_layer_norm.bias\", \"model.decoder.layers.22.self_attn.k_proj.weight\", \"model.decoder.layers.22.self_attn.k_proj.bias\", \"model.decoder.layers.22.self_attn.v_proj.weight\", \"model.decoder.layers.22.self_attn.v_proj.bias\", \"model.decoder.layers.22.self_attn.q_proj.weight\", \"model.decoder.layers.22.self_attn.q_proj.bias\", \"model.decoder.layers.22.self_attn.out_proj.weight\", \"model.decoder.layers.22.self_attn.out_proj.bias\", \"model.decoder.layers.22.self_attn_layer_norm.weight\", \"model.decoder.layers.22.self_attn_layer_norm.bias\", \"model.decoder.layers.22.fc1.weight\", \"model.decoder.layers.22.fc1.bias\", \"model.decoder.layers.22.fc2.weight\", \"model.decoder.layers.22.fc2.bias\", \"model.decoder.layers.22.final_layer_norm.weight\", \"model.decoder.layers.22.final_layer_norm.bias\", \"model.decoder.layers.23.self_attn.k_proj.weight\", \"model.decoder.layers.23.self_attn.k_proj.bias\", \"model.decoder.layers.23.self_attn.v_proj.weight\", \"model.decoder.layers.23.self_attn.v_proj.bias\", \"model.decoder.layers.23.self_attn.q_proj.weight\", \"model.decoder.layers.23.self_attn.q_proj.bias\", \"model.decoder.layers.23.self_attn.out_proj.weight\", \"model.decoder.layers.23.self_attn.out_proj.bias\", \"model.decoder.layers.23.self_attn_layer_norm.weight\", \"model.decoder.layers.23.self_attn_layer_norm.bias\", \"model.decoder.layers.23.fc1.weight\", \"model.decoder.layers.23.fc1.bias\", \"model.decoder.layers.23.fc2.weight\", \"model.decoder.layers.23.fc2.bias\", \"model.decoder.layers.23.final_layer_norm.weight\", \"model.decoder.layers.23.final_layer_norm.bias\", \"lm_head.weight\". \n\tUnexpected key(s) in state_dict: \"encoder_hash_tok_embedding.0.weight\", \"encoder_hash_tok_embedding.1.weight\", \"encoder_hash_tok_embedding.2.weight\", \"encoder_hash_tok_embedding.3.weight\", \"encoder_hash_tok_embedding.4.weight\", \"encoder_hash_tok_embedding.5.weight\", \"global_transformer.layers.0.attention.wk.weight\", \"global_transformer.layers.0.attention.wo.weight\", \"global_transformer.layers.0.attention.wq.weight\", \"global_transformer.layers.0.attention.wv.weight\", \"global_transformer.layers.0.attention_norm.weight\", \"global_transformer.layers.0.feed_forward.w1.weight\", \"global_transformer.layers.0.feed_forward.w2.weight\", \"global_transformer.layers.0.feed_forward.w3.weight\", \"global_transformer.layers.0.ffn_norm.weight\", \"global_transformer.layers.1.attention.wk.weight\", \"global_transformer.layers.1.attention.wo.weight\", \"global_transformer.layers.1.attention.wq.weight\", \"global_transformer.layers.1.attention.wv.weight\", \"global_transformer.layers.1.attention_norm.weight\", \"global_transformer.layers.1.feed_forward.w1.weight\", \"global_transformer.layers.1.feed_forward.w2.weight\", \"global_transformer.layers.1.feed_forward.w3.weight\", \"global_transformer.layers.1.ffn_norm.weight\", \"global_transformer.layers.10.attention.wk.weight\", \"global_transformer.layers.10.attention.wo.weight\", \"global_transformer.layers.10.attention.wq.weight\", \"global_transformer.layers.10.attention.wv.weight\", \"global_transformer.layers.10.attention_norm.weight\", \"global_transformer.layers.10.feed_forward.w1.weight\", \"global_transformer.layers.10.feed_forward.w2.weight\", \"global_transformer.layers.10.feed_forward.w3.weight\", \"global_transformer.layers.10.ffn_norm.weight\", \"global_transformer.layers.11.attention.wk.weight\", \"global_transformer.layers.11.attention.wo.weight\", \"global_transformer.layers.11.attention.wq.weight\", \"global_transformer.layers.11.attention.wv.weight\", \"global_transformer.layers.11.attention_norm.weight\", \"global_transformer.layers.11.feed_forward.w1.weight\", \"global_transformer.layers.11.feed_forward.w2.weight\", \"global_transformer.layers.11.feed_forward.w3.weight\", \"global_transformer.layers.11.ffn_norm.weight\", \"global_transformer.layers.12.attention.wk.weight\", \"global_transformer.layers.12.attention.wo.weight\", \"global_transformer.layers.12.attention.wq.weight\", \"global_transformer.layers.12.attention.wv.weight\", \"global_transformer.layers.12.attention_norm.weight\", \"global_transformer.layers.12.feed_forward.w1.weight\", \"global_transformer.layers.12.feed_forward.w2.weight\", \"global_transformer.layers.12.feed_forward.w3.weight\", \"global_transformer.layers.12.ffn_norm.weight\", \"global_transformer.layers.13.attention.wk.weight\", \"global_transformer.layers.13.attention.wo.weight\", \"global_transformer.layers.13.attention.wq.weight\", \"global_transformer.layers.13.attention.wv.weight\", \"global_transformer.layers.13.attention_norm.weight\", \"global_transformer.layers.13.feed_forward.w1.weight\", \"global_transformer.layers.13.feed_forward.w2.weight\", \"global_transformer.layers.13.feed_forward.w3.weight\", \"global_transformer.layers.13.ffn_norm.weight\", \"global_transformer.layers.14.attention.wk.weight\", \"global_transformer.layers.14.attention.wo.weight\", \"global_transformer.layers.14.attention.wq.weight\", \"global_transformer.layers.14.attention.wv.weight\", \"global_transformer.layers.14.attention_norm.weight\", \"global_transformer.layers.14.feed_forward.w1.weight\", \"global_transformer.layers.14.feed_forward.w2.weight\", \"global_transformer.layers.14.feed_forward.w3.weight\", \"global_transformer.layers.14.ffn_norm.weight\", \"global_transformer.layers.15.attention.wk.weight\", \"global_transformer.layers.15.attention.wo.weight\", \"global_transformer.layers.15.attention.wq.weight\", \"global_transformer.layers.15.attention.wv.weight\", \"global_transformer.layers.15.attention_norm.weight\", \"global_transformer.layers.15.feed_forward.w1.weight\", \"global_transformer.layers.15.feed_forward.w2.weight\", \"global_transformer.layers.15.feed_forward.w3.weight\", \"global_transformer.layers.15.ffn_norm.weight\", \"global_transformer.layers.16.attention.wk.weight\", \"global_transformer.layers.16.attention.wo.weight\", \"global_transformer.layers.16.attention.wq.weight\", \"global_transformer.layers.16.attention.wv.weight\", \"global_transformer.layers.16.attention_norm.weight\", \"global_transformer.layers.16.feed_forward.w1.weight\", \"global_transformer.layers.16.feed_forward.w2.weight\", \"global_transformer.layers.16.feed_forward.w3.weight\", \"global_transformer.layers.16.ffn_norm.weight\", \"global_transformer.layers.17.attention.wk.weight\", \"global_transformer.layers.17.attention.wo.weight\", \"global_transformer.layers.17.attention.wq.weight\", \"global_transformer.layers.17.attention.wv.weight\", \"global_transformer.layers.17.attention_norm.weight\", \"global_transformer.layers.17.feed_forward.w1.weight\", \"global_transformer.layers.17.feed_forward.w2.weight\", \"global_transformer.layers.17.feed_forward.w3.weight\", \"global_transformer.layers.17.ffn_norm.weight\", \"global_transformer.layers.18.attention.wk.weight\", \"global_transformer.layers.18.attention.wo.weight\", \"global_transformer.layers.18.attention.wq.weight\", \"global_transformer.layers.18.attention.wv.weight\", \"global_transformer.layers.18.attention_norm.weight\", \"global_transformer.layers.18.feed_forward.w1.weight\", \"global_transformer.layers.18.feed_forward.w2.weight\", \"global_transformer.layers.18.feed_forward.w3.weight\", \"global_transformer.layers.18.ffn_norm.weight\", \"global_transformer.layers.19.attention.wk.weight\", \"global_transformer.layers.19.attention.wo.weight\", \"global_transformer.layers.19.attention.wq.weight\", \"global_transformer.layers.19.attention.wv.weight\", \"global_transformer.layers.19.attention_norm.weight\", \"global_transformer.layers.19.feed_forward.w1.weight\", \"global_transformer.layers.19.feed_forward.w2.weight\", \"global_transformer.layers.19.feed_forward.w3.weight\", \"global_transformer.layers.19.ffn_norm.weight\", \"global_transformer.layers.2.attention.wk.weight\", \"global_transformer.layers.2.attention.wo.weight\", \"global_transformer.layers.2.attention.wq.weight\", \"global_transformer.layers.2.attention.wv.weight\", \"global_transformer.layers.2.attention_norm.weight\", \"global_transformer.layers.2.feed_forward.w1.weight\", \"global_transformer.layers.2.feed_forward.w2.weight\", \"global_transformer.layers.2.feed_forward.w3.weight\", \"global_transformer.layers.2.ffn_norm.weight\", \"global_transformer.layers.20.attention.wk.weight\", \"global_transformer.layers.20.attention.wo.weight\", \"global_transformer.layers.20.attention.wq.weight\", \"global_transformer.layers.20.attention.wv.weight\", \"global_transformer.layers.20.attention_norm.weight\", \"global_transformer.layers.20.feed_forward.w1.weight\", \"global_transformer.layers.20.feed_forward.w2.weight\", \"global_transformer.layers.20.feed_forward.w3.weight\", \"global_transformer.layers.20.ffn_norm.weight\", \"global_transformer.layers.21.attention.wk.weight\", \"global_transformer.layers.21.attention.wo.weight\", \"global_transformer.layers.21.attention.wq.weight\", \"global_transformer.layers.21.attention.wv.weight\", \"global_transformer.layers.21.attention_norm.weight\", \"global_transformer.layers.21.feed_forward.w1.weight\", \"global_transformer.layers.21.feed_forward.w2.weight\", \"global_transformer.layers.21.feed_forward.w3.weight\", \"global_transformer.layers.21.ffn_norm.weight\", \"global_transformer.layers.22.attention.wk.weight\", \"global_transformer.layers.22.attention.wo.weight\", \"global_transformer.layers.22.attention.wq.weight\", \"global_transformer.layers.22.attention.wv.weight\", \"global_transformer.layers.22.attention_norm.weight\", \"global_transformer.layers.22.feed_forward.w1.weight\", \"global_transformer.layers.22.feed_forward.w2.weight\", \"global_transformer.layers.22.feed_forward.w3.weight\", \"global_transformer.layers.22.ffn_norm.weight\", \"global_transformer.layers.23.attention.wk.weight\", \"global_transformer.layers.23.attention.wo.weight\", \"global_transformer.layers.23.attention.wq.weight\", \"global_transformer.layers.23.attention.wv.weight\", \"global_transformer.layers.23.attention_norm.weight\", \"global_transformer.layers.23.feed_forward.w1.weight\", \"global_transformer.layers.23.feed_forward.w2.weight\", \"global_transformer.layers.23.feed_forward.w3.weight\", \"global_transformer.layers.23.ffn_norm.weight\", \"global_transformer.layers.24.attention.wk.weight\", \"global_transformer.layers.24.attention.wo.weight\", \"global_transformer.layers.24.attention.wq.weight\", \"global_transformer.layers.24.attention.wv.weight\", \"global_transformer.layers.24.attention_norm.weight\", \"global_transformer.layers.24.feed_forward.w1.weight\", \"global_transformer.layers.24.feed_forward.w2.weight\", \"global_transformer.layers.24.feed_forward.w3.weight\", \"global_transformer.layers.24.ffn_norm.weight\", \"global_transformer.layers.3.attention.wk.weight\", \"global_transformer.layers.3.attention.wo.weight\", \"global_transformer.layers.3.attention.wq.weight\", \"global_transformer.layers.3.attention.wv.weight\", \"global_transformer.layers.3.attention_norm.weight\", \"global_transformer.layers.3.feed_forward.w1.weight\", \"global_transformer.layers.3.feed_forward.w2.weight\", \"global_transformer.layers.3.feed_forward.w3.weight\", \"global_transformer.layers.3.ffn_norm.weight\", \"global_transformer.layers.4.attention.wk.weight\", \"global_transformer.layers.4.attention.wo.weight\", \"global_transformer.layers.4.attention.wq.weight\", \"global_transformer.layers.4.attention.wv.weight\", \"global_transformer.layers.4.attention_norm.weight\", \"global_transformer.layers.4.feed_forward.w1.weight\", \"global_transformer.layers.4.feed_forward.w2.weight\", \"global_transformer.layers.4.feed_forward.w3.weight\", \"global_transformer.layers.4.ffn_norm.weight\", \"global_transformer.layers.5.attention.wk.weight\", \"global_transformer.layers.5.attention.wo.weight\", \"global_transformer.layers.5.attention.wq.weight\", \"global_transformer.layers.5.attention.wv.weight\", \"global_transformer.layers.5.attention_norm.weight\", \"global_transformer.layers.5.feed_forward.w1.weight\", \"global_transformer.layers.5.feed_forward.w2.weight\", \"global_transformer.layers.5.feed_forward.w3.weight\", \"global_transformer.layers.5.ffn_norm.weight\", \"global_transformer.layers.6.attention.wk.weight\", \"global_transformer.layers.6.attention.wo.weight\", \"global_transformer.layers.6.attention.wq.weight\", \"global_transformer.layers.6.attention.wv.weight\", \"global_transformer.layers.6.attention_norm.weight\", \"global_transformer.layers.6.feed_forward.w1.weight\", \"global_transformer.layers.6.feed_forward.w2.weight\", \"global_transformer.layers.6.feed_forward.w3.weight\", \"global_transformer.layers.6.ffn_norm.weight\", \"global_transformer.layers.7.attention.wk.weight\", \"global_transformer.layers.7.attention.wo.weight\", \"global_transformer.layers.7.attention.wq.weight\", \"global_transformer.layers.7.attention.wv.weight\", \"global_transformer.layers.7.attention_norm.weight\", \"global_transformer.layers.7.feed_forward.w1.weight\", \"global_transformer.layers.7.feed_forward.w2.weight\", \"global_transformer.layers.7.feed_forward.w3.weight\", \"global_transformer.layers.7.ffn_norm.weight\", \"global_transformer.layers.8.attention.wk.weight\", \"global_transformer.layers.8.attention.wo.weight\", \"global_transformer.layers.8.attention.wq.weight\", \"global_transformer.layers.8.attention.wv.weight\", \"global_transformer.layers.8.attention_norm.weight\", \"global_transformer.layers.8.feed_forward.w1.weight\", \"global_transformer.layers.8.feed_forward.w2.weight\", \"global_transformer.layers.8.feed_forward.w3.weight\", \"global_transformer.layers.8.ffn_norm.weight\", \"global_transformer.layers.9.attention.wk.weight\", \"global_transformer.layers.9.attention.wo.weight\", \"global_transformer.layers.9.attention.wq.weight\", \"global_transformer.layers.9.attention.wv.weight\", \"global_transformer.layers.9.attention_norm.weight\", \"global_transformer.layers.9.feed_forward.w1.weight\", \"global_transformer.layers.9.feed_forward.w2.weight\", \"global_transformer.layers.9.feed_forward.w3.weight\", \"global_transformer.layers.9.ffn_norm.weight\", \"local_decoder.cross_attn_layers.0.cross_attn_norm_kv.weight\", \"local_decoder.cross_attn_layers.0.cross_attn_norm_q.weight\", \"local_decoder.cross_attn_layers.0.wk.weight\", \"local_decoder.cross_attn_layers.0.wo.weight\", \"local_decoder.cross_attn_layers.0.wq.weight\", \"local_decoder.cross_attn_layers.0.wv.weight\", \"local_decoder.cross_attn_layers.1.cross_attn_norm_kv.weight\", \"local_decoder.cross_attn_layers.1.cross_attn_norm_q.weight\", \"local_decoder.cross_attn_layers.1.wk.weight\", \"local_decoder.cross_attn_layers.1.wo.weight\", \"local_decoder.cross_attn_layers.1.wq.weight\", \"local_decoder.cross_attn_layers.1.wv.weight\", \"local_decoder.cross_attn_layers.2.cross_attn_norm_kv.weight\", \"local_decoder.cross_attn_layers.2.cross_attn_norm_q.weight\", \"local_decoder.cross_attn_layers.2.wk.weight\", \"local_decoder.cross_attn_layers.2.wo.weight\", \"local_decoder.cross_attn_layers.2.wq.weight\", \"local_decoder.cross_attn_layers.2.wv.weight\", \"local_decoder.cross_attn_layers.3.cross_attn_norm_kv.weight\", \"local_decoder.cross_attn_layers.3.cross_attn_norm_q.weight\", \"local_decoder.cross_attn_layers.3.wk.weight\", \"local_decoder.cross_attn_layers.3.wo.weight\", \"local_decoder.cross_attn_layers.3.wq.weight\", \"local_decoder.cross_attn_layers.3.wv.weight\", \"local_decoder.cross_attn_layers.4.cross_attn_norm_kv.weight\", \"local_decoder.cross_attn_layers.4.cross_attn_norm_q.weight\", \"local_decoder.cross_attn_layers.4.wk.weight\", \"local_decoder.cross_attn_layers.4.wo.weight\", \"local_decoder.cross_attn_layers.4.wq.weight\", \"local_decoder.cross_attn_layers.4.wv.weight\", \"local_decoder.cross_attn_layers.5.cross_attn_norm_kv.weight\", \"local_decoder.cross_attn_layers.5.cross_attn_norm_q.weight\", \"local_decoder.cross_attn_layers.5.wk.weight\", \"local_decoder.cross_attn_layers.5.wo.weight\", \"local_decoder.cross_attn_layers.5.wq.weight\", \"local_decoder.cross_attn_layers.5.wv.weight\", \"local_decoder.cross_attn_layers.6.cross_attn_norm_kv.weight\", \"local_decoder.cross_attn_layers.6.cross_attn_norm_q.weight\", \"local_decoder.cross_attn_layers.6.wk.weight\", \"local_decoder.cross_attn_layers.6.wo.weight\", \"local_decoder.cross_attn_layers.6.wq.weight\", \"local_decoder.cross_attn_layers.6.wv.weight\", \"local_decoder.cross_attn_layers.7.cross_attn_norm_kv.weight\", \"local_decoder.cross_attn_layers.7.cross_attn_norm_q.weight\", \"local_decoder.cross_attn_layers.7.wk.weight\", \"local_decoder.cross_attn_layers.7.wo.weight\", \"local_decoder.cross_attn_layers.7.wq.weight\", \"local_decoder.cross_attn_layers.7.wv.weight\", \"local_decoder.cross_attn_layers.8.cross_attn_norm_kv.weight\", \"local_decoder.cross_attn_layers.8.cross_attn_norm_q.weight\", \"local_decoder.cross_attn_layers.8.wk.weight\", \"local_decoder.cross_attn_layers.8.wo.weight\", \"local_decoder.cross_attn_layers.8.wq.weight\", \"local_decoder.cross_attn_layers.8.wv.weight\", \"local_decoder.layers.0.attention.wk.weight\", \"local_decoder.layers.0.attention.wo.weight\", \"local_decoder.layers.0.attention.wq.weight\", \"local_decoder.layers.0.attention.wv.weight\", \"local_decoder.layers.0.attention_norm.weight\", \"local_decoder.layers.0.feed_forward.w1.weight\", \"local_decoder.layers.0.feed_forward.w2.weight\", \"local_decoder.layers.0.feed_forward.w3.weight\", \"local_decoder.layers.0.ffn_norm.weight\", \"local_decoder.layers.1.attention.wk.weight\", \"local_decoder.layers.1.attention.wo.weight\", \"local_decoder.layers.1.attention.wq.weight\", \"local_decoder.layers.1.attention.wv.weight\", \"local_decoder.layers.1.attention_norm.weight\", \"local_decoder.layers.1.feed_forward.w1.weight\", \"local_decoder.layers.1.feed_forward.w2.weight\", \"local_decoder.layers.1.feed_forward.w3.weight\", \"local_decoder.layers.1.ffn_norm.weight\", \"local_decoder.layers.2.attention.wk.weight\", \"local_decoder.layers.2.attention.wo.weight\", \"local_decoder.layers.2.attention.wq.weight\", \"local_decoder.layers.2.attention.wv.weight\", \"local_decoder.layers.2.attention_norm.weight\", \"local_decoder.layers.2.feed_forward.w1.weight\", \"local_decoder.layers.2.feed_forward.w2.weight\", \"local_decoder.layers.2.feed_forward.w3.weight\", \"local_decoder.layers.2.ffn_norm.weight\", \"local_decoder.layers.3.attention.wk.weight\", \"local_decoder.layers.3.attention.wo.weight\", \"local_decoder.layers.3.attention.wq.weight\", \"local_decoder.layers.3.attention.wv.weight\", \"local_decoder.layers.3.attention_norm.weight\", \"local_decoder.layers.3.feed_forward.w1.weight\", \"local_decoder.layers.3.feed_forward.w2.weight\", \"local_decoder.layers.3.feed_forward.w3.weight\", \"local_decoder.layers.3.ffn_norm.weight\", \"local_decoder.layers.4.attention.wk.weight\", \"local_decoder.layers.4.attention.wo.weight\", \"local_decoder.layers.4.attention.wq.weight\", \"local_decoder.layers.4.attention.wv.weight\", \"local_decoder.layers.4.attention_norm.weight\", \"local_decoder.layers.4.feed_forward.w1.weight\", \"local_decoder.layers.4.feed_forward.w2.weight\", \"local_decoder.layers.4.feed_forward.w3.weight\", \"local_decoder.layers.4.ffn_norm.weight\", \"local_decoder.layers.5.attention.wk.weight\", \"local_decoder.layers.5.attention.wo.weight\", \"local_decoder.layers.5.attention.wq.weight\", \"local_decoder.layers.5.attention.wv.weight\", \"local_decoder.layers.5.attention_norm.weight\", \"local_decoder.layers.5.feed_forward.w1.weight\", \"local_decoder.layers.5.feed_forward.w2.weight\", \"local_decoder.layers.5.feed_forward.w3.weight\", \"local_decoder.layers.5.ffn_norm.weight\", \"local_decoder.layers.6.attention.wk.weight\", \"local_decoder.layers.6.attention.wo.weight\", \"local_decoder.layers.6.attention.wq.weight\", \"local_decoder.layers.6.attention.wv.weight\", \"local_decoder.layers.6.attention_norm.weight\", \"local_decoder.layers.6.feed_forward.w1.weight\", \"local_decoder.layers.6.feed_forward.w2.weight\", \"local_decoder.layers.6.feed_forward.w3.weight\", \"local_decoder.layers.6.ffn_norm.weight\", \"local_decoder.layers.7.attention.wk.weight\", \"local_decoder.layers.7.attention.wo.weight\", \"local_decoder.layers.7.attention.wq.weight\", \"local_decoder.layers.7.attention.wv.weight\", \"local_decoder.layers.7.attention_norm.weight\", \"local_decoder.layers.7.feed_forward.w1.weight\", \"local_decoder.layers.7.feed_forward.w2.weight\", \"local_decoder.layers.7.feed_forward.w3.weight\", \"local_decoder.layers.7.ffn_norm.weight\", \"local_decoder.layers.8.attention.wk.weight\", \"local_decoder.layers.8.attention.wo.weight\", \"local_decoder.layers.8.attention.wq.weight\", \"local_decoder.layers.8.attention.wv.weight\", \"local_decoder.layers.8.attention_norm.weight\", \"local_decoder.layers.8.feed_forward.w1.weight\", \"local_decoder.layers.8.feed_forward.w2.weight\", \"local_decoder.layers.8.feed_forward.w3.weight\", \"local_decoder.layers.8.ffn_norm.weight\", \"local_decoder.norm.weight\", \"local_decoder.output.weight\", \"local_decoder.patch_embedding_projection.weight\", \"local_encoder.cross_attn_layers.0.cross_attn_norm_kv.weight\", \"local_encoder.cross_attn_layers.0.cross_attn_norm_q.weight\", \"local_encoder.cross_attn_layers.0.wk.weight\", \"local_encoder.cross_attn_layers.0.wo.weight\", \"local_encoder.cross_attn_layers.0.wq.weight\", \"local_encoder.cross_attn_layers.0.wv.weight\", \"local_encoder.layers.0.attention.wk.weight\", \"local_encoder.layers.0.attention.wo.weight\", \"local_encoder.layers.0.attention.wq.weight\", \"local_encoder.layers.0.attention.wv.weight\", \"local_encoder.layers.0.attention_norm.weight\", \"local_encoder.layers.0.feed_forward.w1.weight\", \"local_encoder.layers.0.feed_forward.w2.weight\", \"local_encoder.layers.0.feed_forward.w3.weight\", \"local_encoder.layers.0.ffn_norm.weight\", \"local_encoder.patch_embedding_projection.weight\", \"local_encoder.tok_embeddings.weight\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ec3e2517a4f7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Now load the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for OPTForCausalLM:\n\tMissing key(s) in state_dict: \"model.decoder.embed_tokens.weight\", \"model.decoder.embed_positions.weight\", \"model.decoder.project_out.weight\", \"model.decoder.project_in.weight\", \"model.decoder.layers.0.self_attn.k_proj.weight\", \"model.decoder.layers.0.self_attn.k_proj.bias\", \"model.decoder.layers.0.self_attn.v_proj.weight\", \"model.decoder.layers.0.self_attn.v_proj.bias\", \"model.decoder.layers.0.self_attn.q_proj.weight\", \"model.decoder.layers.0.self_attn.q_proj.bias\", \"model.decoder.layers.0.self_attn.out_proj.weight\", \"model.decoder.layers.0.self_attn.out_proj.bias\", \"model.decoder.layers.0.self_attn_layer_norm.weight\", \"model.decoder.layers.0.self_attn_layer_norm.bias\", \"model.decoder.layers.0.fc1.weight\", \"model.decoder.layers.0.fc1.bias\", \"model.decoder.layers.0.fc2.weight\", \"model.decoder.layers.0.fc2.bias\", \"model.decoder.layers.0.final_layer_norm.weight\", \"model.decoder.layers.0.final_layer_norm.bias\", \"model.decoder.layers.1.self_attn.k_proj.weight\", \"model.decoder.layers.1.self_attn.k_proj.bias\", \"model.decoder.layers.1.self_attn.v_proj.weight\", \"model.decoder.layers.1.self_attn.v_proj.bias\", \"model.decoder.layers.1.self_attn.q_proj.weight\", \"model.decoder.layers.1.self_attn.q_proj.bias\", \"model.decoder.layers.1.self_attn.out_proj.weight\", \"model.decoder.layers.1.self_attn.out_proj.bias\", \"model.decoder.layers.1.self_attn_layer_norm.weight\", \"model.decoder.layers.1.self_attn_layer_norm.bias\", \"model.decoder.layers.1.fc1.weight\", \"model.decoder.layers.1.fc1.bias\", ...\n\tUnexpected key(s) in state_dict: \"encoder_hash_tok_embedding.0.weight\", \"encoder_hash_tok_embedding.1.weight\", \"encoder_hash_tok_embedding.2.weight\", \"encoder_hash_tok_embedding.3.weight\", \"encoder_hash_tok_embedding.4.weight\", \"encoder_hash_tok_embedding.5.weight\", \"global_transformer.layers.0.attention.wk.weight\", \"global_transformer.layers.0.attention.wo.weight\", \"global_transformer.layers.0.attention.wq.weight\", \"global_transformer.layers.0.attention.wv.weight\", \"global_transformer.layers.0.attention_norm.weight\", \"global_transformer.layers.0.feed_forward.w1.weight\", \"global_transformer.layers.0.feed_forward.w2.weight\", \"global_transformer.layers.0.feed_forward.w3.weight\", \"global_transformer.layers.0.ffn_norm.weight\", \"global_transformer.layers.1.attention.wk.weight\", \"global_transformer.layers.1.attention.wo.weight\", \"global_transformer.layers.1.attention.wq.weight\", \"global_transformer.layers.1.attention.wv.weight\", \"global_transformer.layers.1.attention_norm.weight\", \"global_transformer.layers.1.feed_forward.w1.weight\", \"global_transformer.layers.1.feed_forward.w2.weight\", \"global_transformer.layers.1.feed_forward.w3.weight\", \"global_transformer.layers.1.ffn_norm.weight\", \"global_transformer.layers.10.attention.wk.weight\", \"global_transformer.layers.10.attention.wo.weight\", \"global_transformer.layers.10.attention.wq.weight\", \"global_transformer.layers.10.attention.wv.weight\", \"global_transformer.layers.10.attention_norm.weight\", \"global_transformer.layers.10.feed_..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSz7MYFZxAPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BloomForCausalLM\n",
        "\n",
        "# Load BLT-1B model\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Define and initialize the model using BloomForCausalLM\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "\n",
        "# Now load the weights, using strict=False to ignore mismatched keys\n",
        "model.load_state_dict(model_weights, strict=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "77f471e5fd7740dbb24be36fc5454f47",
            "bcfda591b71f4bbdbe1b5b42c7249f4e",
            "7029c1cf5f334a23858b3c327ec6d092",
            "dbc088d308f3490fae85fb36f300caf7",
            "00a74ba2c8cf4023b39648e9c1c4d8fb",
            "6843d3f27ea84ad8bc484bfb692c57f1",
            "5af922a87bf1446499f6ca851c961c6e",
            "03f04dc8f6874d83917465cc54fe5340",
            "aece4236eb144e00a8dbefc08ce40cf4",
            "69fbc99b54914aad83634b4ddc3dcc98",
            "5b1343ad068947809b857848aaf9818e",
            "eb95701d0d8146ada10dbb3de7cda519",
            "ceeadcf920eb4c51861d8c865affdb60",
            "de294bfbba604d68a739db3b72e73379",
            "14b6964c771646f38f00915a1164df17",
            "205bb7b50e0f4c27998a3cbaf4ec1ef8",
            "230fc155266440df97d51b63d7e6432e",
            "82a6d34c456d4a04b5513f627b4e4e9c",
            "929090e773f8432ab9af59420235c08b",
            "4221dd8b0bed410ba8c88123d25b0dc5",
            "3f4fbf0e14d54867bda9f49ef8cc2d2b",
            "d490dc4551684169beced0f6fbfebe97"
          ]
        },
        "id": "EyAyrawRxAnf",
        "outputId": "8b8baaa4-b3bd-47d2-d1bb-6a7b187f764e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77f471e5fd7740dbb24be36fc5454f47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb95701d0d8146ada10dbb3de7cda519"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['transformer.word_embeddings.weight', 'transformer.word_embeddings_layernorm.weight', 'transformer.word_embeddings_layernorm.bias', 'transformer.h.0.input_layernorm.weight', 'transformer.h.0.input_layernorm.bias', 'transformer.h.0.self_attention.query_key_value.weight', 'transformer.h.0.self_attention.query_key_value.bias', 'transformer.h.0.self_attention.dense.weight', 'transformer.h.0.self_attention.dense.bias', 'transformer.h.0.post_attention_layernorm.weight', 'transformer.h.0.post_attention_layernorm.bias', 'transformer.h.0.mlp.dense_h_to_4h.weight', 'transformer.h.0.mlp.dense_h_to_4h.bias', 'transformer.h.0.mlp.dense_4h_to_h.weight', 'transformer.h.0.mlp.dense_4h_to_h.bias', 'transformer.h.1.input_layernorm.weight', 'transformer.h.1.input_layernorm.bias', 'transformer.h.1.self_attention.query_key_value.weight', 'transformer.h.1.self_attention.query_key_value.bias', 'transformer.h.1.self_attention.dense.weight', 'transformer.h.1.self_attention.dense.bias', 'transformer.h.1.post_attention_layernorm.weight', 'transformer.h.1.post_attention_layernorm.bias', 'transformer.h.1.mlp.dense_h_to_4h.weight', 'transformer.h.1.mlp.dense_h_to_4h.bias', 'transformer.h.1.mlp.dense_4h_to_h.weight', 'transformer.h.1.mlp.dense_4h_to_h.bias', 'transformer.h.2.input_layernorm.weight', 'transformer.h.2.input_layernorm.bias', 'transformer.h.2.self_attention.query_key_value.weight', 'transformer.h.2.self_attention.query_key_value.bias', 'transformer.h.2.self_attention.dense.weight', 'transformer.h.2.self_attention.dense.bias', 'transformer.h.2.post_attention_layernorm.weight', 'transformer.h.2.post_attention_layernorm.bias', 'transformer.h.2.mlp.dense_h_to_4h.weight', 'transformer.h.2.mlp.dense_h_to_4h.bias', 'transformer.h.2.mlp.dense_4h_to_h.weight', 'transformer.h.2.mlp.dense_4h_to_h.bias', 'transformer.h.3.input_layernorm.weight', 'transformer.h.3.input_layernorm.bias', 'transformer.h.3.self_attention.query_key_value.weight', 'transformer.h.3.self_attention.query_key_value.bias', 'transformer.h.3.self_attention.dense.weight', 'transformer.h.3.self_attention.dense.bias', 'transformer.h.3.post_attention_layernorm.weight', 'transformer.h.3.post_attention_layernorm.bias', 'transformer.h.3.mlp.dense_h_to_4h.weight', 'transformer.h.3.mlp.dense_h_to_4h.bias', 'transformer.h.3.mlp.dense_4h_to_h.weight', 'transformer.h.3.mlp.dense_4h_to_h.bias', 'transformer.h.4.input_layernorm.weight', 'transformer.h.4.input_layernorm.bias', 'transformer.h.4.self_attention.query_key_value.weight', 'transformer.h.4.self_attention.query_key_value.bias', 'transformer.h.4.self_attention.dense.weight', 'transformer.h.4.self_attention.dense.bias', 'transformer.h.4.post_attention_layernorm.weight', 'transformer.h.4.post_attention_layernorm.bias', 'transformer.h.4.mlp.dense_h_to_4h.weight', 'transformer.h.4.mlp.dense_h_to_4h.bias', 'transformer.h.4.mlp.dense_4h_to_h.weight', 'transformer.h.4.mlp.dense_4h_to_h.bias', 'transformer.h.5.input_layernorm.weight', 'transformer.h.5.input_layernorm.bias', 'transformer.h.5.self_attention.query_key_value.weight', 'transformer.h.5.self_attention.query_key_value.bias', 'transformer.h.5.self_attention.dense.weight', 'transformer.h.5.self_attention.dense.bias', 'transformer.h.5.post_attention_layernorm.weight', 'transformer.h.5.post_attention_layernorm.bias', 'transformer.h.5.mlp.dense_h_to_4h.weight', 'transformer.h.5.mlp.dense_h_to_4h.bias', 'transformer.h.5.mlp.dense_4h_to_h.weight', 'transformer.h.5.mlp.dense_4h_to_h.bias', 'transformer.h.6.input_layernorm.weight', 'transformer.h.6.input_layernorm.bias', 'transformer.h.6.self_attention.query_key_value.weight', 'transformer.h.6.self_attention.query_key_value.bias', 'transformer.h.6.self_attention.dense.weight', 'transformer.h.6.self_attention.dense.bias', 'transformer.h.6.post_attention_layernorm.weight', 'transformer.h.6.post_attention_layernorm.bias', 'transformer.h.6.mlp.dense_h_to_4h.weight', 'transformer.h.6.mlp.dense_h_to_4h.bias', 'transformer.h.6.mlp.dense_4h_to_h.weight', 'transformer.h.6.mlp.dense_4h_to_h.bias', 'transformer.h.7.input_layernorm.weight', 'transformer.h.7.input_layernorm.bias', 'transformer.h.7.self_attention.query_key_value.weight', 'transformer.h.7.self_attention.query_key_value.bias', 'transformer.h.7.self_attention.dense.weight', 'transformer.h.7.self_attention.dense.bias', 'transformer.h.7.post_attention_layernorm.weight', 'transformer.h.7.post_attention_layernorm.bias', 'transformer.h.7.mlp.dense_h_to_4h.weight', 'transformer.h.7.mlp.dense_h_to_4h.bias', 'transformer.h.7.mlp.dense_4h_to_h.weight', 'transformer.h.7.mlp.dense_4h_to_h.bias', 'transformer.h.8.input_layernorm.weight', 'transformer.h.8.input_layernorm.bias', 'transformer.h.8.self_attention.query_key_value.weight', 'transformer.h.8.self_attention.query_key_value.bias', 'transformer.h.8.self_attention.dense.weight', 'transformer.h.8.self_attention.dense.bias', 'transformer.h.8.post_attention_layernorm.weight', 'transformer.h.8.post_attention_layernorm.bias', 'transformer.h.8.mlp.dense_h_to_4h.weight', 'transformer.h.8.mlp.dense_h_to_4h.bias', 'transformer.h.8.mlp.dense_4h_to_h.weight', 'transformer.h.8.mlp.dense_4h_to_h.bias', 'transformer.h.9.input_layernorm.weight', 'transformer.h.9.input_layernorm.bias', 'transformer.h.9.self_attention.query_key_value.weight', 'transformer.h.9.self_attention.query_key_value.bias', 'transformer.h.9.self_attention.dense.weight', 'transformer.h.9.self_attention.dense.bias', 'transformer.h.9.post_attention_layernorm.weight', 'transformer.h.9.post_attention_layernorm.bias', 'transformer.h.9.mlp.dense_h_to_4h.weight', 'transformer.h.9.mlp.dense_h_to_4h.bias', 'transformer.h.9.mlp.dense_4h_to_h.weight', 'transformer.h.9.mlp.dense_4h_to_h.bias', 'transformer.h.10.input_layernorm.weight', 'transformer.h.10.input_layernorm.bias', 'transformer.h.10.self_attention.query_key_value.weight', 'transformer.h.10.self_attention.query_key_value.bias', 'transformer.h.10.self_attention.dense.weight', 'transformer.h.10.self_attention.dense.bias', 'transformer.h.10.post_attention_layernorm.weight', 'transformer.h.10.post_attention_layernorm.bias', 'transformer.h.10.mlp.dense_h_to_4h.weight', 'transformer.h.10.mlp.dense_h_to_4h.bias', 'transformer.h.10.mlp.dense_4h_to_h.weight', 'transformer.h.10.mlp.dense_4h_to_h.bias', 'transformer.h.11.input_layernorm.weight', 'transformer.h.11.input_layernorm.bias', 'transformer.h.11.self_attention.query_key_value.weight', 'transformer.h.11.self_attention.query_key_value.bias', 'transformer.h.11.self_attention.dense.weight', 'transformer.h.11.self_attention.dense.bias', 'transformer.h.11.post_attention_layernorm.weight', 'transformer.h.11.post_attention_layernorm.bias', 'transformer.h.11.mlp.dense_h_to_4h.weight', 'transformer.h.11.mlp.dense_h_to_4h.bias', 'transformer.h.11.mlp.dense_4h_to_h.weight', 'transformer.h.11.mlp.dense_4h_to_h.bias', 'transformer.h.12.input_layernorm.weight', 'transformer.h.12.input_layernorm.bias', 'transformer.h.12.self_attention.query_key_value.weight', 'transformer.h.12.self_attention.query_key_value.bias', 'transformer.h.12.self_attention.dense.weight', 'transformer.h.12.self_attention.dense.bias', 'transformer.h.12.post_attention_layernorm.weight', 'transformer.h.12.post_attention_layernorm.bias', 'transformer.h.12.mlp.dense_h_to_4h.weight', 'transformer.h.12.mlp.dense_h_to_4h.bias', 'transformer.h.12.mlp.dense_4h_to_h.weight', 'transformer.h.12.mlp.dense_4h_to_h.bias', 'transformer.h.13.input_layernorm.weight', 'transformer.h.13.input_layernorm.bias', 'transformer.h.13.self_attention.query_key_value.weight', 'transformer.h.13.self_attention.query_key_value.bias', 'transformer.h.13.self_attention.dense.weight', 'transformer.h.13.self_attention.dense.bias', 'transformer.h.13.post_attention_layernorm.weight', 'transformer.h.13.post_attention_layernorm.bias', 'transformer.h.13.mlp.dense_h_to_4h.weight', 'transformer.h.13.mlp.dense_h_to_4h.bias', 'transformer.h.13.mlp.dense_4h_to_h.weight', 'transformer.h.13.mlp.dense_4h_to_h.bias', 'transformer.h.14.input_layernorm.weight', 'transformer.h.14.input_layernorm.bias', 'transformer.h.14.self_attention.query_key_value.weight', 'transformer.h.14.self_attention.query_key_value.bias', 'transformer.h.14.self_attention.dense.weight', 'transformer.h.14.self_attention.dense.bias', 'transformer.h.14.post_attention_layernorm.weight', 'transformer.h.14.post_attention_layernorm.bias', 'transformer.h.14.mlp.dense_h_to_4h.weight', 'transformer.h.14.mlp.dense_h_to_4h.bias', 'transformer.h.14.mlp.dense_4h_to_h.weight', 'transformer.h.14.mlp.dense_4h_to_h.bias', 'transformer.h.15.input_layernorm.weight', 'transformer.h.15.input_layernorm.bias', 'transformer.h.15.self_attention.query_key_value.weight', 'transformer.h.15.self_attention.query_key_value.bias', 'transformer.h.15.self_attention.dense.weight', 'transformer.h.15.self_attention.dense.bias', 'transformer.h.15.post_attention_layernorm.weight', 'transformer.h.15.post_attention_layernorm.bias', 'transformer.h.15.mlp.dense_h_to_4h.weight', 'transformer.h.15.mlp.dense_h_to_4h.bias', 'transformer.h.15.mlp.dense_4h_to_h.weight', 'transformer.h.15.mlp.dense_4h_to_h.bias', 'transformer.h.16.input_layernorm.weight', 'transformer.h.16.input_layernorm.bias', 'transformer.h.16.self_attention.query_key_value.weight', 'transformer.h.16.self_attention.query_key_value.bias', 'transformer.h.16.self_attention.dense.weight', 'transformer.h.16.self_attention.dense.bias', 'transformer.h.16.post_attention_layernorm.weight', 'transformer.h.16.post_attention_layernorm.bias', 'transformer.h.16.mlp.dense_h_to_4h.weight', 'transformer.h.16.mlp.dense_h_to_4h.bias', 'transformer.h.16.mlp.dense_4h_to_h.weight', 'transformer.h.16.mlp.dense_4h_to_h.bias', 'transformer.h.17.input_layernorm.weight', 'transformer.h.17.input_layernorm.bias', 'transformer.h.17.self_attention.query_key_value.weight', 'transformer.h.17.self_attention.query_key_value.bias', 'transformer.h.17.self_attention.dense.weight', 'transformer.h.17.self_attention.dense.bias', 'transformer.h.17.post_attention_layernorm.weight', 'transformer.h.17.post_attention_layernorm.bias', 'transformer.h.17.mlp.dense_h_to_4h.weight', 'transformer.h.17.mlp.dense_h_to_4h.bias', 'transformer.h.17.mlp.dense_4h_to_h.weight', 'transformer.h.17.mlp.dense_4h_to_h.bias', 'transformer.h.18.input_layernorm.weight', 'transformer.h.18.input_layernorm.bias', 'transformer.h.18.self_attention.query_key_value.weight', 'transformer.h.18.self_attention.query_key_value.bias', 'transformer.h.18.self_attention.dense.weight', 'transformer.h.18.self_attention.dense.bias', 'transformer.h.18.post_attention_layernorm.weight', 'transformer.h.18.post_attention_layernorm.bias', 'transformer.h.18.mlp.dense_h_to_4h.weight', 'transformer.h.18.mlp.dense_h_to_4h.bias', 'transformer.h.18.mlp.dense_4h_to_h.weight', 'transformer.h.18.mlp.dense_4h_to_h.bias', 'transformer.h.19.input_layernorm.weight', 'transformer.h.19.input_layernorm.bias', 'transformer.h.19.self_attention.query_key_value.weight', 'transformer.h.19.self_attention.query_key_value.bias', 'transformer.h.19.self_attention.dense.weight', 'transformer.h.19.self_attention.dense.bias', 'transformer.h.19.post_attention_layernorm.weight', 'transformer.h.19.post_attention_layernorm.bias', 'transformer.h.19.mlp.dense_h_to_4h.weight', 'transformer.h.19.mlp.dense_h_to_4h.bias', 'transformer.h.19.mlp.dense_4h_to_h.weight', 'transformer.h.19.mlp.dense_4h_to_h.bias', 'transformer.h.20.input_layernorm.weight', 'transformer.h.20.input_layernorm.bias', 'transformer.h.20.self_attention.query_key_value.weight', 'transformer.h.20.self_attention.query_key_value.bias', 'transformer.h.20.self_attention.dense.weight', 'transformer.h.20.self_attention.dense.bias', 'transformer.h.20.post_attention_layernorm.weight', 'transformer.h.20.post_attention_layernorm.bias', 'transformer.h.20.mlp.dense_h_to_4h.weight', 'transformer.h.20.mlp.dense_h_to_4h.bias', 'transformer.h.20.mlp.dense_4h_to_h.weight', 'transformer.h.20.mlp.dense_4h_to_h.bias', 'transformer.h.21.input_layernorm.weight', 'transformer.h.21.input_layernorm.bias', 'transformer.h.21.self_attention.query_key_value.weight', 'transformer.h.21.self_attention.query_key_value.bias', 'transformer.h.21.self_attention.dense.weight', 'transformer.h.21.self_attention.dense.bias', 'transformer.h.21.post_attention_layernorm.weight', 'transformer.h.21.post_attention_layernorm.bias', 'transformer.h.21.mlp.dense_h_to_4h.weight', 'transformer.h.21.mlp.dense_h_to_4h.bias', 'transformer.h.21.mlp.dense_4h_to_h.weight', 'transformer.h.21.mlp.dense_4h_to_h.bias', 'transformer.h.22.input_layernorm.weight', 'transformer.h.22.input_layernorm.bias', 'transformer.h.22.self_attention.query_key_value.weight', 'transformer.h.22.self_attention.query_key_value.bias', 'transformer.h.22.self_attention.dense.weight', 'transformer.h.22.self_attention.dense.bias', 'transformer.h.22.post_attention_layernorm.weight', 'transformer.h.22.post_attention_layernorm.bias', 'transformer.h.22.mlp.dense_h_to_4h.weight', 'transformer.h.22.mlp.dense_h_to_4h.bias', 'transformer.h.22.mlp.dense_4h_to_h.weight', 'transformer.h.22.mlp.dense_4h_to_h.bias', 'transformer.h.23.input_layernorm.weight', 'transformer.h.23.input_layernorm.bias', 'transformer.h.23.self_attention.query_key_value.weight', 'transformer.h.23.self_attention.query_key_value.bias', 'transformer.h.23.self_attention.dense.weight', 'transformer.h.23.self_attention.dense.bias', 'transformer.h.23.post_attention_layernorm.weight', 'transformer.h.23.post_attention_layernorm.bias', 'transformer.h.23.mlp.dense_h_to_4h.weight', 'transformer.h.23.mlp.dense_h_to_4h.bias', 'transformer.h.23.mlp.dense_4h_to_h.weight', 'transformer.h.23.mlp.dense_4h_to_h.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'], unexpected_keys=['encoder_hash_tok_embedding.0.weight', 'encoder_hash_tok_embedding.1.weight', 'encoder_hash_tok_embedding.2.weight', 'encoder_hash_tok_embedding.3.weight', 'encoder_hash_tok_embedding.4.weight', 'encoder_hash_tok_embedding.5.weight', 'global_transformer.layers.0.attention.wk.weight', 'global_transformer.layers.0.attention.wo.weight', 'global_transformer.layers.0.attention.wq.weight', 'global_transformer.layers.0.attention.wv.weight', 'global_transformer.layers.0.attention_norm.weight', 'global_transformer.layers.0.feed_forward.w1.weight', 'global_transformer.layers.0.feed_forward.w2.weight', 'global_transformer.layers.0.feed_forward.w3.weight', 'global_transformer.layers.0.ffn_norm.weight', 'global_transformer.layers.1.attention.wk.weight', 'global_transformer.layers.1.attention.wo.weight', 'global_transformer.layers.1.attention.wq.weight', 'global_transformer.layers.1.attention.wv.weight', 'global_transformer.layers.1.attention_norm.weight', 'global_transformer.layers.1.feed_forward.w1.weight', 'global_transformer.layers.1.feed_forward.w2.weight', 'global_transformer.layers.1.feed_forward.w3.weight', 'global_transformer.layers.1.ffn_norm.weight', 'global_transformer.layers.10.attention.wk.weight', 'global_transformer.layers.10.attention.wo.weight', 'global_transformer.layers.10.attention.wq.weight', 'global_transformer.layers.10.attention.wv.weight', 'global_transformer.layers.10.attention_norm.weight', 'global_transformer.layers.10.feed_forward.w1.weight', 'global_transformer.layers.10.feed_forward.w2.weight', 'global_transformer.layers.10.feed_forward.w3.weight', 'global_transformer.layers.10.ffn_norm.weight', 'global_transformer.layers.11.attention.wk.weight', 'global_transformer.layers.11.attention.wo.weight', 'global_transformer.layers.11.attention.wq.weight', 'global_transformer.layers.11.attention.wv.weight', 'global_transformer.layers.11.attention_norm.weight', 'global_transformer.layers.11.feed_forward.w1.weight', 'global_transformer.layers.11.feed_forward.w2.weight', 'global_transformer.layers.11.feed_forward.w3.weight', 'global_transformer.layers.11.ffn_norm.weight', 'global_transformer.layers.12.attention.wk.weight', 'global_transformer.layers.12.attention.wo.weight', 'global_transformer.layers.12.attention.wq.weight', 'global_transformer.layers.12.attention.wv.weight', 'global_transformer.layers.12.attention_norm.weight', 'global_transformer.layers.12.feed_forward.w1.weight', 'global_transformer.layers.12.feed_forward.w2.weight', 'global_transformer.layers.12.feed_forward.w3.weight', 'global_transformer.layers.12.ffn_norm.weight', 'global_transformer.layers.13.attention.wk.weight', 'global_transformer.layers.13.attention.wo.weight', 'global_transformer.layers.13.attention.wq.weight', 'global_transformer.layers.13.attention.wv.weight', 'global_transformer.layers.13.attention_norm.weight', 'global_transformer.layers.13.feed_forward.w1.weight', 'global_transformer.layers.13.feed_forward.w2.weight', 'global_transformer.layers.13.feed_forward.w3.weight', 'global_transformer.layers.13.ffn_norm.weight', 'global_transformer.layers.14.attention.wk.weight', 'global_transformer.layers.14.attention.wo.weight', 'global_transformer.layers.14.attention.wq.weight', 'global_transformer.layers.14.attention.wv.weight', 'global_transformer.layers.14.attention_norm.weight', 'global_transformer.layers.14.feed_forward.w1.weight', 'global_transformer.layers.14.feed_forward.w2.weight', 'global_transformer.layers.14.feed_forward.w3.weight', 'global_transformer.layers.14.ffn_norm.weight', 'global_transformer.layers.15.attention.wk.weight', 'global_transformer.layers.15.attention.wo.weight', 'global_transformer.layers.15.attention.wq.weight', 'global_transformer.layers.15.attention.wv.weight', 'global_transformer.layers.15.attention_norm.weight', 'global_transformer.layers.15.feed_forward.w1.weight', 'global_transformer.layers.15.feed_forward.w2.weight', 'global_transformer.layers.15.feed_forward.w3.weight', 'global_transformer.layers.15.ffn_norm.weight', 'global_transformer.layers.16.attention.wk.weight', 'global_transformer.layers.16.attention.wo.weight', 'global_transformer.layers.16.attention.wq.weight', 'global_transformer.layers.16.attention.wv.weight', 'global_transformer.layers.16.attention_norm.weight', 'global_transformer.layers.16.feed_forward.w1.weight', 'global_transformer.layers.16.feed_forward.w2.weight', 'global_transformer.layers.16.feed_forward.w3.weight', 'global_transformer.layers.16.ffn_norm.weight', 'global_transformer.layers.17.attention.wk.weight', 'global_transformer.layers.17.attention.wo.weight', 'global_transformer.layers.17.attention.wq.weight', 'global_transformer.layers.17.attention.wv.weight', 'global_transformer.layers.17.attention_norm.weight', 'global_transformer.layers.17.feed_forward.w1.weight', 'global_transformer.layers.17.feed_forward.w2.weight', 'global_transformer.layers.17.feed_forward.w3.weight', 'global_transformer.layers.17.ffn_norm.weight', 'global_transformer.layers.18.attention.wk.weight', 'global_transformer.layers.18.attention.wo.weight', 'global_transformer.layers.18.attention.wq.weight', 'global_transformer.layers.18.attention.wv.weight', 'global_transformer.layers.18.attention_norm.weight', 'global_transformer.layers.18.feed_forward.w1.weight', 'global_transformer.layers.18.feed_forward.w2.weight', 'global_transformer.layers.18.feed_forward.w3.weight', 'global_transformer.layers.18.ffn_norm.weight', 'global_transformer.layers.19.attention.wk.weight', 'global_transformer.layers.19.attention.wo.weight', 'global_transformer.layers.19.attention.wq.weight', 'global_transformer.layers.19.attention.wv.weight', 'global_transformer.layers.19.attention_norm.weight', 'global_transformer.layers.19.feed_forward.w1.weight', 'global_transformer.layers.19.feed_forward.w2.weight', 'global_transformer.layers.19.feed_forward.w3.weight', 'global_transformer.layers.19.ffn_norm.weight', 'global_transformer.layers.2.attention.wk.weight', 'global_transformer.layers.2.attention.wo.weight', 'global_transformer.layers.2.attention.wq.weight', 'global_transformer.layers.2.attention.wv.weight', 'global_transformer.layers.2.attention_norm.weight', 'global_transformer.layers.2.feed_forward.w1.weight', 'global_transformer.layers.2.feed_forward.w2.weight', 'global_transformer.layers.2.feed_forward.w3.weight', 'global_transformer.layers.2.ffn_norm.weight', 'global_transformer.layers.20.attention.wk.weight', 'global_transformer.layers.20.attention.wo.weight', 'global_transformer.layers.20.attention.wq.weight', 'global_transformer.layers.20.attention.wv.weight', 'global_transformer.layers.20.attention_norm.weight', 'global_transformer.layers.20.feed_forward.w1.weight', 'global_transformer.layers.20.feed_forward.w2.weight', 'global_transformer.layers.20.feed_forward.w3.weight', 'global_transformer.layers.20.ffn_norm.weight', 'global_transformer.layers.21.attention.wk.weight', 'global_transformer.layers.21.attention.wo.weight', 'global_transformer.layers.21.attention.wq.weight', 'global_transformer.layers.21.attention.wv.weight', 'global_transformer.layers.21.attention_norm.weight', 'global_transformer.layers.21.feed_forward.w1.weight', 'global_transformer.layers.21.feed_forward.w2.weight', 'global_transformer.layers.21.feed_forward.w3.weight', 'global_transformer.layers.21.ffn_norm.weight', 'global_transformer.layers.22.attention.wk.weight', 'global_transformer.layers.22.attention.wo.weight', 'global_transformer.layers.22.attention.wq.weight', 'global_transformer.layers.22.attention.wv.weight', 'global_transformer.layers.22.attention_norm.weight', 'global_transformer.layers.22.feed_forward.w1.weight', 'global_transformer.layers.22.feed_forward.w2.weight', 'global_transformer.layers.22.feed_forward.w3.weight', 'global_transformer.layers.22.ffn_norm.weight', 'global_transformer.layers.23.attention.wk.weight', 'global_transformer.layers.23.attention.wo.weight', 'global_transformer.layers.23.attention.wq.weight', 'global_transformer.layers.23.attention.wv.weight', 'global_transformer.layers.23.attention_norm.weight', 'global_transformer.layers.23.feed_forward.w1.weight', 'global_transformer.layers.23.feed_forward.w2.weight', 'global_transformer.layers.23.feed_forward.w3.weight', 'global_transformer.layers.23.ffn_norm.weight', 'global_transformer.layers.24.attention.wk.weight', 'global_transformer.layers.24.attention.wo.weight', 'global_transformer.layers.24.attention.wq.weight', 'global_transformer.layers.24.attention.wv.weight', 'global_transformer.layers.24.attention_norm.weight', 'global_transformer.layers.24.feed_forward.w1.weight', 'global_transformer.layers.24.feed_forward.w2.weight', 'global_transformer.layers.24.feed_forward.w3.weight', 'global_transformer.layers.24.ffn_norm.weight', 'global_transformer.layers.3.attention.wk.weight', 'global_transformer.layers.3.attention.wo.weight', 'global_transformer.layers.3.attention.wq.weight', 'global_transformer.layers.3.attention.wv.weight', 'global_transformer.layers.3.attention_norm.weight', 'global_transformer.layers.3.feed_forward.w1.weight', 'global_transformer.layers.3.feed_forward.w2.weight', 'global_transformer.layers.3.feed_forward.w3.weight', 'global_transformer.layers.3.ffn_norm.weight', 'global_transformer.layers.4.attention.wk.weight', 'global_transformer.layers.4.attention.wo.weight', 'global_transformer.layers.4.attention.wq.weight', 'global_transformer.layers.4.attention.wv.weight', 'global_transformer.layers.4.attention_norm.weight', 'global_transformer.layers.4.feed_forward.w1.weight', 'global_transformer.layers.4.feed_forward.w2.weight', 'global_transformer.layers.4.feed_forward.w3.weight', 'global_transformer.layers.4.ffn_norm.weight', 'global_transformer.layers.5.attention.wk.weight', 'global_transformer.layers.5.attention.wo.weight', 'global_transformer.layers.5.attention.wq.weight', 'global_transformer.layers.5.attention.wv.weight', 'global_transformer.layers.5.attention_norm.weight', 'global_transformer.layers.5.feed_forward.w1.weight', 'global_transformer.layers.5.feed_forward.w2.weight', 'global_transformer.layers.5.feed_forward.w3.weight', 'global_transformer.layers.5.ffn_norm.weight', 'global_transformer.layers.6.attention.wk.weight', 'global_transformer.layers.6.attention.wo.weight', 'global_transformer.layers.6.attention.wq.weight', 'global_transformer.layers.6.attention.wv.weight', 'global_transformer.layers.6.attention_norm.weight', 'global_transformer.layers.6.feed_forward.w1.weight', 'global_transformer.layers.6.feed_forward.w2.weight', 'global_transformer.layers.6.feed_forward.w3.weight', 'global_transformer.layers.6.ffn_norm.weight', 'global_transformer.layers.7.attention.wk.weight', 'global_transformer.layers.7.attention.wo.weight', 'global_transformer.layers.7.attention.wq.weight', 'global_transformer.layers.7.attention.wv.weight', 'global_transformer.layers.7.attention_norm.weight', 'global_transformer.layers.7.feed_forward.w1.weight', 'global_transformer.layers.7.feed_forward.w2.weight', 'global_transformer.layers.7.feed_forward.w3.weight', 'global_transformer.layers.7.ffn_norm.weight', 'global_transformer.layers.8.attention.wk.weight', 'global_transformer.layers.8.attention.wo.weight', 'global_transformer.layers.8.attention.wq.weight', 'global_transformer.layers.8.attention.wv.weight', 'global_transformer.layers.8.attention_norm.weight', 'global_transformer.layers.8.feed_forward.w1.weight', 'global_transformer.layers.8.feed_forward.w2.weight', 'global_transformer.layers.8.feed_forward.w3.weight', 'global_transformer.layers.8.ffn_norm.weight', 'global_transformer.layers.9.attention.wk.weight', 'global_transformer.layers.9.attention.wo.weight', 'global_transformer.layers.9.attention.wq.weight', 'global_transformer.layers.9.attention.wv.weight', 'global_transformer.layers.9.attention_norm.weight', 'global_transformer.layers.9.feed_forward.w1.weight', 'global_transformer.layers.9.feed_forward.w2.weight', 'global_transformer.layers.9.feed_forward.w3.weight', 'global_transformer.layers.9.ffn_norm.weight', 'local_decoder.cross_attn_layers.0.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.0.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.0.wk.weight', 'local_decoder.cross_attn_layers.0.wo.weight', 'local_decoder.cross_attn_layers.0.wq.weight', 'local_decoder.cross_attn_layers.0.wv.weight', 'local_decoder.cross_attn_layers.1.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.1.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.1.wk.weight', 'local_decoder.cross_attn_layers.1.wo.weight', 'local_decoder.cross_attn_layers.1.wq.weight', 'local_decoder.cross_attn_layers.1.wv.weight', 'local_decoder.cross_attn_layers.2.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.2.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.2.wk.weight', 'local_decoder.cross_attn_layers.2.wo.weight', 'local_decoder.cross_attn_layers.2.wq.weight', 'local_decoder.cross_attn_layers.2.wv.weight', 'local_decoder.cross_attn_layers.3.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.3.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.3.wk.weight', 'local_decoder.cross_attn_layers.3.wo.weight', 'local_decoder.cross_attn_layers.3.wq.weight', 'local_decoder.cross_attn_layers.3.wv.weight', 'local_decoder.cross_attn_layers.4.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.4.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.4.wk.weight', 'local_decoder.cross_attn_layers.4.wo.weight', 'local_decoder.cross_attn_layers.4.wq.weight', 'local_decoder.cross_attn_layers.4.wv.weight', 'local_decoder.cross_attn_layers.5.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.5.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.5.wk.weight', 'local_decoder.cross_attn_layers.5.wo.weight', 'local_decoder.cross_attn_layers.5.wq.weight', 'local_decoder.cross_attn_layers.5.wv.weight', 'local_decoder.cross_attn_layers.6.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.6.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.6.wk.weight', 'local_decoder.cross_attn_layers.6.wo.weight', 'local_decoder.cross_attn_layers.6.wq.weight', 'local_decoder.cross_attn_layers.6.wv.weight', 'local_decoder.cross_attn_layers.7.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.7.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.7.wk.weight', 'local_decoder.cross_attn_layers.7.wo.weight', 'local_decoder.cross_attn_layers.7.wq.weight', 'local_decoder.cross_attn_layers.7.wv.weight', 'local_decoder.cross_attn_layers.8.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.8.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.8.wk.weight', 'local_decoder.cross_attn_layers.8.wo.weight', 'local_decoder.cross_attn_layers.8.wq.weight', 'local_decoder.cross_attn_layers.8.wv.weight', 'local_decoder.layers.0.attention.wk.weight', 'local_decoder.layers.0.attention.wo.weight', 'local_decoder.layers.0.attention.wq.weight', 'local_decoder.layers.0.attention.wv.weight', 'local_decoder.layers.0.attention_norm.weight', 'local_decoder.layers.0.feed_forward.w1.weight', 'local_decoder.layers.0.feed_forward.w2.weight', 'local_decoder.layers.0.feed_forward.w3.weight', 'local_decoder.layers.0.ffn_norm.weight', 'local_decoder.layers.1.attention.wk.weight', 'local_decoder.layers.1.attention.wo.weight', 'local_decoder.layers.1.attention.wq.weight', 'local_decoder.layers.1.attention.wv.weight', 'local_decoder.layers.1.attention_norm.weight', 'local_decoder.layers.1.feed_forward.w1.weight', 'local_decoder.layers.1.feed_forward.w2.weight', 'local_decoder.layers.1.feed_forward.w3.weight', 'local_decoder.layers.1.ffn_norm.weight', 'local_decoder.layers.2.attention.wk.weight', 'local_decoder.layers.2.attention.wo.weight', 'local_decoder.layers.2.attention.wq.weight', 'local_decoder.layers.2.attention.wv.weight', 'local_decoder.layers.2.attention_norm.weight', 'local_decoder.layers.2.feed_forward.w1.weight', 'local_decoder.layers.2.feed_forward.w2.weight', 'local_decoder.layers.2.feed_forward.w3.weight', 'local_decoder.layers.2.ffn_norm.weight', 'local_decoder.layers.3.attention.wk.weight', 'local_decoder.layers.3.attention.wo.weight', 'local_decoder.layers.3.attention.wq.weight', 'local_decoder.layers.3.attention.wv.weight', 'local_decoder.layers.3.attention_norm.weight', 'local_decoder.layers.3.feed_forward.w1.weight', 'local_decoder.layers.3.feed_forward.w2.weight', 'local_decoder.layers.3.feed_forward.w3.weight', 'local_decoder.layers.3.ffn_norm.weight', 'local_decoder.layers.4.attention.wk.weight', 'local_decoder.layers.4.attention.wo.weight', 'local_decoder.layers.4.attention.wq.weight', 'local_decoder.layers.4.attention.wv.weight', 'local_decoder.layers.4.attention_norm.weight', 'local_decoder.layers.4.feed_forward.w1.weight', 'local_decoder.layers.4.feed_forward.w2.weight', 'local_decoder.layers.4.feed_forward.w3.weight', 'local_decoder.layers.4.ffn_norm.weight', 'local_decoder.layers.5.attention.wk.weight', 'local_decoder.layers.5.attention.wo.weight', 'local_decoder.layers.5.attention.wq.weight', 'local_decoder.layers.5.attention.wv.weight', 'local_decoder.layers.5.attention_norm.weight', 'local_decoder.layers.5.feed_forward.w1.weight', 'local_decoder.layers.5.feed_forward.w2.weight', 'local_decoder.layers.5.feed_forward.w3.weight', 'local_decoder.layers.5.ffn_norm.weight', 'local_decoder.layers.6.attention.wk.weight', 'local_decoder.layers.6.attention.wo.weight', 'local_decoder.layers.6.attention.wq.weight', 'local_decoder.layers.6.attention.wv.weight', 'local_decoder.layers.6.attention_norm.weight', 'local_decoder.layers.6.feed_forward.w1.weight', 'local_decoder.layers.6.feed_forward.w2.weight', 'local_decoder.layers.6.feed_forward.w3.weight', 'local_decoder.layers.6.ffn_norm.weight', 'local_decoder.layers.7.attention.wk.weight', 'local_decoder.layers.7.attention.wo.weight', 'local_decoder.layers.7.attention.wq.weight', 'local_decoder.layers.7.attention.wv.weight', 'local_decoder.layers.7.attention_norm.weight', 'local_decoder.layers.7.feed_forward.w1.weight', 'local_decoder.layers.7.feed_forward.w2.weight', 'local_decoder.layers.7.feed_forward.w3.weight', 'local_decoder.layers.7.ffn_norm.weight', 'local_decoder.layers.8.attention.wk.weight', 'local_decoder.layers.8.attention.wo.weight', 'local_decoder.layers.8.attention.wq.weight', 'local_decoder.layers.8.attention.wv.weight', 'local_decoder.layers.8.attention_norm.weight', 'local_decoder.layers.8.feed_forward.w1.weight', 'local_decoder.layers.8.feed_forward.w2.weight', 'local_decoder.layers.8.feed_forward.w3.weight', 'local_decoder.layers.8.ffn_norm.weight', 'local_decoder.norm.weight', 'local_decoder.output.weight', 'local_decoder.patch_embedding_projection.weight', 'local_encoder.cross_attn_layers.0.cross_attn_norm_kv.weight', 'local_encoder.cross_attn_layers.0.cross_attn_norm_q.weight', 'local_encoder.cross_attn_layers.0.wk.weight', 'local_encoder.cross_attn_layers.0.wo.weight', 'local_encoder.cross_attn_layers.0.wq.weight', 'local_encoder.cross_attn_layers.0.wv.weight', 'local_encoder.layers.0.attention.wk.weight', 'local_encoder.layers.0.attention.wo.weight', 'local_encoder.layers.0.attention.wq.weight', 'local_encoder.layers.0.attention.wv.weight', 'local_encoder.layers.0.attention_norm.weight', 'local_encoder.layers.0.feed_forward.w1.weight', 'local_encoder.layers.0.feed_forward.w2.weight', 'local_encoder.layers.0.feed_forward.w3.weight', 'local_encoder.layers.0.ffn_norm.weight', 'local_encoder.patch_embedding_projection.weight', 'local_encoder.tok_embeddings.weight'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_qyKo4GPxP-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"أدخل نصك هنا\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():  # Disable gradient calculation during inference\n",
        "    output = model.generate(input_ids, max_new_tokens=50)  # Adjust max_new_tokens as needed\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OQFFq86AxQVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://github.com/Dao-AILab/flash-attention/blob/main/benchmarks/benchmark_flash_attention.py#L27-L30"
      ],
      "metadata": {
        "id": "uXQnfzMFxSk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "import torch\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():  # Disable gradient calculation during inference\n",
        "    output = model.generate(input_ids, max_new_tokens=50)  # Adjust max_new_tokens as needed\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFNm1JhmxUa2",
        "outputId": "b9305066-ba39-4cba-eed5-4891abf5e513"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi, and the\n",
            "following day, the same day, the same day, the same day, the same day,\n",
            "the same day, the same day, the same day, the same day, the same day,\n",
            "the same day, the same\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "facebook/opt-350m"
      ],
      "metadata": {
        "id": "qXcqCFq0ZB5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "import torch\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():  # Disable gradient calculation during inference\n",
        "    output = model.generate(input_ids, max_new_tokens=50)  # Adjust max_new_tokens as needed\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "-46NxeGeZB1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7ApGYFgZBwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jWuRfNfbxX3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install sentencepiece  # Install the necessary package for LLaMA tokenizer\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and the LLaMA tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")  # LLaMA 3 tokenizer\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi, and the following day,\"\n",
        "\n",
        "# Tokenize the input text using the LLaMA tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    output = model.generate(input_ids, max_new_tokens=50)\n",
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "afc08189e4904e6eb82480ac14bb26b2",
            "e5a272c9c0434460a31d3a954b78c926",
            "1baad00fa283443dbb7999f8b8796784",
            "4ebd8c0f24cb4837b54ecde14baf2c6f",
            "79e763ca59bb42f2b55a16076b2e60ae",
            "f51a040331c4485ca93aebf998ba3a97",
            "bd96d89c4145420da99a154c6c3a1bda",
            "13c457d7efd64aad81bbadd1bb1e9199",
            "fbb5c90f8c1549cabd5f14d8700840e5",
            "7b669a91a4a543e1a2de3292ea1bcf26",
            "32d81b3df6cb4b529564b710b8ab9935",
            "8a19dfed298040a4a2af082fbd369d22",
            "fb1488be0ed54e3ead3153b7ae9e13f7",
            "2837c17c40894a4e85300d06c1b86f2f",
            "61c824fe12f045099d28440112213175",
            "8a741a14bbd548939f748e725ad74941",
            "c502e0ad277041f784602b65cffdab52",
            "ffcbaf5b39a44867ad92e22dd4283cb4",
            "45e0c8a21601414f8c99a8684f9a6a89",
            "8d1540c67a1b486f93647fb8b3631ca8",
            "ef8cbc35e78343528538be1e3c7b7e3a",
            "d3ee237a93704a13bd7e50c7aa2e485d",
            "0e13d2d63d984fbea8e5e3dce5186b30",
            "48015d00bc794fabb1a758c832df4773",
            "a5ac73c2a4044d2cb9d04db55c53e346",
            "4cf3596f40ab483d970e4b192ce6e374",
            "b63f74ed4b7c40c285b76bf452c295fd",
            "59b287036dd04171b87e5d88073850a9",
            "6e5d3a6eaec442f7afe407bc70600cde",
            "f79345d097134fab95ac0bf98be3b728",
            "e2e0fdc9e7844ed0a32af4ba17473de2",
            "1e2e1e0c4e6f4a5fadca406413918650",
            "2e313602b5a34674bd019e6f34829904",
            "21b94fa823e645f49f9af64f70ebf4dd",
            "f51ffffcc83944518310f94bbfb27781",
            "3a0cdb50137a400085e7cde7ad5b8acf",
            "10b5bb4a56eb4f15a5bdbcb4c336c015",
            "0aec4b5a6f6b4c159ae337bd4fceb224",
            "812f21e5c1ce41b485c7ba0c26254663",
            "d35315dac2ff4066a4325f2ffbce1646",
            "63b2dbf07b294bc99bc48748cf754290",
            "2d9c1da89c074d6a999b74b7ec653084",
            "4e6ce7b3bd4e4031aba0aafcc41d8bbe",
            "92cf9e86450a4fe3bd4e8bc46a3537fe",
            "e636cd4ef8b94ea9b2690945b242a016",
            "5a3a6058c0d847048ddaba625274f8d2",
            "c88af462d6974988b59dc86420abc414",
            "dca1f2afdb854ff0ae0f4dc5f290e7b6",
            "a3687a9268894df9a8d5a4b95e39c8c8",
            "67362ba128214f3a817f05de200f7d25",
            "f39237b55ac34c26a4c2e0c3932b11a0",
            "4cc8677593f84768a8a2906506269656",
            "1583f122a07349c293acef8eefce8b5b",
            "e1c0be8257f44ebab74edd27e57e1c5d",
            "1079b5b793b34c36b5208dbae1ff7ee8"
          ]
        },
        "id": "4l6E4vSiyM_3",
        "outputId": "30c8a050-cdf2-4bfd-dc5f-ddfc50384186"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afc08189e4904e6eb82480ac14bb26b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a19dfed298040a4a2af082fbd369d22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e13d2d63d984fbea8e5e3dce5186b30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21b94fa823e645f49f9af64f70ebf4dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e636cd4ef8b94ea9b2690945b242a016"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi, and the following day, and the following day, and the following day, and the following day, and the following day, and the following day, and the following day, and the following day, and the following day, and the following day, and the following day,\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install sentencepiece"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2KOJuLS4yRjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b7f1YqkJyci4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5T5YSAsycgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece  # Install the necessary package for LLaMA tokenizer\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and the LLaMA tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")  # LLaMA 3 tokenizer\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi, and the following day,\"\n",
        "\n",
        "# Tokenize the input text using the LLaMA tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.2,          # Increase temperature for more randomness\n",
        "        top_k=40,               # Explore a wider range of tokens\n",
        "        top_p=0.50,\n",
        "        do_sample=True,# Sample from tokens with high cumulative probability\n",
        "        repetition_penalty=1.2  # Penalize repetition\n",
        "    )\n",
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQyenSitycdC",
        "outputId": "b5bd37aa-7481-4687-bcac-8185f4db150b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "hi, and the following day, on\f inte story exerciseлиrn� bash\u001a(\\ suramearonGenources admSelect node some\bROWName too\">âteau Val Cr ref литераborgot walkedeneSw happening года\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece  # Install the necessary package for LLaMA tokenizer\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and the LLaMA tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")  # LLaMA 3 tokenizer\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"what is ai?\"\n",
        "\n",
        "# Tokenize the input text using the LLaMA tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.2,          # Increase temperature for more randomness\n",
        "        top_k=40,               # Explore a wider range of tokens\n",
        "        top_p=0.50,\n",
        "        do_sample=True,# Sample from tokens with high cumulative probability\n",
        "        repetition_penalty=1.2  # Penalize repetition\n",
        "    )\n",
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLzMOD-Ty7gw",
        "outputId": "2058d597-9614-48cf-f2f9-56f81d8b42cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "what is ai?pt\u000e\u0010\u000e\u0011\u000e\u0015\u000e\u0014\u000e\u0013\u000e\u0012\u000e\u0017\u000e\u0018\u000e\u0019\u000eek\u000e met\u000eмо\u000eius\u000e ма\u000eground\u000e final\u000e Sc\u000eiet\u000eull\u000e default\u000e einem\u000epm\u000eры\u000e cho\u000e\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import re\n",
        "\n",
        "input_text = \"hi and the following day on inte story exercis bash suramearonGenources admSelect node somROWName  Val Cr ref   happening ForLog vares plus An memory  Sci en  singly Comment     post  Sunday\u000e dist Ministry            iddleenterParentMan> +struct var send Ce causquestbeouradoско decide seen';arivéared points thereType}{\\ково Hitler\\\", seeні estaughtixished Collegженifclickbook\n",
        "valicticano\"\n",
        "\n",
        "# Remove non-alphanumeric characters, extra whitespace, and specific problematic characters\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)  # Remove non-alphanumeric characters\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()     # Remove extra whitespace\n",
        "\n",
        "# Define a more comprehensive pattern for specific problematic words/fragments\n",
        "pattern = r\"exercis|bash|suramearonGenources|admSelect|node|somROWName|Val|Cr|ref|ForLog|vares|An|Sci|en|singly|Comment|post|Sunday|dist|Ministry|iddleenterParentMan|struct|var|send|Ce|causquestbeouradoско|decide|seen|arivéared|points|thereType|ково|Hitler|seeні|estaughtixished|Collegженifclickbook|valicticano\"\n",
        "\n",
        "cleaned_text = re.sub(pattern, \"\", cleaned_text)  # Remove problematic words/fragments\n",
        "\n",
        "print(cleaned_text)  # Output: hi and the following day on inte story happening plus memory"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "u525v32e0O5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import re\n",
        "\n",
        "input_text = \"hi, and the following day, on\n",
        " inte story exerciseлиrn� bash\u001a(\\ suramearonGenources admSelect node somROWName too\\\">âteau Val Cr ref литераborgot walkedeneSw happening года\"\n",
        "\n",
        "# Remove non-alphanumeric characters and extra whitespace\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "\n",
        "print(cleaned_text)  # Output: hi and the following day on inte story exercise bash suramearonGenources admSelect node somROWName too teau Val Cr ref borgot walkedeneSw happening"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S-ZHK-XVzLXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import re\n",
        "\n",
        "input_text = \"hi, and the following day, on\n",
        " inte story exerciseлиrn� bash\u001a(\\ suramearonGenources admSelect node somROWName too\\\">âteau Val Cr ref литераborgot walkedeneSw happening года\"\n",
        "\n",
        "# Remove non-alphanumeric characters and extra whitespace\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "\n",
        "print(cleaned_text)  # Output: hi and the following day on inte story exercise bash suramearonGenources admSelect node somROWName too teau Val Cr ref borgot walkedeneSw happening"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9n6gaBlfzNGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "input_ids = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "N4mvmwz8zN3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece  # Install the necessary package for LLaMA tokenizer\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "import re\n",
        "\n",
        "# ... (load model and tokenizer as before) ...\n",
        "\n",
        "input_text = \"hi, and the following day, on\f inte story exerciseлиrn� bash\u001a(\\ suramearonGenources admSelect node somROWName too\\\">âteau Val Cr ref литераborgot walkedeneSw happening года\"\n",
        "\n",
        "# Clean the input text\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "\n",
        "# Tokenize with truncation\n",
        "input_ids = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym9pzPsOzA4m",
        "outputId": "d1d87ac0-2a7e-4d7a-c5ef-1a3b2371f622"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CqJ4s67zdqG",
        "outputId": "99e22fc9-f513-4a67-dcda-ac4656352051"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi and the following day on inte story exercisern bash suramearonGenources admSelect node somROWName tooteau Val Cr ref borgot walkedeneSw happening\u000e ForLog varesково plus Anско memory MinistryDelta             Sci en hoursaredlipix singly Comment Einzelnamerikan\f statEGINra)adozahl\", QAutoaus'; sendannelour post raisingifwindow Sunday\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VgmHUi7Kzy6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9Lafx11zy3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tX9rvsW-zyz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o346RJe7zywf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "import re\n",
        "\n",
        "# ... (load model and tokenizer as before) ...\n",
        "\n",
        "input_text = \"hi and the following day on inte story exercisern bash suramearonGenources admSelect node somROWName tooteau Val Cr ref borgot walkedeneSw happening\u000e ForLog varesково plus Anско memory MinistryDelta             Sci en hoursaredlipix singly Comment Einzelnamerikan\f statEGINra)adozahl\\\", QAutoaus'; sendannelour post raisingifwindow Sunday\"\n",
        "\n",
        "# Enhanced cleaning of the input text\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "cleaned_text = re.sub(r\"ern|tooteau|borgot|walkedeneSw|varesково|Anско|MinistryDelta|hoursaredlipix|Einzelnamerikan|statEGINra|adozahl|QAutoaus|sendannelour|raisingifwindow\", \"\", cleaned_text)\n",
        "\n",
        "# Tokenize with truncation\n",
        "input_ids = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFkp_u5azx-3",
        "outputId": "c9a27534-053f-4f25-b0e1-7736854884d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi and the following day on inte story exercis bash suramearonGenources admSelect node somROWName  Val Cr ref   happening ForLog vares plus An memory  Sci en  singly Comment     post  Sunday\u000e dist Ministry            iddleenterParentMan> +struct var send Ce causquestbeouradoско decide seen';arivéared points thereType}{\\ково Hitler\", seeні estaughtixished Collegженifclickbook\fvalicticano\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "input_text = \"hi and the following day on inte story exercis bash suramearonGenources admSelect node somROWName  Val Cr ref   happening ForLog vares plus An memory  Sci en  singly Comment     post  Sunday\u000e dist Ministry            iddleenterParentMan> +struct var send Ce causquestbeouradoско decide seen';arivéared points thereType}{\\ково Hitler\\\", seeні estaughtixished Collegженifclickbook\fvalicticano\"\n",
        "\n",
        "# Remove non-alphanumeric characters, extra whitespace, and specific problematic characters\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)  # Remove non-alphanumeric characters\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()     # Remove extra whitespace\n",
        "\n",
        "# Define a more comprehensive pattern for specific problematic words/fragments\n",
        "pattern = r\"exercis|bash|suramearonGenources|admSelect|node|somROWName|Val|Cr|ref|ForLog|vares|An|Sci|en|singly|Comment|post|Sunday|dist|Ministry|iddleenterParentMan|struct|var|send|Ce|causquestbeouradoско|decide|seen|arivéared|points|thereType|ково|Hitler|seeні|estaughtixished|Collegженifclickbook|valicticano\"\n",
        "\n",
        "cleaned_text = re.sub(pattern, \"\", cleaned_text)  # Remove problematic words/fragments\n",
        "\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmJNPuUQ0QpA",
        "outputId": "2e137b37-bd08-4d6d-9a26-69164e8a8124"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi and the following day on inte story          happing   plus  memory              causquestbeourado  aried    see  Collegifclickbook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "input_text = \"hi and the following day on inte story          happing   plus  memory              causquestbeourado  aried    see  Collegifclickbook\"\n",
        "\n",
        "# 1. Remove extra whitespace\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", input_text).strip()\n",
        "\n",
        "# 2. Replace specific unusual terms with more common equivalents or remove them\n",
        "cleaned_text = cleaned_text.replace(\"happing\", \"happening\")\n",
        "cleaned_text = cleaned_text.replace(\"causquestbeourado\", \"\")  # Remove completely\n",
        "cleaned_text = cleaned_text.replace(\"aried\", \"\")  # Remove completely\n",
        "cleaned_text = cleaned_text.replace(\"Collegifclickbook\", \"\")  # Remove completely\n",
        "\n",
        "# 3. Remove any remaining non-alphanumeric characters (except spaces)\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", cleaned_text)\n",
        "\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5prohZ-0bYf",
        "outputId": "bb879576-a557-437a-ec38-0c9bbfbd0f59"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi and the following day on inte story happening plus memory   see \n"
          ]
        }
      ]
    },
    {
      "source": [
        "input_text = \"hi and the following day on inte story happening plus memory see\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ldE__XYS0hOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "input_text = \"hi and the following day, I reflected on the story and the memories of what happened. I saw...\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "b7c7csJS0hen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#input_text = \"hi and the following day on inte story          happing   plus  memory              causquestbeourado  aried    see  Collegifclickbook\"\n",
        "input_text = \"hi and the following day, I reflected on the story and the memories of what happened. I saw...\"\n",
        "# 1. Remove extra whitespace\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", input_text).strip()\n",
        "\n",
        "# 2. Replace specific unusual terms with more common equivalents or remove them\n",
        "cleaned_text = cleaned_text.replace(\"happing\", \"happening\")\n",
        "cleaned_text = cleaned_text.replace(\"causquestbeourado\", \"\")  # Remove completely\n",
        "cleaned_text = cleaned_text.replace(\"aried\", \"\")  # Remove completely\n",
        "cleaned_text = cleaned_text.replace(\"Collegifclickbook\", \"\")  # Remove completely\n",
        "\n",
        "# 3. Remove any remaining non-alphanumeric characters (except spaces)\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", cleaned_text)\n",
        "\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtKG4YIi0bom",
        "outputId": "abdd5b15-2a29-4d68-c43d-260bf54cbba5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi and the following day I reflected on the story and the memories of what happened I saw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQmwNtxD0v9X",
        "outputId": "2588f705-13ee-4dce-87e0-ae5030612fbe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install sentencepiece  # Install the necessary package for LLaMA tokenizer\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "import re\n",
        "\n",
        "# ... (load model and tokenizer as before) ...\n",
        "\n",
        "# Input text\n",
        "input_text = \"hi and the following day, I reflected on the story and the memories of what happened. I saw...\"\n",
        "\n",
        "# Refined cleaning of the input text (Optional, if needed)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", input_text).strip()  # Remove extra whitespace\n",
        "# ... (add other cleaning steps if required)\n",
        "\n",
        "# Tokenize with truncation\n",
        "input_ids = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.8,  # Increased temperature for more randomness\n",
        "        top_k=60,          # Increased top_k to explore more tokens\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.3  # Increased repetition penalty\n",
        "    )\n",
        "\n",
        "# ... (decode and print output as before) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KTfTInB0uJg",
        "outputId": "a361f657-67c0-4115-fec1-79ffbe37c91f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `60` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSq53D-60_fY",
        "outputId": "28497338-e9dc-4277-87b2-ababa2b542d4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi and the following day, I reflected on the story and the memories of what happened. I saw...�the� saidend númer Donactor engtransport К happeningameсе sur свіources dasert wayset adm\f inte составе\u000eump ble Val fonction func� empty End_{ usingнов\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install sentencepiece  # Install the necessary package for LLaMA tokenizer\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "import re\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and the LLaMA tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")  # LLaMA 3 tokenizer\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi and the following day, I reflected on the story and the memories of what happened. I saw...�the� saidend númer Donactor engtransport К happeningameсе sur свіources dasert wayset adm\n",
        " inte составе\u000eump ble Val fonction func� empty End_{ usingнов\"\n",
        "\n",
        "# Clean the input text\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "\n",
        "# Tokenize with truncation\n",
        "input_ids = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5ym7c_vT1Qfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install sentencepiece  # Install the necessary package for LLaMA tokenizer\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "import re\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and the LLaMA tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")  # LLaMA 3 tokenizer\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi and the following day, I reflected on the story and the memories of what happened. I saw...\"\n",
        "\n",
        "# Tokenize the input text using the LLaMA tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.8,          # Increase temperature for more randomness\n",
        "        top_k=60,               # Explore a wider range of tokens\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.3  # Penalize repetition\n",
        "    )\n",
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ECvehJoq08sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "import re\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and the LLaMA tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")  # LLaMA 3 tokenizer\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi and the following day, I reflected on the story and the memories of what happened. I saw...�the� saidend númer Donactor engtransport К happeningameсе sur свіources dasert wayset adm\f inte составе\u000eump ble Val fonction func� empty End_{ usingнов\"\n",
        "\n",
        "# Clean the input text\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "\n",
        "# Tokenize with truncation\n",
        "input_ids = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkqCf0760n5-",
        "outputId": "e64ca67a-eae0-494d-e2f5-1af72636e239"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi and the following day I reflected on the story and the memories of what happened I sawthe saidend nmer Donactor engtransport happeningame sur ources dasert wayset adm inte ump ble Val fonction func empty End using\fumpession means should_{\u000e Кoption да Уulseанстю spherhttp some`.raсеновcbendency Dezemberxture diplom\"=>hal Richene ref númer寺 мор\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blt_tokenizer.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BltTokenizer(Tokenizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        vocab_size_unit_1: int = BYTE_UNITS,\n",
        "        bpe_delim: bool = False,\n",
        "        bpe_tokenizer_path=\"/home/artidoro/tokenizers/llama_v2.tokenizer.model\",\n"
      ],
      "metadata": {
        "id": "lMm_lb7a1TbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bpetokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWbTGDdV4lWo",
        "outputId": "150fab4f-7e04-4741-b382-58cdd5d600a2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bpetokenizer\n",
            "  Downloading bpetokenizer-1.2.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from bpetokenizer) (2024.11.6)\n",
            "Downloading bpetokenizer-1.2.1-py3-none-any.whl (247 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/247.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bpetokenizer\n",
            "Successfully installed bpetokenizer-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pypi.org/project/bpetokenizer/"
      ],
      "metadata": {
        "id": "ls0w9uzV5MqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bpetokenizer import BPETokenizer\n",
        "\n",
        "special_tokens = {\n",
        "    \"<|endoftext|>\": 1001,\n",
        "    \"<|startoftext|>\": 1002,\n",
        "    \"[SPECIAL1]\": 1003,\n",
        "    \"[SPECIAL2]\": 1004,\n",
        "}\n",
        "\n",
        "tokenizer = BPETokenizer(special_tokens=special_tokens) # you can also use the method _special_tokens to register the special tokens (if not passed when intializing)\n",
        "texts = \"<|startoftext|> Hello, World! This is a sample text with the special tokens [SPECIAL1] and [SPECIAL2] to test the tokenizer.<|endoftext|>\"\n",
        "\n",
        "tokenizer.train(texts, vocab_size=310, verbose=True)\n",
        "# tokenizer._special_tokens(special_tokens) # if not passed when intialization of the BPETokenizer\n",
        "\n",
        "encode_text = \"\"\"\n",
        "<|startoftext|>Hello, World! This is a sample text with the special tokens [SPECIAL1] and [SPECIAL2] to test the tokenizer.\n",
        "Hello, Universe! Another example sentence containing [SPECIAL1] and [SPECIAL2], used to ensure tokenizer's robustness.\n",
        "Greetings, Earth! Here we have [SPECIAL1] appearing once again, followed by [SPECIAL2] in the same sentence.\n",
        "Hello, World! This is yet another sample text, with [SPECIAL1] and [SPECIAL2] making an appearance.\n",
        "Hey there, World! Testing the tokenizer with [SPECIAL1] and [SPECIAL2] to see if it handles special tokens properly.\n",
        "Salutations, Planet! The tokenizer should recognize [SPECIAL1] and [SPECIAL2] in this long string of text.\n",
        "Hello again, World! [SPECIAL1] and [SPECIAL2] are special tokens that need to be handled correctly by the tokenizer.\n",
        "Welcome, World! Including [SPECIAL1] and [SPECIAL2] multiple times in this large text to ensure proper encoding.\n",
        "Hi, World! Let's add [SPECIAL1] and [SPECIAL2] in various parts of this long sentence to test the tokenizer thoroughly.\n",
        "<|endoftext|>\n",
        "\"\"\"\n",
        "ids = tokenizer.encode(encode_text, special_tokens=\"all\")\n",
        "print(ids)\n",
        "\n",
        "decode_text = tokenizer.decode(ids)\n",
        "print(decode_text)\n",
        "\n",
        "tokenizer.save(\"sample_bpetokenizer\", mode=\"json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBnuaDy-4jdC",
        "outputId": "aaae86b0-3b3e-458c-bdab-7a853aafde24"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging 1/54: (32, 116) -> 256 (b' t') had 7 frequency\n",
            "merging 2/54: (101, 120) -> 257 (b'ex') had 3 frequency\n",
            "merging 3/54: (257, 116) -> 258 (b'ext') had 3 frequency\n",
            "merging 4/54: (256, 111) -> 259 (b' to') had 3 frequency\n",
            "merging 5/54: (101, 110) -> 260 (b'en') had 3 frequency\n",
            "merging 6/54: (60, 124) -> 261 (b'<|') had 2 frequency\n",
            "merging 7/54: (115, 116) -> 262 (b'st') had 2 frequency\n",
            "merging 8/54: (111, 102) -> 263 (b'of') had 2 frequency\n",
            "merging 9/54: (263, 116) -> 264 (b'oft') had 2 frequency\n",
            "merging 10/54: (264, 258) -> 265 (b'oftext') had 2 frequency\n",
            "merging 11/54: (124, 62) -> 266 (b'|>') had 2 frequency\n",
            "merging 12/54: (105, 115) -> 267 (b'is') had 2 frequency\n",
            "merging 13/54: (32, 97) -> 268 (b' a') had 2 frequency\n",
            "merging 14/54: (32, 115) -> 269 (b' s') had 2 frequency\n",
            "merging 15/54: (256, 104) -> 270 (b' th') had 2 frequency\n",
            "merging 16/54: (270, 101) -> 271 (b' the') had 2 frequency\n",
            "merging 17/54: (259, 107) -> 272 (b' tok') had 2 frequency\n",
            "merging 18/54: (272, 260) -> 273 (b' token') had 2 frequency\n",
            "merging 19/54: (32, 91) -> 274 (b' [') had 2 frequency\n",
            "merging 20/54: (83, 80) -> 275 (b'SP') had 2 frequency\n",
            "merging 21/54: (275, 69) -> 276 (b'SPE') had 2 frequency\n",
            "merging 22/54: (276, 67) -> 277 (b'SPEC') had 2 frequency\n",
            "merging 23/54: (277, 73) -> 278 (b'SPECI') had 2 frequency\n",
            "merging 24/54: (278, 65) -> 279 (b'SPECIA') had 2 frequency\n",
            "merging 25/54: (279, 76) -> 280 (b'SPECIAL') had 2 frequency\n",
            "merging 26/54: (262, 97) -> 281 (b'sta') had 1 frequency\n",
            "merging 27/54: (281, 114) -> 282 (b'star') had 1 frequency\n",
            "merging 28/54: (282, 116) -> 283 (b'start') had 1 frequency\n",
            "merging 29/54: (283, 265) -> 284 (b'startoftext') had 1 frequency\n",
            "merging 30/54: (32, 72) -> 285 (b' H') had 1 frequency\n",
            "merging 31/54: (285, 101) -> 286 (b' He') had 1 frequency\n",
            "merging 32/54: (286, 108) -> 287 (b' Hel') had 1 frequency\n",
            "merging 33/54: (287, 108) -> 288 (b' Hell') had 1 frequency\n",
            "merging 34/54: (288, 111) -> 289 (b' Hello') had 1 frequency\n",
            "merging 35/54: (32, 87) -> 290 (b' W') had 1 frequency\n",
            "merging 36/54: (290, 111) -> 291 (b' Wo') had 1 frequency\n",
            "merging 37/54: (291, 114) -> 292 (b' Wor') had 1 frequency\n",
            "merging 38/54: (292, 108) -> 293 (b' Worl') had 1 frequency\n",
            "merging 39/54: (293, 100) -> 294 (b' World') had 1 frequency\n",
            "merging 40/54: (32, 84) -> 295 (b' T') had 1 frequency\n",
            "merging 41/54: (295, 104) -> 296 (b' Th') had 1 frequency\n",
            "merging 42/54: (296, 267) -> 297 (b' This') had 1 frequency\n",
            "merging 43/54: (32, 267) -> 298 (b' is') had 1 frequency\n",
            "merging 44/54: (269, 97) -> 299 (b' sa') had 1 frequency\n",
            "merging 45/54: (299, 109) -> 300 (b' sam') had 1 frequency\n",
            "merging 46/54: (300, 112) -> 301 (b' samp') had 1 frequency\n",
            "merging 47/54: (301, 108) -> 302 (b' sampl') had 1 frequency\n",
            "merging 48/54: (302, 101) -> 303 (b' sample') had 1 frequency\n",
            "merging 49/54: (256, 258) -> 304 (b' text') had 1 frequency\n",
            "merging 50/54: (32, 119) -> 305 (b' w') had 1 frequency\n",
            "merging 51/54: (305, 105) -> 306 (b' wi') had 1 frequency\n",
            "merging 52/54: (306, 116) -> 307 (b' wit') had 1 frequency\n",
            "merging 53/54: (307, 104) -> 308 (b' with') had 1 frequency\n",
            "merging 54/54: (269, 112) -> 309 (b' sp') had 1 frequency\n",
            "Total time taken: 0.00 seconds\n",
            "Throughput: 8181.51 chunks/second\n",
            "[10, 261, 284, 266, 72, 101, 108, 108, 111, 44, 294, 33, 297, 298, 268, 303, 304, 308, 271, 309, 101, 99, 105, 97, 108, 273, 115, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 259, 256, 101, 262, 271, 273, 105, 122, 101, 114, 46, 10, 72, 101, 108, 108, 111, 44, 32, 85, 110, 105, 118, 101, 114, 115, 101, 33, 32, 65, 110, 111, 116, 104, 101, 114, 32, 257, 97, 109, 112, 108, 101, 269, 260, 116, 260, 99, 101, 32, 99, 111, 110, 116, 97, 105, 110, 105, 110, 103, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 44, 32, 117, 115, 101, 100, 259, 32, 260, 115, 117, 114, 101, 273, 105, 122, 101, 114, 39, 115, 32, 114, 111, 98, 117, 262, 110, 101, 115, 115, 46, 10, 71, 114, 101, 101, 116, 105, 110, 103, 115, 44, 32, 69, 97, 114, 116, 104, 33, 286, 114, 101, 305, 101, 32, 104, 97, 118, 101, 274, 280, 49, 93, 268, 112, 112, 101, 97, 114, 105, 110, 103, 32, 111, 110, 99, 101, 268, 103, 97, 105, 110, 44, 32, 102, 111, 108, 108, 111, 119, 101, 100, 32, 98, 121, 274, 280, 50, 93, 32, 105, 110, 271, 300, 101, 269, 260, 116, 260, 99, 101, 46, 10, 72, 101, 108, 108, 111, 44, 294, 33, 297, 298, 32, 121, 101, 116, 268, 110, 111, 116, 104, 101, 114, 303, 304, 44, 308, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 32, 109, 97, 107, 105, 110, 103, 268, 110, 268, 112, 112, 101, 97, 114, 97, 110, 99, 101, 46, 10, 72, 101, 121, 271, 114, 101, 44, 294, 33, 295, 101, 262, 105, 110, 103, 271, 273, 105, 122, 101, 114, 308, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 259, 269, 101, 101, 32, 105, 102, 32, 105, 116, 32, 104, 97, 110, 100, 108, 101, 115, 309, 101, 99, 105, 97, 108, 273, 115, 32, 112, 114, 111, 112, 101, 114, 108, 121, 46, 10, 83, 97, 108, 117, 116, 97, 116, 105, 111, 110, 115, 44, 32, 80, 108, 97, 110, 101, 116, 33, 296, 101, 273, 105, 122, 101, 114, 269, 104, 111, 117, 108, 100, 32, 114, 101, 99, 111, 103, 110, 105, 122, 101, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 32, 105, 110, 270, 267, 32, 108, 111, 110, 103, 32, 262, 114, 105, 110, 103, 32, 263, 304, 46, 10, 72, 101, 108, 108, 111, 268, 103, 97, 105, 110, 44, 294, 33, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 268, 114, 101, 309, 101, 99, 105, 97, 108, 273, 115, 270, 97, 116, 32, 110, 101, 101, 100, 259, 32, 98, 101, 32, 104, 97, 110, 100, 108, 101, 100, 32, 99, 111, 114, 114, 101, 99, 116, 108, 121, 32, 98, 121, 271, 273, 105, 122, 101, 114, 46, 10, 87, 101, 108, 99, 111, 109, 101, 44, 294, 33, 32, 73, 110, 99, 108, 117, 100, 105, 110, 103, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 32, 109, 117, 108, 116, 105, 112, 108, 101, 256, 105, 109, 101, 115, 32, 105, 110, 270, 267, 32, 108, 97, 114, 103, 101, 304, 259, 32, 260, 115, 117, 114, 101, 32, 112, 114, 111, 112, 101, 114, 32, 260, 99, 111, 100, 105, 110, 103, 46, 10, 72, 105, 44, 294, 33, 32, 76, 101, 116, 39, 115, 268, 100, 100, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 32, 105, 110, 32, 118, 97, 114, 105, 111, 117, 115, 32, 112, 97, 114, 116, 115, 32, 263, 270, 267, 32, 108, 111, 110, 103, 269, 260, 116, 260, 99, 101, 259, 256, 101, 262, 271, 273, 105, 122, 101, 114, 270, 111, 114, 111, 117, 103, 104, 108, 121, 46, 10, 261, 260, 100, 265, 266, 10]\n",
            "\n",
            "<|startoftext|>Hello, World! This is a sample text with the special tokens [SPECIAL1] and [SPECIAL2] to test the tokenizer.\n",
            "Hello, Universe! Another example sentence containing [SPECIAL1] and [SPECIAL2], used to ensure tokenizer's robustness.\n",
            "Greetings, Earth! Here we have [SPECIAL1] appearing once again, followed by [SPECIAL2] in the same sentence.\n",
            "Hello, World! This is yet another sample text, with [SPECIAL1] and [SPECIAL2] making an appearance.\n",
            "Hey there, World! Testing the tokenizer with [SPECIAL1] and [SPECIAL2] to see if it handles special tokens properly.\n",
            "Salutations, Planet! The tokenizer should recognize [SPECIAL1] and [SPECIAL2] in this long string of text.\n",
            "Hello again, World! [SPECIAL1] and [SPECIAL2] are special tokens that need to be handled correctly by the tokenizer.\n",
            "Welcome, World! Including [SPECIAL1] and [SPECIAL2] multiple times in this large text to ensure proper encoding.\n",
            "Hi, World! Let's add [SPECIAL1] and [SPECIAL2] in various parts of this long sentence to test the tokenizer thoroughly.\n",
            "<|endoftext|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bpetokenizer import BPETokenzier\n",
        "from bpetokenizer import BPETokenizer\n",
        "\n",
        "tokenizer = BPETokenizer.from_pretrained(\"wi17k_base\", verbose=True)\n",
        "\n",
        "texts = \"\"\"\n",
        "def get_stats(tokens, counts=None) -> dict:\n",
        "    \"Get statistics of the tokens. Includes the frequency of each consecutive pair of tokens\"\n",
        "    counts = if counts is None else counts\n",
        "    for pair in zip(tokens, tokens[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\"\"\"\n",
        "tokenizer.tokens(texts, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "mjcUEoqg4uiI",
        "outputId": "062eb48a-e87a-496d-af04-90d12a21447a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'BPETokenzier' from 'bpetokenizer' (/usr/local/lib/python3.11/dist-packages/bpetokenizer/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f6a7e657cb2a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbpetokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBPETokenzier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbpetokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBPETokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPETokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wi17k_base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'BPETokenzier' from 'bpetokenizer' (/usr/local/lib/python3.11/dist-packages/bpetokenizer/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "import re\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and the LLaMA tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")  # LLaMA 3 tokenizer\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi and the following day, I reflected on the story and the memories of what happened. I saw...�the� saidend númer Donactor engtransport К happeningameсе sur свіources dasert wayset adm\f inte составе\u000eump ble Val fonction func� empty End_{ usingнов\"\n",
        "\n",
        "# Clean the input text\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "\n",
        "# Tokenize with truncation\n",
        "input_ids = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMyWmKUW4uw3",
        "outputId": "f171a5fe-4f1d-4a6c-a823-79e7163dc635"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi and the following day I reflected on the story and the memories of what happened I sawthe saidend nmer Donactor engtransport happeningame sur ources dasert wayset adm inte ump ble Val fonction func empty End using\fumpession means should_{\u000e Кoption да Уulseанстю spherhttp some`.raсеновcbendency Dezemberxture diplom\"=>hal Richene ref númer寺 мор\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NousResearch/Llama-2-7b-chat-hf"
      ],
      "metadata": {
        "id": "bNscgjZv5ZXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, AutoTokenizer\n",
        "import re\n",
        "import torch\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/ blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and the LLaMA tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")  # LLaMA 3 tokenizer\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi and the following day, I reflected on the story and the memories of what happened. I saw...�the� saidend númer Donactor engtransport К happeningameсе sur свіources dasert wayset adm\f inte составе\u000eump ble Val fonction func� empty End_{ usingнов\"\n",
        "\n",
        "# Clean the input text\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "\n",
        "# Tokenize with truncation\n",
        "input_ids = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kjm24I85lph",
        "outputId": "3846cb59-2487-4b04-8deb-d25706a20f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using a model of type llama to instantiate a model of type bloom. This is not supported for all configurations of models and can yield errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta-llama/Llama-3.1-8B-Instruct"
      ],
      "metadata": {
        "id": "7HPGCMxV510Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WsqjHQfb77Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4UfjSmH77M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "from bytelatent.generate_blt import generate_nocache\n",
        "\n",
        "# 1) Point this to the folder containing:\n",
        "#    hf-weights/blt-1b/consolidated.safetensors\n",
        "#    hf-weights/blt-1b/entropy_model/consolidated.safetensors\n",
        "checkpoint_path = \"safetensors/blt_1b\"\n",
        "\n",
        "# 2) Load model, tokenizer, and training config\n",
        "model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(checkpoint_path)\n",
        "\n",
        "# 3) Build the patcher with realtime entropy\n",
        "patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "patcher_args.realtime_patching = True\n",
        "patcher_args.entropy_model_checkpoint_dir = f\"{checkpoint_path}/entropy_model\"\n",
        "patcher = patcher_args.build()\n",
        "\n",
        "# 4) Generate!\n",
        "prompt = \"Once upon a time\"\n",
        "outputs = generate_nocache([prompt], model=model, tokenizer=tokenizer, patcher=patcher)\n",
        "\n",
        "# 5) Decode and print\n",
        "for out_ids in outputs:\n",
        "    print(tokenizer.decode(out_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "FePhzTB-77KR",
        "outputId": "39628cb0-2aa4-49a8-cff5-8a91c5a3af18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bytelatent'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-827ba4edff9a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbytelatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_consolidated_model_and_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbytelatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_blt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_nocache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 1) Point this to the folder containing:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#    hf-weights/blt-1b/consolidated.safetensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bytelatent'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from safetensors.torch import load_file\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.tokenizers.blt_tokenizer import BltTokenizer\n",
        "from bytelatent.generate_blt import generate_nocache\n",
        "from bytelatent.train.config import TrainConfig  # adjust import to where your config loader lives\n",
        "\n",
        "# 1) Load raw weights\n",
        "model_weights   = load_file(\"/content/safetensors/ blt_1b/consolidated.safetensors\")\n",
        "entropy_weights = load_file(\"safetensors/entropy_model/consolidated.safetensors\")\n",
        "\n",
        "# 2) Instantiate your model and tokenizer\n",
        "#    -- you need a TrainConfig or Config object with the same architecture\n",
        "train_cfg = TrainConfig.from_yaml(\"path/to/your/train_config.yaml\")\n",
        "cfg       = train_cfg.model        # or however your config is stored\n",
        "model     = ByteLatentTransformer(cfg)\n",
        "tokenizer = BltTokenizer()\n",
        "\n",
        "# 3) Load weights into model\n",
        "model.load_state_dict(model_weights, strict=True)\n",
        "\n",
        "# 4) Build the patcher and load entropy model weights\n",
        "patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "patcher_args.realtime_patching = True\n",
        "# you may need to manually load entropy into the patcher submodule:\n",
        "patcher = patcher_args.build()\n",
        "patcher.entropy_model.load_state_dict(entropy_weights, strict=True)\n",
        "\n",
        "# 5) Run generation\n",
        "prompt  = \"Once upon a time\"\n",
        "outputs = generate_nocache([prompt], model=model, tokenizer=tokenizer, patcher=patcher)\n",
        "\n",
        "for out in outputs:\n",
        "    print(tokenizer.decode(out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "cgv3JPUh8MMZ",
        "outputId": "d8606fbe-2853-493b-e63c-a836441a220f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bytelatent'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8b6a13b35d02>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msafetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbytelatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mByteLatentTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbytelatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblt_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBltTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbytelatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_blt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_nocache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bytelatent'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/blt.git      # Or update your clone\n",
        "%cd blt\n",
        "!pip install -e .                                          # Reads setup.py and installs 'bytelatent' locally :contentReference[oaicite:5]{index=5}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vdm6-nb8cWJ",
        "outputId": "dfb5d3a1-94a8-4e35-99e3-fd0c71575db3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'blt'...\n",
            "remote: Enumerating objects: 1133, done.\u001b[K\n",
            "remote: Counting objects: 100% (303/303), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 1133 (delta 244), reused 214 (delta 214), pack-reused 830 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1133/1133), 611.70 KiB | 10.73 MiB/s, done.\n",
            "Resolving deltas: 100% (737/737), done.\n",
            "/content/blt\n",
            "Obtaining file:///content/blt\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from bytelatent==0.1.0) (0.2.0)\n",
            "Collecting tiktoken (from bytelatent==0.1.0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting xformers (from bytelatent==0.1.0)\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->bytelatent==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->bytelatent==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers->bytelatent==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from xformers->bytelatent==0.1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->xformers->bytelatent==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->xformers->bytelatent==0.1.0) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.6/664.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:56\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n",
            "    unknown package:\n",
            "        Expected sha256 165764f44ef8c61fcdfdfdbe769d687e06374059fbb388b6c89ecb0e28793a6f\n",
            "             Got        730400f46ed383778979040b8b811fbfa2a4f3e52c3751344d853ce2f4762014\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/facebookresearch/blt.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VW-Cn-kM8gvw",
        "outputId": "7b5025f6-0286-4ba6-820f-03b76dddb46f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/blt.git\n",
            "  Cloning https://github.com/facebookresearch/blt.git to /tmp/pip-req-build-qh4qrm_8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/blt.git /tmp/pip-req-build-qh4qrm_8\n",
            "  Resolved https://github.com/facebookresearch/blt.git to commit 1b67cbe02202d312eafa4153bf7d1442dac5ce49\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from bytelatent==0.1.0) (0.2.0)\n",
            "Collecting tiktoken (from bytelatent==0.1.0)\n",
            "  Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting xformers (from bytelatent==0.1.0)\n",
            "  Using cached xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->bytelatent==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->bytelatent==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers->bytelatent==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from xformers->bytelatent==0.1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->xformers->bytelatent==0.1.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers->bytelatent==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->xformers->bytelatent==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->xformers->bytelatent==0.1.0) (3.0.2)\n",
            "Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Building wheels for collected packages: bytelatent\n",
            "  Building wheel for bytelatent (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bytelatent: filename=bytelatent-0.1.0-py3-none-any.whl size=147313 sha256=8e1731cde2184538577e8b863190a3efe5a82c0162eafb63e528c2b730e977f9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dkzg95tf/wheels/3d/64/37/67643000e3de9ec8337f8a465a24a1953deec045a9456d1270\n",
            "Successfully built bytelatent\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, xformers, bytelatent\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bytelatent-0.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tiktoken-0.9.0 xformers-0.0.29.post3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import bytelatent; print(bytelatent.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlqeyTg-8rv4",
        "outputId": "64df3dd6-276b-4afb-804a-7dbd9ede1a2b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "AttributeError: module 'bytelatent' has no attribute '__version__'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bytelatent --v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6rHdmh89dRK",
        "outputId": "c94d0492-cc31-4389-e9f2-e35da01c4c7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: bytelatent: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# Load BLT-1B model\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n"
      ],
      "metadata": {
        "id": "wVXvLDnb9few"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load BLT-1B model\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Define model architecture\n",
        "class BLT1BModel(nn.Module):\n",
        "    def __init__(self, model_weights, entropy_weights):\n",
        "        super().__init__()\n",
        "        self.config = self._extract_config_from_weights(model_weights)\n",
        "\n",
        "        # Initialize model components\n",
        "        self.token_embeddings = nn.Embedding(self.config['vocab_size'], self.config['d_model'])\n",
        "        self.position_embeddings = nn.Embedding(self.config['max_position_embeddings'], self.config['d_model'])\n",
        "\n",
        "        # Initialize transformer layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(self.config) for _ in range(self.config['n_layers'])\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.norm = nn.LayerNorm(self.config['d_model'])\n",
        "        self.lm_head = nn.Linear(self.config['d_model'], self.config['vocab_size'], bias=False)\n",
        "\n",
        "        # Load weights\n",
        "        self._load_weights(model_weights)\n",
        "\n",
        "        # Entropy model\n",
        "        self.entropy_model = EntropyModel(entropy_weights)\n",
        "\n",
        "    def _extract_config_from_weights(self, weights):\n",
        "        # Extract model dimensions from weight shapes\n",
        "        config = {\n",
        "            'vocab_size': weights['token_embeddings.weight'].shape[0],\n",
        "            'd_model': weights['token_embeddings.weight'].shape[1],\n",
        "            'n_layers': sum(1 for k in weights if 'layers' in k and 'attention.q_proj.weight' in k),\n",
        "            'n_heads': weights['layers.0.attention.q_proj.weight'].shape[0] // weights['layers.0.attention.q_proj.weight'].shape[1],\n",
        "            'max_position_embeddings': weights.get('position_embeddings.weight', torch.zeros(2048, 768)).shape[0],\n",
        "        }\n",
        "        return config\n",
        "\n",
        "    def _load_weights(self, weights):\n",
        "        # Load embeddings\n",
        "        self.token_embeddings.weight.data = weights['token_embeddings.weight']\n",
        "        if 'position_embeddings.weight' in weights:\n",
        "            self.position_embeddings.weight.data = weights['position_embeddings.weight']\n",
        "\n",
        "        # Load transformer layers\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            prefix = f'layers.{i}.'\n",
        "            # Load attention weights\n",
        "            layer.attention.q_proj.weight.data = weights[f'{prefix}attention.q_proj.weight']\n",
        "            layer.attention.k_proj.weight.data = weights[f'{prefix}attention.k_proj.weight']\n",
        "            layer.attention.v_proj.weight.data = weights[f'{prefix}attention.v_proj.weight']\n",
        "            layer.attention.out_proj.weight.data = weights[f'{prefix}attention.out_proj.weight']\n",
        "\n",
        "            # Load MLP weights\n",
        "            layer.mlp.fc1.weight.data = weights[f'{prefix}mlp.fc1.weight']\n",
        "            layer.mlp.fc2.weight.data = weights[f'{prefix}mlp.fc2.weight']\n",
        "\n",
        "            # Load norms\n",
        "            layer.attention_norm.weight.data = weights[f'{prefix}attention_norm.weight']\n",
        "            layer.ffn_norm.weight.data = weights[f'{prefix}ffn_norm.weight']\n",
        "\n",
        "        # Load output layer\n",
        "        self.norm.weight.data = weights['norm.weight']\n",
        "        self.lm_head.weight.data = weights['lm_head.weight']\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Get sequence length\n",
        "        seq_length = input_ids.size(1)\n",
        "\n",
        "        # Get embeddings\n",
        "        token_emb = self.token_embeddings(input_ids)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        position_ids = torch.arange(0, seq_length, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
        "        position_emb = self.position_embeddings(position_ids)\n",
        "\n",
        "        hidden_states = token_emb + position_emb\n",
        "\n",
        "        # Apply transformer layers\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states, attention_mask)\n",
        "\n",
        "        # Apply final normalization\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.lm_head(hidden_states)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=1.0, top_k=50, top_p=0.95):\n",
        "        \"\"\"Generate text using the model\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current sequence\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # Apply entropy model for better sampling\n",
        "                next_token_logits = self.entropy_model(next_token_logits)\n",
        "\n",
        "                # Apply top-k filtering\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # Apply top-p (nucleus) filtering\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # Remove tokens with cumulative probability above the threshold\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # Shift the indices to the right to keep also the first token above the threshold\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # Sample next token\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # Append next token to the sequence\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # Check if we've generated an EOS token\n",
        "                if next_token[0, 0].item() == self.config.get('eos_token_id', -1):\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = SelfAttention(config)\n",
        "        self.attention_norm = nn.LayerNorm(config['d_model'])\n",
        "        self.mlp = MLP(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        # Self-attention with residual connection\n",
        "        attn_output = self.attention(self.attention_norm(x), attention_mask)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # MLP with residual connection\n",
        "        mlp_output = self.mlp(self.ffn_norm(x))\n",
        "        x = x + mlp_output\n",
        "\n",
        "        return x\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.out_proj = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Project queries, keys, values\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            scores = scores + attention_mask\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape and apply output projection\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.out_proj(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(config['d_model'], 4 * config['d_model'], bias=False)\n",
        "        self.fc2 = nn.Linear(4 * config['d_model'], config['d_model'], bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class EntropyModel(nn.Module):\n",
        "    def __init__(self, weights):\n",
        "        super().__init__()\n",
        "        # Extract dimensions from weights\n",
        "        hidden_dim = weights.get('fc1.weight', torch.zeros(1, 768)).shape[1]\n",
        "        output_dim = weights.get('fc2.weight', torch.zeros(768, 1)).shape[0]\n",
        "\n",
        "        # Define entropy model layers\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
        "        self.fc2 = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "        # Load weights\n",
        "        self._load_weights(weights)\n",
        "\n",
        "    def _load_weights(self, weights):\n",
        "        # Load weights if available\n",
        "        if 'fc1.weight' in weights:\n",
        "            self.fc1.weight.data = weights['fc1.weight']\n",
        "            if 'fc1.bias' in weights:\n",
        "                self.fc1.bias.data = weights['fc1.bias']\n",
        "\n",
        "        if 'fc2.weight' in weights:\n",
        "            self.fc2.weight.data = weights['fc2.weight']\n",
        "            if 'fc2.bias' in weights:\n",
        "                self.fc2.bias.data = weights['fc2.bias']\n",
        "\n",
        "    def forward(self, logits):\n",
        "        # Apply entropy adjustment to logits\n",
        "        x = F.relu(self.fc1(logits))\n",
        "        entropy_adjustment = self.fc2(x)\n",
        "        return logits + entropy_adjustment\n",
        "\n",
        "# Example usage\n",
        "def load_tokenizer():\n",
        "    # Placeholder for tokenizer loading code\n",
        "    # You would need to implement this based on your tokenizer\n",
        "    pass\n",
        "\n",
        "def inference_example():\n",
        "    # Create the model\n",
        "    model = BLT1BModel(model_weights, entropy_weights)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = load_tokenizer()\n",
        "\n",
        "    # Example prompt\n",
        "    prompt = \"Once upon a time\"\n",
        "\n",
        "    # Tokenize input\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Generate text\n",
        "    output_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=100,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "    # Decode output\n",
        "    generated_text = tokenizer.decode(output_ids[0])\n",
        "    print(generated_text)\n",
        "\n",
        "# Uncomment to run inference\n",
        "# inference_example()"
      ],
      "metadata": {
        "id": "bZc7E_GU9uPg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "\n",
        "# دالة للتحقق من وجود الملفات\n",
        "def check_files():\n",
        "    print(\"جاري التحقق من وجود الملفات...\")\n",
        "\n",
        "    blt_path = '/content/safetensors/blt_1b/consolidated.safetensors'\n",
        "    entropy_path = '/content/safetensors/entropy_model/consolidated.safetensors'\n",
        "\n",
        "    if not os.path.exists(blt_path):\n",
        "        print(f\"خطأ: الملف غير موجود في المسار: {blt_path}\")\n",
        "    else:\n",
        "        print(f\"✓ تم العثور على ملف BLT-1B: {blt_path}\")\n",
        "\n",
        "    if not os.path.exists(entropy_path):\n",
        "        print(f\"خطأ: الملف غير موجود في المسار: {entropy_path}\")\n",
        "    else:\n",
        "        print(f\"✓ تم العثور على ملف نموذج Entropy: {entropy_path}\")\n",
        "\n",
        "# دالة لتحميل النماذج مع معالجة الأخطاء\n",
        "def load_models():\n",
        "    print(\"جاري محاولة تحميل النماذج...\")\n",
        "\n",
        "    try:\n",
        "        print(\"بدء تحميل نموذج BLT-1B...\")\n",
        "        start_time = time.time()\n",
        "        model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "        end_time = time.time()\n",
        "        print(f\"✓ تم تحميل نموذج BLT-1B بنجاح في {end_time - start_time:.2f} ثانية\")\n",
        "        print(f\"- عدد المفاتيح: {len(model_weights)}\")\n",
        "\n",
        "        # عرض بعض المفاتيح كعينة\n",
        "        keys = list(model_weights.keys())\n",
        "        if keys:\n",
        "            print(\"- أمثلة للمفاتيح:\")\n",
        "            for key in keys[:5]:  # عرض أول 5 مفاتيح\n",
        "                print(f\"  * {key}: شكل {model_weights[key].shape}\")\n",
        "\n",
        "        print(\"\\nبدء تحميل نموذج Entropy...\")\n",
        "        start_time = time.time()\n",
        "        entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "        end_time = time.time()\n",
        "        print(f\"✓ تم تحميل نموذج Entropy بنجاح في {end_time - start_time:.2f} ثانية\")\n",
        "        print(f\"- عدد المفاتيح: {len(entropy_weights)}\")\n",
        "\n",
        "        # عرض بعض المفاتيح كعينة\n",
        "        keys = list(entropy_weights.keys())\n",
        "        if keys:\n",
        "            print(\"- أمثلة للمفاتيح:\")\n",
        "            for key in keys[:5]:  # عرض أول 5 مفاتيح\n",
        "                print(f\"  * {key}: شكل {entropy_weights[key].shape}\")\n",
        "\n",
        "        return model_weights, entropy_weights\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"خطأ: الملف غير موجود: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ غير متوقع أثناء تحميل الملفات: {str(e)}\")\n",
        "\n",
        "    return None, None\n",
        "\n",
        "# استخراج معلومات التكوين من النموذج\n",
        "def extract_config(model_weights):\n",
        "    if not model_weights:\n",
        "        print(\"لا يمكن استخراج التكوين: النموذج غير محمل\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\nجاري استخراج معلومات التكوين من النموذج...\")\n",
        "\n",
        "    try:\n",
        "        # البحث عن مفاتيح مفيدة لاستخراج المعلومات\n",
        "        vocab_size = None\n",
        "        d_model = None\n",
        "        n_layers = 0\n",
        "\n",
        "        # البحث عن معلومات embedding\n",
        "        for key in model_weights.keys():\n",
        "            if 'embed' in key.lower() or 'token' in key.lower():\n",
        "                shape = model_weights[key].shape\n",
        "                print(f\"وجدت مصفوفة embedding محتملة: {key} بشكل {shape}\")\n",
        "                if len(shape) == 2:\n",
        "                    vocab_size = shape[0]\n",
        "                    d_model = shape[1]\n",
        "                    print(f\"- حجم المفردات المحتمل: {vocab_size}\")\n",
        "                    print(f\"- أبعاد النموذج المحتملة: {d_model}\")\n",
        "\n",
        "        # حساب عدد الطبقات\n",
        "        layer_keys = [key for key in model_weights.keys() if 'layers' in key]\n",
        "        if layer_keys:\n",
        "            layer_numbers = set()\n",
        "            for key in layer_keys:\n",
        "                parts = key.split('.')\n",
        "                for i, part in enumerate(parts):\n",
        "                    if part == 'layers' and i+1 < len(parts) and parts[i+1].isdigit():\n",
        "                        layer_numbers.add(int(parts[i+1]))\n",
        "\n",
        "            n_layers = max(layer_numbers) + 1 if layer_numbers else 0\n",
        "            print(f\"- عدد الطبقات المحتمل: {n_layers}\")\n",
        "\n",
        "        config = {\n",
        "            'vocab_size': vocab_size,\n",
        "            'd_model': d_model,\n",
        "            'n_layers': n_layers\n",
        "        }\n",
        "\n",
        "        print(\"تم استخراج التكوين بنجاح:\")\n",
        "        for key, value in config.items():\n",
        "            print(f\"- {key}: {value}\")\n",
        "\n",
        "        return config\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ أثناء استخراج التكوين: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# الدالة الرئيسية\n",
        "def main():\n",
        "    print(\"=== تشخيص نموذج BLT-1B ===\")\n",
        "\n",
        "    # التحقق من وجود الملفات\n",
        "    check_files()\n",
        "\n",
        "    # محاولة تحميل النماذج\n",
        "    model_weights, entropy_weights = load_models()\n",
        "\n",
        "    # استخراج التكوين إذا تم تحميل النموذج بنجاح\n",
        "    if model_weights is not None:\n",
        "        config = extract_config(model_weights)\n",
        "\n",
        "    print(\"\\nاكتمل التشخيص.\")\n",
        "    print(\"يمكنك الآن استخدام معلومات التكوين لبناء نموذج كامل استناداً إلى هذه البيانات.\")\n",
        "\n",
        "# تشغيل الكود\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v5yRZOc-rVS",
        "outputId": "b9755187-f419-444a-9fc2-38e3ea33d1cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== تشخيص نموذج BLT-1B ===\n",
            "جاري التحقق من وجود الملفات...\n",
            "✓ تم العثور على ملف BLT-1B: /content/safetensors/blt_1b/consolidated.safetensors\n",
            "✓ تم العثور على ملف نموذج Entropy: /content/safetensors/entropy_model/consolidated.safetensors\n",
            "جاري محاولة تحميل النماذج...\n",
            "بدء تحميل نموذج BLT-1B...\n",
            "✓ تم تحميل نموذج BLT-1B بنجاح في 0.04 ثانية\n",
            "- عدد المفاتيح: 386\n",
            "- أمثلة للمفاتيح:\n",
            "  * encoder_hash_tok_embedding.0.weight: شكل torch.Size([500002, 1024])\n",
            "  * encoder_hash_tok_embedding.1.weight: شكل torch.Size([500002, 1024])\n",
            "  * encoder_hash_tok_embedding.2.weight: شكل torch.Size([500002, 1024])\n",
            "  * encoder_hash_tok_embedding.3.weight: شكل torch.Size([500002, 1024])\n",
            "  * encoder_hash_tok_embedding.4.weight: شكل torch.Size([500002, 1024])\n",
            "\n",
            "بدء تحميل نموذج Entropy...\n",
            "✓ تم تحميل نموذج Entropy بنجاح في 0.01 ثانية\n",
            "- عدد المفاتيح: 129\n",
            "- أمثلة للمفاتيح:\n",
            "  * layers.0.attention.wk.weight: شكل torch.Size([768, 768])\n",
            "  * layers.0.attention.wo.weight: شكل torch.Size([768, 768])\n",
            "  * layers.0.attention.wq.weight: شكل torch.Size([768, 768])\n",
            "  * layers.0.attention.wv.weight: شكل torch.Size([768, 768])\n",
            "  * layers.0.attention_norm.weight: شكل torch.Size([768])\n",
            "\n",
            "جاري استخراج معلومات التكوين من النموذج...\n",
            "وجدت مصفوفة embedding محتملة: encoder_hash_tok_embedding.0.weight بشكل torch.Size([500002, 1024])\n",
            "- حجم المفردات المحتمل: 500002\n",
            "- أبعاد النموذج المحتملة: 1024\n",
            "وجدت مصفوفة embedding محتملة: encoder_hash_tok_embedding.1.weight بشكل torch.Size([500002, 1024])\n",
            "- حجم المفردات المحتمل: 500002\n",
            "- أبعاد النموذج المحتملة: 1024\n",
            "وجدت مصفوفة embedding محتملة: encoder_hash_tok_embedding.2.weight بشكل torch.Size([500002, 1024])\n",
            "- حجم المفردات المحتمل: 500002\n",
            "- أبعاد النموذج المحتملة: 1024\n",
            "وجدت مصفوفة embedding محتملة: encoder_hash_tok_embedding.3.weight بشكل torch.Size([500002, 1024])\n",
            "- حجم المفردات المحتمل: 500002\n",
            "- أبعاد النموذج المحتملة: 1024\n",
            "وجدت مصفوفة embedding محتملة: encoder_hash_tok_embedding.4.weight بشكل torch.Size([500002, 1024])\n",
            "- حجم المفردات المحتمل: 500002\n",
            "- أبعاد النموذج المحتملة: 1024\n",
            "وجدت مصفوفة embedding محتملة: encoder_hash_tok_embedding.5.weight بشكل torch.Size([500002, 1024])\n",
            "- حجم المفردات المحتمل: 500002\n",
            "- أبعاد النموذج المحتملة: 1024\n",
            "وجدت مصفوفة embedding محتملة: local_decoder.patch_embedding_projection.weight بشكل torch.Size([2048, 2048])\n",
            "- حجم المفردات المحتمل: 2048\n",
            "- أبعاد النموذج المحتملة: 2048\n",
            "وجدت مصفوفة embedding محتملة: local_encoder.patch_embedding_projection.weight بشكل torch.Size([2048, 1024])\n",
            "- حجم المفردات المحتمل: 2048\n",
            "- أبعاد النموذج المحتملة: 1024\n",
            "وجدت مصفوفة embedding محتملة: local_encoder.tok_embeddings.weight بشكل torch.Size([260, 1024])\n",
            "- حجم المفردات المحتمل: 260\n",
            "- أبعاد النموذج المحتملة: 1024\n",
            "- عدد الطبقات المحتمل: 25\n",
            "تم استخراج التكوين بنجاح:\n",
            "- vocab_size: 260\n",
            "- d_model: 1024\n",
            "- n_layers: 25\n",
            "\n",
            "اكتمل التشخيص.\n",
            "يمكنك الآن استخدام معلومات التكوين لبناء نموذج كامل استناداً إلى هذه البيانات.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,  # حجم المفردات من التشخيص\n",
        "    'd_model': 1024,    # أبعاد النموذج من التشخيص\n",
        "    'n_layers': 25,     # عدد الطبقات من التشخيص\n",
        "    'n_heads': 16,      # عدد رؤوس الانتباه (تقدير: عادة ما تكون d_model/64)\n",
        "    'max_seq_len': 2048 # الحد الأقصى لطول التسلسل (تقدير)\n",
        "}\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "\n",
        "        # مشابه للمفاتيح التي وجدناها في التشخيص\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wo = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # تطبيق التحويلات الخطية\n",
        "        q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.wk(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.wv(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # حساب درجات الانتباه\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # تطبيق القناع إذا كان موجودًا\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # تطبيق softmax للحصول على أوزان الانتباه\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # تطبيق أوزان الانتباه على القيم\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # إعادة تشكيل وتطبيق الإسقاط النهائي\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.wo(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "class BLTFFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(config['d_model'], 4 * config['d_model'], bias=False)\n",
        "        self.w2 = nn.Linear(4 * config['d_model'], config['d_model'], bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.gelu(self.w1(x)))\n",
        "\n",
        "class BLTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BLTAttention(config)\n",
        "        self.attention_norm = nn.LayerNorm(config['d_model'])\n",
        "        self.ffn = BLTFFN(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # طبقة الانتباه مع اتصال متبقي\n",
        "        attn_output = self.attention(self.attention_norm(x), mask)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # طبقة FFN مع اتصال متبقي\n",
        "        ffn_output = self.ffn(self.ffn_norm(x))\n",
        "        x = x + ffn_output\n",
        "\n",
        "        return x\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # طبقة التضمين\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "\n",
        "        # طبقات المحول\n",
        "        self.layers = nn.ModuleList([\n",
        "            BLTBlock(config) for _ in range(config['n_layers'])\n",
        "        ])\n",
        "\n",
        "        # طبقة التطبيع النهائية\n",
        "        self.norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "        # رأس التوقع\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "    def _init_weights_from_model(self, weights):\n",
        "        print(\"تهيئة أوزان النموذج من الملف المحمل...\")\n",
        "\n",
        "        # قائمة المفاتيح التي تم تحميلها بنجاح\n",
        "        loaded_keys = []\n",
        "\n",
        "        # تحميل التضمينات\n",
        "        if 'local_encoder.tok_embeddings.weight' in weights:\n",
        "            self.tok_embeddings.weight.data = weights['local_encoder.tok_embeddings.weight']\n",
        "            loaded_keys.append('local_encoder.tok_embeddings.weight')\n",
        "\n",
        "        # تحميل طبقات المحول\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            prefix = f'layers.{i}.'\n",
        "\n",
        "            # تحميل أوزان الانتباه\n",
        "            for name, param in [\n",
        "                ('attention.wq.weight', layer.attention.wq.weight),\n",
        "                ('attention.wk.weight', layer.attention.wk.weight),\n",
        "                ('attention.wv.weight', layer.attention.wv.weight),\n",
        "                ('attention.wo.weight', layer.attention.wo.weight),\n",
        "                ('attention_norm.weight', layer.attention_norm.weight),\n",
        "                ('ffn_norm.weight', layer.ffn_norm.weight)\n",
        "            ]:\n",
        "                key = prefix + name\n",
        "                if key in weights:\n",
        "                    param.data = weights[key]\n",
        "                    loaded_keys.append(key)\n",
        "\n",
        "            # تحميل أوزان FFN\n",
        "            # قد تختلف الأسماء حسب النموذج الفعلي\n",
        "            ffn_keys = [k for k in weights.keys() if prefix in k and ('feed_forward' in k or 'ffn' in k)]\n",
        "            if len(ffn_keys) >= 2:  # نفترض وجود وزنين على الأقل لطبقة FFN\n",
        "                for key in ffn_keys:\n",
        "                    if 'w1' in key or 'fc1' in key:\n",
        "                        layer.ffn.w1.weight.data = weights[key]\n",
        "                        loaded_keys.append(key)\n",
        "                    elif 'w2' in key or 'fc2' in key:\n",
        "                        layer.ffn.w2.weight.data = weights[key]\n",
        "                        loaded_keys.append(key)\n",
        "\n",
        "        # تحميل طبقة التطبيع النهائية\n",
        "        norm_keys = [k for k in weights.keys() if 'norm' in k and not any(x in k for x in ['attention', 'ffn'])]\n",
        "        if norm_keys:\n",
        "            self.norm.weight.data = weights[norm_keys[0]]\n",
        "            loaded_keys.append(norm_keys[0])\n",
        "\n",
        "        # تحميل رأس الإخراج\n",
        "        output_keys = [k for k in weights.keys() if 'lm_head' in k or 'output' in k]\n",
        "        if output_keys:\n",
        "            self.output.weight.data = weights[output_keys[0]]\n",
        "            loaded_keys.append(output_keys[0])\n",
        "\n",
        "        print(f\"تم تحميل {len(loaded_keys)} من أصل {len(weights)} مفتاح من النموذج\")\n",
        "        return loaded_keys\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        # الحصول على التضمينات\n",
        "        h = self.tok_embeddings(input_ids)\n",
        "\n",
        "        # إنشاء قناع الانتباه إذا لم يتم توفيره\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones((batch_size, seq_len, seq_len), device=input_ids.device)\n",
        "\n",
        "        # تطبيق طبقات المحول\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, attention_mask)\n",
        "\n",
        "        # تطبيق التطبيع النهائي\n",
        "        h = self.norm(h)\n",
        "\n",
        "        # الحصول على التوقعات\n",
        "        logits = self.output(h)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"توليد النص باستخدام النموذج\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # الحصول على نتائج النموذج للتسلسل الحالي\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # تطبيق تصفية top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # تطبيق تصفية top-p (nucleus)\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # إزالة الرموز ذات الاحتمال التراكمي فوق العتبة\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # نقل المؤشرات إلى اليمين للحفاظ أيضًا على الرمز الأول فوق العتبة\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # اختيار الرمز التالي\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # إضافة الرمز التالي إلى التسلسل\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # التحقق مما إذا كنا قد أنشأنا رمز EOS\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "def load_tokenizer(vocab_size=260):\n",
        "    \"\"\"استبدال مؤقت للمحول - تنفيذ بسيط\"\"\"\n",
        "    class SimpleTokenizer:\n",
        "        def __init__(self, vocab_size):\n",
        "            self.vocab_size = vocab_size\n",
        "\n",
        "        def __call__(self, text, return_tensors=\"pt\"):\n",
        "            # هذا مجرد محاكاة، في الواقع تحتاج إلى تنفيذ التحويل الفعلي\n",
        "            # هنا نقوم بتحويل كل حرف إلى رقم بسيط\n",
        "            tokens = [ord(c) % (self.vocab_size - 4) + 4 for c in text]\n",
        "            if return_tensors == \"pt\":\n",
        "                return SimpleNamespace(input_ids=torch.tensor([tokens]))\n",
        "            return tokens\n",
        "\n",
        "        def decode(self, ids):\n",
        "            # تحويل الرموز مرة أخرى إلى نص\n",
        "            if isinstance(ids, torch.Tensor):\n",
        "                ids = ids.tolist()\n",
        "            text = \"\"\n",
        "            for id in ids:\n",
        "                if id >= 4 and id < self.vocab_size:\n",
        "                    text += chr((id - 4) % 26 + 97)  # تحويل بسيط إلى أحرف\n",
        "            return text\n",
        "\n",
        "    return SimpleTokenizer(vocab_size)\n",
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "# دالة لتنفيذ النموذج واختباره\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    tokenizer = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0])\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# تشغيل النموذج\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "04KkrMt3-rn5",
        "outputId": "29ea59d0-4d31-46b6-b466-d668c3c9a9d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n",
            "تهيئة أوزان النموذج من الملف المحمل...\n",
            "تم تحميل 73 من أصل 386 مفتاح من النموذج\n",
            "تجهيز واختبار المحول...\n",
            "اختبار النموذج مع مثال بسيط...\n",
            "المدخل: 'hi'\n",
            "رموز المدخل: [[108, 109]]\n",
            "بدء التوليد...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expected m1 and m2 to have the same dtype, but got: c10::BFloat16 != float",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-93ac80a83379>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;31m# تشغيل النموذج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-93ac80a83379>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"بدء التوليد...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         output_ids = model.generate(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-93ac80a83379>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;31m# الحصول على نتائج النموذج للتسلسل الحالي\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-93ac80a83379>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# تطبيق طبقات المحول\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# تطبيق التطبيع النهائي\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-93ac80a83379>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# طبقة الانتباه مع اتصال متبقي\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-93ac80a83379>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# تطبيق التحويلات الخطية\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: c10::BFloat16 != float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([key for key in model_weights.keys()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR_Kkfpb_U4S",
        "outputId": "df85cb66-1e63-4050-8874-56ba9a9abc5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['encoder_hash_tok_embedding.0.weight', 'encoder_hash_tok_embedding.1.weight', 'encoder_hash_tok_embedding.2.weight', 'encoder_hash_tok_embedding.3.weight', 'encoder_hash_tok_embedding.4.weight', 'encoder_hash_tok_embedding.5.weight', 'global_transformer.layers.0.attention.wk.weight', 'global_transformer.layers.0.attention.wo.weight', 'global_transformer.layers.0.attention.wq.weight', 'global_transformer.layers.0.attention.wv.weight', 'global_transformer.layers.0.attention_norm.weight', 'global_transformer.layers.0.feed_forward.w1.weight', 'global_transformer.layers.0.feed_forward.w2.weight', 'global_transformer.layers.0.feed_forward.w3.weight', 'global_transformer.layers.0.ffn_norm.weight', 'global_transformer.layers.1.attention.wk.weight', 'global_transformer.layers.1.attention.wo.weight', 'global_transformer.layers.1.attention.wq.weight', 'global_transformer.layers.1.attention.wv.weight', 'global_transformer.layers.1.attention_norm.weight', 'global_transformer.layers.1.feed_forward.w1.weight', 'global_transformer.layers.1.feed_forward.w2.weight', 'global_transformer.layers.1.feed_forward.w3.weight', 'global_transformer.layers.1.ffn_norm.weight', 'global_transformer.layers.10.attention.wk.weight', 'global_transformer.layers.10.attention.wo.weight', 'global_transformer.layers.10.attention.wq.weight', 'global_transformer.layers.10.attention.wv.weight', 'global_transformer.layers.10.attention_norm.weight', 'global_transformer.layers.10.feed_forward.w1.weight', 'global_transformer.layers.10.feed_forward.w2.weight', 'global_transformer.layers.10.feed_forward.w3.weight', 'global_transformer.layers.10.ffn_norm.weight', 'global_transformer.layers.11.attention.wk.weight', 'global_transformer.layers.11.attention.wo.weight', 'global_transformer.layers.11.attention.wq.weight', 'global_transformer.layers.11.attention.wv.weight', 'global_transformer.layers.11.attention_norm.weight', 'global_transformer.layers.11.feed_forward.w1.weight', 'global_transformer.layers.11.feed_forward.w2.weight', 'global_transformer.layers.11.feed_forward.w3.weight', 'global_transformer.layers.11.ffn_norm.weight', 'global_transformer.layers.12.attention.wk.weight', 'global_transformer.layers.12.attention.wo.weight', 'global_transformer.layers.12.attention.wq.weight', 'global_transformer.layers.12.attention.wv.weight', 'global_transformer.layers.12.attention_norm.weight', 'global_transformer.layers.12.feed_forward.w1.weight', 'global_transformer.layers.12.feed_forward.w2.weight', 'global_transformer.layers.12.feed_forward.w3.weight', 'global_transformer.layers.12.ffn_norm.weight', 'global_transformer.layers.13.attention.wk.weight', 'global_transformer.layers.13.attention.wo.weight', 'global_transformer.layers.13.attention.wq.weight', 'global_transformer.layers.13.attention.wv.weight', 'global_transformer.layers.13.attention_norm.weight', 'global_transformer.layers.13.feed_forward.w1.weight', 'global_transformer.layers.13.feed_forward.w2.weight', 'global_transformer.layers.13.feed_forward.w3.weight', 'global_transformer.layers.13.ffn_norm.weight', 'global_transformer.layers.14.attention.wk.weight', 'global_transformer.layers.14.attention.wo.weight', 'global_transformer.layers.14.attention.wq.weight', 'global_transformer.layers.14.attention.wv.weight', 'global_transformer.layers.14.attention_norm.weight', 'global_transformer.layers.14.feed_forward.w1.weight', 'global_transformer.layers.14.feed_forward.w2.weight', 'global_transformer.layers.14.feed_forward.w3.weight', 'global_transformer.layers.14.ffn_norm.weight', 'global_transformer.layers.15.attention.wk.weight', 'global_transformer.layers.15.attention.wo.weight', 'global_transformer.layers.15.attention.wq.weight', 'global_transformer.layers.15.attention.wv.weight', 'global_transformer.layers.15.attention_norm.weight', 'global_transformer.layers.15.feed_forward.w1.weight', 'global_transformer.layers.15.feed_forward.w2.weight', 'global_transformer.layers.15.feed_forward.w3.weight', 'global_transformer.layers.15.ffn_norm.weight', 'global_transformer.layers.16.attention.wk.weight', 'global_transformer.layers.16.attention.wo.weight', 'global_transformer.layers.16.attention.wq.weight', 'global_transformer.layers.16.attention.wv.weight', 'global_transformer.layers.16.attention_norm.weight', 'global_transformer.layers.16.feed_forward.w1.weight', 'global_transformer.layers.16.feed_forward.w2.weight', 'global_transformer.layers.16.feed_forward.w3.weight', 'global_transformer.layers.16.ffn_norm.weight', 'global_transformer.layers.17.attention.wk.weight', 'global_transformer.layers.17.attention.wo.weight', 'global_transformer.layers.17.attention.wq.weight', 'global_transformer.layers.17.attention.wv.weight', 'global_transformer.layers.17.attention_norm.weight', 'global_transformer.layers.17.feed_forward.w1.weight', 'global_transformer.layers.17.feed_forward.w2.weight', 'global_transformer.layers.17.feed_forward.w3.weight', 'global_transformer.layers.17.ffn_norm.weight', 'global_transformer.layers.18.attention.wk.weight', 'global_transformer.layers.18.attention.wo.weight', 'global_transformer.layers.18.attention.wq.weight', 'global_transformer.layers.18.attention.wv.weight', 'global_transformer.layers.18.attention_norm.weight', 'global_transformer.layers.18.feed_forward.w1.weight', 'global_transformer.layers.18.feed_forward.w2.weight', 'global_transformer.layers.18.feed_forward.w3.weight', 'global_transformer.layers.18.ffn_norm.weight', 'global_transformer.layers.19.attention.wk.weight', 'global_transformer.layers.19.attention.wo.weight', 'global_transformer.layers.19.attention.wq.weight', 'global_transformer.layers.19.attention.wv.weight', 'global_transformer.layers.19.attention_norm.weight', 'global_transformer.layers.19.feed_forward.w1.weight', 'global_transformer.layers.19.feed_forward.w2.weight', 'global_transformer.layers.19.feed_forward.w3.weight', 'global_transformer.layers.19.ffn_norm.weight', 'global_transformer.layers.2.attention.wk.weight', 'global_transformer.layers.2.attention.wo.weight', 'global_transformer.layers.2.attention.wq.weight', 'global_transformer.layers.2.attention.wv.weight', 'global_transformer.layers.2.attention_norm.weight', 'global_transformer.layers.2.feed_forward.w1.weight', 'global_transformer.layers.2.feed_forward.w2.weight', 'global_transformer.layers.2.feed_forward.w3.weight', 'global_transformer.layers.2.ffn_norm.weight', 'global_transformer.layers.20.attention.wk.weight', 'global_transformer.layers.20.attention.wo.weight', 'global_transformer.layers.20.attention.wq.weight', 'global_transformer.layers.20.attention.wv.weight', 'global_transformer.layers.20.attention_norm.weight', 'global_transformer.layers.20.feed_forward.w1.weight', 'global_transformer.layers.20.feed_forward.w2.weight', 'global_transformer.layers.20.feed_forward.w3.weight', 'global_transformer.layers.20.ffn_norm.weight', 'global_transformer.layers.21.attention.wk.weight', 'global_transformer.layers.21.attention.wo.weight', 'global_transformer.layers.21.attention.wq.weight', 'global_transformer.layers.21.attention.wv.weight', 'global_transformer.layers.21.attention_norm.weight', 'global_transformer.layers.21.feed_forward.w1.weight', 'global_transformer.layers.21.feed_forward.w2.weight', 'global_transformer.layers.21.feed_forward.w3.weight', 'global_transformer.layers.21.ffn_norm.weight', 'global_transformer.layers.22.attention.wk.weight', 'global_transformer.layers.22.attention.wo.weight', 'global_transformer.layers.22.attention.wq.weight', 'global_transformer.layers.22.attention.wv.weight', 'global_transformer.layers.22.attention_norm.weight', 'global_transformer.layers.22.feed_forward.w1.weight', 'global_transformer.layers.22.feed_forward.w2.weight', 'global_transformer.layers.22.feed_forward.w3.weight', 'global_transformer.layers.22.ffn_norm.weight', 'global_transformer.layers.23.attention.wk.weight', 'global_transformer.layers.23.attention.wo.weight', 'global_transformer.layers.23.attention.wq.weight', 'global_transformer.layers.23.attention.wv.weight', 'global_transformer.layers.23.attention_norm.weight', 'global_transformer.layers.23.feed_forward.w1.weight', 'global_transformer.layers.23.feed_forward.w2.weight', 'global_transformer.layers.23.feed_forward.w3.weight', 'global_transformer.layers.23.ffn_norm.weight', 'global_transformer.layers.24.attention.wk.weight', 'global_transformer.layers.24.attention.wo.weight', 'global_transformer.layers.24.attention.wq.weight', 'global_transformer.layers.24.attention.wv.weight', 'global_transformer.layers.24.attention_norm.weight', 'global_transformer.layers.24.feed_forward.w1.weight', 'global_transformer.layers.24.feed_forward.w2.weight', 'global_transformer.layers.24.feed_forward.w3.weight', 'global_transformer.layers.24.ffn_norm.weight', 'global_transformer.layers.3.attention.wk.weight', 'global_transformer.layers.3.attention.wo.weight', 'global_transformer.layers.3.attention.wq.weight', 'global_transformer.layers.3.attention.wv.weight', 'global_transformer.layers.3.attention_norm.weight', 'global_transformer.layers.3.feed_forward.w1.weight', 'global_transformer.layers.3.feed_forward.w2.weight', 'global_transformer.layers.3.feed_forward.w3.weight', 'global_transformer.layers.3.ffn_norm.weight', 'global_transformer.layers.4.attention.wk.weight', 'global_transformer.layers.4.attention.wo.weight', 'global_transformer.layers.4.attention.wq.weight', 'global_transformer.layers.4.attention.wv.weight', 'global_transformer.layers.4.attention_norm.weight', 'global_transformer.layers.4.feed_forward.w1.weight', 'global_transformer.layers.4.feed_forward.w2.weight', 'global_transformer.layers.4.feed_forward.w3.weight', 'global_transformer.layers.4.ffn_norm.weight', 'global_transformer.layers.5.attention.wk.weight', 'global_transformer.layers.5.attention.wo.weight', 'global_transformer.layers.5.attention.wq.weight', 'global_transformer.layers.5.attention.wv.weight', 'global_transformer.layers.5.attention_norm.weight', 'global_transformer.layers.5.feed_forward.w1.weight', 'global_transformer.layers.5.feed_forward.w2.weight', 'global_transformer.layers.5.feed_forward.w3.weight', 'global_transformer.layers.5.ffn_norm.weight', 'global_transformer.layers.6.attention.wk.weight', 'global_transformer.layers.6.attention.wo.weight', 'global_transformer.layers.6.attention.wq.weight', 'global_transformer.layers.6.attention.wv.weight', 'global_transformer.layers.6.attention_norm.weight', 'global_transformer.layers.6.feed_forward.w1.weight', 'global_transformer.layers.6.feed_forward.w2.weight', 'global_transformer.layers.6.feed_forward.w3.weight', 'global_transformer.layers.6.ffn_norm.weight', 'global_transformer.layers.7.attention.wk.weight', 'global_transformer.layers.7.attention.wo.weight', 'global_transformer.layers.7.attention.wq.weight', 'global_transformer.layers.7.attention.wv.weight', 'global_transformer.layers.7.attention_norm.weight', 'global_transformer.layers.7.feed_forward.w1.weight', 'global_transformer.layers.7.feed_forward.w2.weight', 'global_transformer.layers.7.feed_forward.w3.weight', 'global_transformer.layers.7.ffn_norm.weight', 'global_transformer.layers.8.attention.wk.weight', 'global_transformer.layers.8.attention.wo.weight', 'global_transformer.layers.8.attention.wq.weight', 'global_transformer.layers.8.attention.wv.weight', 'global_transformer.layers.8.attention_norm.weight', 'global_transformer.layers.8.feed_forward.w1.weight', 'global_transformer.layers.8.feed_forward.w2.weight', 'global_transformer.layers.8.feed_forward.w3.weight', 'global_transformer.layers.8.ffn_norm.weight', 'global_transformer.layers.9.attention.wk.weight', 'global_transformer.layers.9.attention.wo.weight', 'global_transformer.layers.9.attention.wq.weight', 'global_transformer.layers.9.attention.wv.weight', 'global_transformer.layers.9.attention_norm.weight', 'global_transformer.layers.9.feed_forward.w1.weight', 'global_transformer.layers.9.feed_forward.w2.weight', 'global_transformer.layers.9.feed_forward.w3.weight', 'global_transformer.layers.9.ffn_norm.weight', 'local_decoder.cross_attn_layers.0.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.0.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.0.wk.weight', 'local_decoder.cross_attn_layers.0.wo.weight', 'local_decoder.cross_attn_layers.0.wq.weight', 'local_decoder.cross_attn_layers.0.wv.weight', 'local_decoder.cross_attn_layers.1.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.1.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.1.wk.weight', 'local_decoder.cross_attn_layers.1.wo.weight', 'local_decoder.cross_attn_layers.1.wq.weight', 'local_decoder.cross_attn_layers.1.wv.weight', 'local_decoder.cross_attn_layers.2.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.2.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.2.wk.weight', 'local_decoder.cross_attn_layers.2.wo.weight', 'local_decoder.cross_attn_layers.2.wq.weight', 'local_decoder.cross_attn_layers.2.wv.weight', 'local_decoder.cross_attn_layers.3.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.3.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.3.wk.weight', 'local_decoder.cross_attn_layers.3.wo.weight', 'local_decoder.cross_attn_layers.3.wq.weight', 'local_decoder.cross_attn_layers.3.wv.weight', 'local_decoder.cross_attn_layers.4.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.4.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.4.wk.weight', 'local_decoder.cross_attn_layers.4.wo.weight', 'local_decoder.cross_attn_layers.4.wq.weight', 'local_decoder.cross_attn_layers.4.wv.weight', 'local_decoder.cross_attn_layers.5.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.5.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.5.wk.weight', 'local_decoder.cross_attn_layers.5.wo.weight', 'local_decoder.cross_attn_layers.5.wq.weight', 'local_decoder.cross_attn_layers.5.wv.weight', 'local_decoder.cross_attn_layers.6.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.6.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.6.wk.weight', 'local_decoder.cross_attn_layers.6.wo.weight', 'local_decoder.cross_attn_layers.6.wq.weight', 'local_decoder.cross_attn_layers.6.wv.weight', 'local_decoder.cross_attn_layers.7.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.7.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.7.wk.weight', 'local_decoder.cross_attn_layers.7.wo.weight', 'local_decoder.cross_attn_layers.7.wq.weight', 'local_decoder.cross_attn_layers.7.wv.weight', 'local_decoder.cross_attn_layers.8.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.8.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.8.wk.weight', 'local_decoder.cross_attn_layers.8.wo.weight', 'local_decoder.cross_attn_layers.8.wq.weight', 'local_decoder.cross_attn_layers.8.wv.weight', 'local_decoder.layers.0.attention.wk.weight', 'local_decoder.layers.0.attention.wo.weight', 'local_decoder.layers.0.attention.wq.weight', 'local_decoder.layers.0.attention.wv.weight', 'local_decoder.layers.0.attention_norm.weight', 'local_decoder.layers.0.feed_forward.w1.weight', 'local_decoder.layers.0.feed_forward.w2.weight', 'local_decoder.layers.0.feed_forward.w3.weight', 'local_decoder.layers.0.ffn_norm.weight', 'local_decoder.layers.1.attention.wk.weight', 'local_decoder.layers.1.attention.wo.weight', 'local_decoder.layers.1.attention.wq.weight', 'local_decoder.layers.1.attention.wv.weight', 'local_decoder.layers.1.attention_norm.weight', 'local_decoder.layers.1.feed_forward.w1.weight', 'local_decoder.layers.1.feed_forward.w2.weight', 'local_decoder.layers.1.feed_forward.w3.weight', 'local_decoder.layers.1.ffn_norm.weight', 'local_decoder.layers.2.attention.wk.weight', 'local_decoder.layers.2.attention.wo.weight', 'local_decoder.layers.2.attention.wq.weight', 'local_decoder.layers.2.attention.wv.weight', 'local_decoder.layers.2.attention_norm.weight', 'local_decoder.layers.2.feed_forward.w1.weight', 'local_decoder.layers.2.feed_forward.w2.weight', 'local_decoder.layers.2.feed_forward.w3.weight', 'local_decoder.layers.2.ffn_norm.weight', 'local_decoder.layers.3.attention.wk.weight', 'local_decoder.layers.3.attention.wo.weight', 'local_decoder.layers.3.attention.wq.weight', 'local_decoder.layers.3.attention.wv.weight', 'local_decoder.layers.3.attention_norm.weight', 'local_decoder.layers.3.feed_forward.w1.weight', 'local_decoder.layers.3.feed_forward.w2.weight', 'local_decoder.layers.3.feed_forward.w3.weight', 'local_decoder.layers.3.ffn_norm.weight', 'local_decoder.layers.4.attention.wk.weight', 'local_decoder.layers.4.attention.wo.weight', 'local_decoder.layers.4.attention.wq.weight', 'local_decoder.layers.4.attention.wv.weight', 'local_decoder.layers.4.attention_norm.weight', 'local_decoder.layers.4.feed_forward.w1.weight', 'local_decoder.layers.4.feed_forward.w2.weight', 'local_decoder.layers.4.feed_forward.w3.weight', 'local_decoder.layers.4.ffn_norm.weight', 'local_decoder.layers.5.attention.wk.weight', 'local_decoder.layers.5.attention.wo.weight', 'local_decoder.layers.5.attention.wq.weight', 'local_decoder.layers.5.attention.wv.weight', 'local_decoder.layers.5.attention_norm.weight', 'local_decoder.layers.5.feed_forward.w1.weight', 'local_decoder.layers.5.feed_forward.w2.weight', 'local_decoder.layers.5.feed_forward.w3.weight', 'local_decoder.layers.5.ffn_norm.weight', 'local_decoder.layers.6.attention.wk.weight', 'local_decoder.layers.6.attention.wo.weight', 'local_decoder.layers.6.attention.wq.weight', 'local_decoder.layers.6.attention.wv.weight', 'local_decoder.layers.6.attention_norm.weight', 'local_decoder.layers.6.feed_forward.w1.weight', 'local_decoder.layers.6.feed_forward.w2.weight', 'local_decoder.layers.6.feed_forward.w3.weight', 'local_decoder.layers.6.ffn_norm.weight', 'local_decoder.layers.7.attention.wk.weight', 'local_decoder.layers.7.attention.wo.weight', 'local_decoder.layers.7.attention.wq.weight', 'local_decoder.layers.7.attention.wv.weight', 'local_decoder.layers.7.attention_norm.weight', 'local_decoder.layers.7.feed_forward.w1.weight', 'local_decoder.layers.7.feed_forward.w2.weight', 'local_decoder.layers.7.feed_forward.w3.weight', 'local_decoder.layers.7.ffn_norm.weight', 'local_decoder.layers.8.attention.wk.weight', 'local_decoder.layers.8.attention.wo.weight', 'local_decoder.layers.8.attention.wq.weight', 'local_decoder.layers.8.attention.wv.weight', 'local_decoder.layers.8.attention_norm.weight', 'local_decoder.layers.8.feed_forward.w1.weight', 'local_decoder.layers.8.feed_forward.w2.weight', 'local_decoder.layers.8.feed_forward.w3.weight', 'local_decoder.layers.8.ffn_norm.weight', 'local_decoder.norm.weight', 'local_decoder.output.weight', 'local_decoder.patch_embedding_projection.weight', 'local_encoder.cross_attn_layers.0.cross_attn_norm_kv.weight', 'local_encoder.cross_attn_layers.0.cross_attn_norm_q.weight', 'local_encoder.cross_attn_layers.0.wk.weight', 'local_encoder.cross_attn_layers.0.wo.weight', 'local_encoder.cross_attn_layers.0.wq.weight', 'local_encoder.cross_attn_layers.0.wv.weight', 'local_encoder.layers.0.attention.wk.weight', 'local_encoder.layers.0.attention.wo.weight', 'local_encoder.layers.0.attention.wq.weight', 'local_encoder.layers.0.attention.wv.weight', 'local_encoder.layers.0.attention_norm.weight', 'local_encoder.layers.0.feed_forward.w1.weight', 'local_encoder.layers.0.feed_forward.w2.weight', 'local_encoder.layers.0.feed_forward.w3.weight', 'local_encoder.layers.0.ffn_norm.weight', 'local_encoder.patch_embedding_projection.weight', 'local_encoder.tok_embeddings.weight']\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ... (other parts of the code remain the same)\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "\n",
        "        # Similar to the keys found in the diagnosis\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wo = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Apply linear transformations, and convert to float32 before view and transpose\n",
        "        q = self.wq(x).type(torch.float32).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # Convert to float32\n",
        "        k = self.wk(x).type(torch.float32).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # Convert to float32\n",
        "        v = self.wv(x).type(torch.float32).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # Convert to float32\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape and apply final projection\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.wo(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# ... (rest of the code remains the same)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Lrot2nwe_iYM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,  # حجم المفردات من التشخيص\n",
        "    'd_model': 1024,    # أبعاد النموذج من التشخيص\n",
        "    'n_layers': 25,     # عدد الطبقات من التشخيص\n",
        "    'n_heads': 16,      # عدد رؤوس الانتباه (تقدير: عادة ما تكون d_model/64)\n",
        "    'max_seq_len': 2048 # الحد الأقصى لطول التسلسل (تقدير)\n",
        "}\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "\n",
        "        # Similar to the keys found in the diagnosis\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wo = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Apply linear transformations, and convert to float32 before view and transpose\n",
        "        q = self.wq(x).type(torch.float32).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # Convert to float32\n",
        "        k = self.wk(x).type(torch.float32).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # Convert to float32\n",
        "        v = self.wv(x).type(torch.float32).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # Convert to float32\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape and apply final projection\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.wo(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "class BLTFFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(config['d_model'], 4 * config['d_model'], bias=False)\n",
        "        self.w2 = nn.Linear(4 * config['d_model'], config['d_model'], bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.gelu(self.w1(x)))\n",
        "\n",
        "class BLTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BLTAttention(config)\n",
        "        self.attention_norm = nn.LayerNorm(config['d_model'])\n",
        "        self.ffn = BLTFFN(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # طبقة الانتباه مع اتصال متبقي\n",
        "        attn_output = self.attention(self.attention_norm(x), mask)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # طبقة FFN مع اتصال متبقي\n",
        "        ffn_output = self.ffn(self.ffn_norm(x))\n",
        "        x = x + ffn_output\n",
        "\n",
        "        return x\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # طبقة التضمين\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "\n",
        "        # طبقات المحول\n",
        "        self.layers = nn.ModuleList([\n",
        "            BLTBlock(config) for _ in range(config['n_layers'])\n",
        "        ])\n",
        "\n",
        "        # طبقة التطبيع النهائية\n",
        "        self.norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "        # رأس التوقع\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "    def _init_weights_from_model(self, weights):\n",
        "        print(\"تهيئة أوزان النموذج من الملف المحمل...\")\n",
        "\n",
        "        # قائمة المفاتيح التي تم تحميلها بنجاح\n",
        "        loaded_keys = []\n",
        "\n",
        "        # تحميل التضمينات\n",
        "        if 'local_encoder.tok_embeddings.weight' in weights:\n",
        "            self.tok_embeddings.weight.data = weights['local_encoder.tok_embeddings.weight']\n",
        "            loaded_keys.append('local_encoder.tok_embeddings.weight')\n",
        "\n",
        "        # تحميل طبقات المحول\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            prefix = f'layers.{i}.'\n",
        "\n",
        "            # تحميل أوزان الانتباه\n",
        "            for name, param in [\n",
        "                ('attention.wq.weight', layer.attention.wq.weight),\n",
        "                ('attention.wk.weight', layer.attention.wk.weight),\n",
        "                ('attention.wv.weight', layer.attention.wv.weight),\n",
        "                ('attention.wo.weight', layer.attention.wo.weight),\n",
        "                ('attention_norm.weight', layer.attention_norm.weight),\n",
        "                ('ffn_norm.weight', layer.ffn_norm.weight)\n",
        "            ]:\n",
        "                key = prefix + name\n",
        "                if key in weights:\n",
        "                    param.data = weights[key]\n",
        "                    loaded_keys.append(key)\n",
        "\n",
        "            # تحميل أوزان FFN\n",
        "            # قد تختلف الأسماء حسب النموذج الفعلي\n",
        "            ffn_keys = [k for k in weights.keys() if prefix in k and ('feed_forward' in k or 'ffn' in k)]\n",
        "            if len(ffn_keys) >= 2:  # نفترض وجود وزنين على الأقل لطبقة FFN\n",
        "                for key in ffn_keys:\n",
        "                    if 'w1' in key or 'fc1' in key:\n",
        "                        layer.ffn.w1.weight.data = weights[key]\n",
        "                        loaded_keys.append(key)\n",
        "                    elif 'w2' in key or 'fc2' in key:\n",
        "                        layer.ffn.w2.weight.data = weights[key]\n",
        "                        loaded_keys.append(key)\n",
        "\n",
        "        # تحميل طبقة التطبيع النهائية\n",
        "        norm_keys = [k for k in weights.keys() if 'norm' in k and not any(x in k for x in ['attention', 'ffn'])]\n",
        "        if norm_keys:\n",
        "            self.norm.weight.data = weights[norm_keys[0]]\n",
        "            loaded_keys.append(norm_keys[0])\n",
        "\n",
        "        # تحميل رأس الإخراج\n",
        "        output_keys = [k for k in weights.keys() if 'lm_head' in k or 'output' in k]\n",
        "        if output_keys:\n",
        "            self.output.weight.data = weights[output_keys[0]]\n",
        "            loaded_keys.append(output_keys[0])\n",
        "\n",
        "        print(f\"تم تحميل {len(loaded_keys)} من أصل {len(weights)} مفتاح من النموذج\")\n",
        "        return loaded_keys\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        # الحصول على التضمينات\n",
        "        h = self.tok_embeddings(input_ids)\n",
        "\n",
        "        # إنشاء قناع الانتباه إذا لم يتم توفيره\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones((batch_size, seq_len, seq_len), device=input_ids.device)\n",
        "\n",
        "        # تطبيق طبقات المحول\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, attention_mask)\n",
        "\n",
        "        # تطبيق التطبيع النهائي\n",
        "        h = self.norm(h)\n",
        "\n",
        "        # الحصول على التوقعات\n",
        "        logits = self.output(h)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"توليد النص باستخدام النموذج\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # الحصول على نتائج النموذج للتسلسل الحالي\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # تطبيق تصفية top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # تطبيق تصفية top-p (nucleus)\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # إزالة الرموز ذات الاحتمال التراكمي فوق العتبة\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # نقل المؤشرات إلى اليمين للحفاظ أيضًا على الرمز الأول فوق العتبة\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # اختيار الرمز التالي\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # إضافة الرمز التالي إلى التسلسل\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # التحقق مما إذا كنا قد أنشأنا رمز EOS\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "def load_tokenizer(vocab_size=260):\n",
        "    \"\"\"استبدال مؤقت للمحول - تنفيذ بسيط\"\"\"\n",
        "    class SimpleTokenizer:\n",
        "        def __init__(self, vocab_size):\n",
        "            self.vocab_size = vocab_size\n",
        "\n",
        "        def __call__(self, text, return_tensors=\"pt\"):\n",
        "            # هذا مجرد محاكاة، في الواقع تحتاج إلى تنفيذ التحويل الفعلي\n",
        "            # هنا نقوم بتحويل كل حرف إلى رقم بسيط\n",
        "            tokens = [ord(c) % (self.vocab_size - 4) + 4 for c in text]\n",
        "            if return_tensors == \"pt\":\n",
        "                return SimpleNamespace(input_ids=torch.tensor([tokens]))\n",
        "            return tokens\n",
        "\n",
        "        def decode(self, ids):\n",
        "            # تحويل الرموز مرة أخرى إلى نص\n",
        "            if isinstance(ids, torch.Tensor):\n",
        "                ids = ids.tolist()\n",
        "            text = \"\"\n",
        "            for id in ids:\n",
        "                if id >= 4 and id < self.vocab_size:\n",
        "                    text += chr((id - 4) % 26 + 97)  # تحويل بسيط إلى أحرف\n",
        "            return text\n",
        "\n",
        "    return SimpleTokenizer(vocab_size)\n",
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "# دالة لتنفيذ النموذج واختباره\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    tokenizer = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0])\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# تشغيل النموذج\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "rFFBMvW-Ara7",
        "outputId": "3ff87c26-1930-44f2-e320-d91cc1aa028f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n",
            "تهيئة أوزان النموذج من الملف المحمل...\n",
            "تم تحميل 73 من أصل 386 مفتاح من النموذج\n",
            "تجهيز واختبار المحول...\n",
            "اختبار النموذج مع مثال بسيط...\n",
            "المدخل: 'hi'\n",
            "رموز المدخل: [[108, 109]]\n",
            "بدء التوليد...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expected m1 and m2 to have the same dtype, but got: c10::BFloat16 != float",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f0c45f7586b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;31m# تشغيل النموذج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-f0c45f7586b9>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"بدء التوليد...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         output_ids = model.generate(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f0c45f7586b9>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;31m# الحصول على نتائج النموذج للتسلسل الحالي\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f0c45f7586b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# تطبيق طبقات المحول\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# تطبيق التطبيع النهائي\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f0c45f7586b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# طبقة الانتباه مع اتصال متبقي\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f0c45f7586b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Apply linear transformations, and convert to float32 before view and transpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to float32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to float32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to float32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: c10::BFloat16 != float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,  # حجم المفردات من التشخيص\n",
        "    'd_model': 1024,    # أبعاد النموذج من التشخيص\n",
        "    'n_layers': 25,     # عدد الطبقات من التشخيص\n",
        "    'n_heads': 16,      # عدد رؤوس الانتباه (تقدير: عادة ما تكون d_model/64)\n",
        "    'max_seq_len': 2048 # الحد الأقصى لطول التسلسل (تقدير)\n",
        "}\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # ... (other parts remain the same)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Convert input to bfloat16\n",
        "        x = x.type(torch.bfloat16)\n",
        "\n",
        "        # Apply linear transformations\n",
        "        q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.wk(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.wv(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape and apply final projection\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.wo(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "class BLTFFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(config['d_model'], 4 * config['d_model'], bias=False)\n",
        "        self.w2 = nn.Linear(4 * config['d_model'], config['d_model'], bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.gelu(self.w1(x)))\n",
        "\n",
        "class BLTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BLTAttention(config)\n",
        "        self.attention_norm = nn.LayerNorm(config['d_model'])\n",
        "        self.ffn = BLTFFN(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # طبقة الانتباه مع اتصال متبقي\n",
        "        attn_output = self.attention(self.attention_norm(x), mask)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # طبقة FFN مع اتصال متبقي\n",
        "        ffn_output = self.ffn(self.ffn_norm(x))\n",
        "        x = x + ffn_output\n",
        "\n",
        "        return x\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # طبقة التضمين\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "\n",
        "        # طبقات المحول\n",
        "        self.layers = nn.ModuleList([\n",
        "            BLTBlock(config) for _ in range(config['n_layers'])\n",
        "        ])\n",
        "\n",
        "        # طبقة التطبيع النهائية\n",
        "        self.norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "        # رأس التوقع\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "    def _init_weights_from_model(self, weights):\n",
        "        print(\"تهيئة أوزان النموذج من الملف المحمل...\")\n",
        "\n",
        "        # قائمة المفاتيح التي تم تحميلها بنجاح\n",
        "        loaded_keys = []\n",
        "\n",
        "        # تحميل التضمينات\n",
        "        if 'local_encoder.tok_embeddings.weight' in weights:\n",
        "            self.tok_embeddings.weight.data = weights['local_encoder.tok_embeddings.weight']\n",
        "            loaded_keys.append('local_encoder.tok_embeddings.weight')\n",
        "\n",
        "        # تحميل طبقات المحول\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            prefix = f'layers.{i}.'\n",
        "\n",
        "            # تحميل أوزان الانتباه\n",
        "            for name, param in [\n",
        "                ('attention.wq.weight', layer.attention.wq.weight),\n",
        "                ('attention.wk.weight', layer.attention.wk.weight),\n",
        "                ('attention.wv.weight', layer.attention.wv.weight),\n",
        "                ('attention.wo.weight', layer.attention.wo.weight),\n",
        "                ('attention_norm.weight', layer.attention_norm.weight),\n",
        "                ('ffn_norm.weight', layer.ffn_norm.weight)\n",
        "            ]:\n",
        "                key = prefix + name\n",
        "                if key in weights:\n",
        "                    param.data = weights[key]\n",
        "                    loaded_keys.append(key)\n",
        "\n",
        "            # تحميل أوزان FFN\n",
        "            # قد تختلف الأسماء حسب النموذج الفعلي\n",
        "            ffn_keys = [k for k in weights.keys() if prefix in k and ('feed_forward' in k or 'ffn' in k)]\n",
        "            if len(ffn_keys) >= 2:  # نفترض وجود وزنين على الأقل لطبقة FFN\n",
        "                for key in ffn_keys:\n",
        "                    if 'w1' in key or 'fc1' in key:\n",
        "                        layer.ffn.w1.weight.data = weights[key]\n",
        "                        loaded_keys.append(key)\n",
        "                    elif 'w2' in key or 'fc2' in key:\n",
        "                        layer.ffn.w2.weight.data = weights[key]\n",
        "                        loaded_keys.append(key)\n",
        "\n",
        "        # تحميل طبقة التطبيع النهائية\n",
        "        norm_keys = [k for k in weights.keys() if 'norm' in k and not any(x in k for x in ['attention', 'ffn'])]\n",
        "        if norm_keys:\n",
        "            self.norm.weight.data = weights[norm_keys[0]]\n",
        "            loaded_keys.append(norm_keys[0])\n",
        "\n",
        "        # تحميل رأس الإخراج\n",
        "        output_keys = [k for k in weights.keys() if 'lm_head' in k or 'output' in k]\n",
        "        if output_keys:\n",
        "            self.output.weight.data = weights[output_keys[0]]\n",
        "            loaded_keys.append(output_keys[0])\n",
        "\n",
        "        print(f\"تم تحميل {len(loaded_keys)} من أصل {len(weights)} مفتاح من النموذج\")\n",
        "        return loaded_keys\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        # الحصول على التضمينات\n",
        "        h = self.tok_embeddings(input_ids)\n",
        "\n",
        "        # إنشاء قناع الانتباه إذا لم يتم توفيره\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones((batch_size, seq_len, seq_len), device=input_ids.device)\n",
        "\n",
        "        # تطبيق طبقات المحول\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, attention_mask)\n",
        "\n",
        "        # تطبيق التطبيع النهائي\n",
        "        h = self.norm(h)\n",
        "\n",
        "        # الحصول على التوقعات\n",
        "        logits = self.output(h)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"توليد النص باستخدام النموذج\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # الحصول على نتائج النموذج للتسلسل الحالي\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # تطبيق تصفية top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # تطبيق تصفية top-p (nucleus)\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # إزالة الرموز ذات الاحتمال التراكمي فوق العتبة\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # نقل المؤشرات إلى اليمين للحفاظ أيضًا على الرمز الأول فوق العتبة\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # اختيار الرمز التالي\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # إضافة الرمز التالي إلى التسلسل\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # التحقق مما إذا كنا قد أنشأنا رمز EOS\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "def load_tokenizer(vocab_size=260):\n",
        "    \"\"\"استبدال مؤقت للمحول - تنفيذ بسيط\"\"\"\n",
        "    class SimpleTokenizer:\n",
        "        def __init__(self, vocab_size):\n",
        "            self.vocab_size = vocab_size\n",
        "\n",
        "        def __call__(self, text, return_tensors=\"pt\"):\n",
        "            # هذا مجرد محاكاة، في الواقع تحتاج إلى تنفيذ التحويل الفعلي\n",
        "            # هنا نقوم بتحويل كل حرف إلى رقم بسيط\n",
        "            tokens = [ord(c) % (self.vocab_size - 4) + 4 for c in text]\n",
        "            if return_tensors == \"pt\":\n",
        "                return SimpleNamespace(input_ids=torch.tensor([tokens]))\n",
        "            return tokens\n",
        "\n",
        "        def decode(self, ids):\n",
        "            # تحويل الرموز مرة أخرى إلى نص\n",
        "            if isinstance(ids, torch.Tensor):\n",
        "                ids = ids.tolist()\n",
        "            text = \"\"\n",
        "            for id in ids:\n",
        "                if id >= 4 and id < self.vocab_size:\n",
        "                    text += chr((id - 4) % 26 + 97)  # تحويل بسيط إلى أحرف\n",
        "            return text\n",
        "\n",
        "    return SimpleTokenizer(vocab_size)\n",
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "# دالة لتنفيذ النموذج واختباره\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    tokenizer = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0])\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# تشغيل النموذج\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Ex-korjV_VJR",
        "outputId": "5c775d8a-9b3a-407a-febf-12722c9f6549"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n",
            "تهيئة أوزان النموذج من الملف المحمل...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'BLTAttention' object has no attribute 'wq'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-99624a8c11d4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;31m# تشغيل النموذج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-99624a8c11d4>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"تهيئة النموذج...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mloaded_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"تجهيز واختبار المحول...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-99624a8c11d4>\u001b[0m in \u001b[0;36m_init_weights_from_model\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# تحميل أوزان الانتباه\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             for name, param in [\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0;34m'attention.wq.weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;34m'attention.wk.weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;34m'attention.wv.weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BLTAttention' object has no attribute 'wq'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w0jlaWivBZVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YcDco_D7BaL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7i20r6Z1BaGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "prompt = \"Explain the theory of special relativity in a simplified way.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "cccZGf2SBaDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zabPbKUZBjhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IciIgcD-BjeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "\n",
        "prompt = \"Explain the theory of special relativity in a simplified way.\"\n",
        "inputs = entropy_weights(prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model_weights.generate(**inputs, max_new_tokens=100)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "HjUOKTO8BjbD",
        "outputId": "e8dfa1cd-a184-4021-b359-7ab72fe2512b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'dict' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c14c20226261>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Explain the theory of special relativity in a simplified way.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zuP7KPdfB-QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ... (other parts of the code remain the same)\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "\n",
        "        # Initialize the linear layers here\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wo = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # ... (rest of the forward method remains the same)\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    # ... (other parts remain the same)\n",
        "\n",
        "    def _init_weights_from_model(self, weights):\n",
        "        print(\"تهيئة أوزان النموذج من الملف المحمل...\")\n",
        "\n",
        "        # ... (other parts remain the same)\n",
        "\n",
        "        # Load attention weights\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            prefix = f'layers.{i}.'\n",
        "\n",
        "            # Update the keys to match the actual structure of your model_weights\n",
        "            for name, param in [\n",
        "                ('attention.q_proj.weight', layer.attention.wq.weight),  # Updated key\n",
        "                ('attention.k_proj.weight', layer.attention.wk.weight),  # Updated key\n",
        "                ('attention.v_proj.weight', layer.attention.wv.weight),  # Updated key\n",
        "                ('attention.out_proj.weight', layer.attention.wo.weight), # Updated key\n",
        "                ('attention_norm.weight', layer.attention_norm.weight),\n",
        "                ('ffn_norm.weight', layer.ffn_norm.weight)\n",
        "            ]:\n",
        "                key = prefix + name\n",
        "                if key in weights:\n",
        "                    param.data = weights[key].type(torch.float32)  # Convert to float32\n",
        "                    loaded_keys.append(key)\n",
        "\n",
        "        # ... (rest of the _init_weights_from_model method remains the same)\n",
        "\n",
        "# ... (rest of the code remains the same)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "VvWxOOwbCCwl",
        "outputId": "090aea61-a2c7-40f7-b3d2-1ffb7eac938e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 21 (<ipython-input-5-1bd64be459ec>, line 24)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-1bd64be459ec>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    class BLTModel(nn.Module):\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 21\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,  # حجم المفردات من التشخيص\n",
        "    'd_model': 1024,    # أبعاد النموذج من التشخيص\n",
        "    'n_layers': 25,     # عدد الطبقات من التشخيص\n",
        "    'n_heads': 16,      # عدد رؤوس الانتباه (تقدير: عادة ما تكون d_model/64)\n",
        "    'max_seq_len': 2048 # الحد الأقصى لطول التسلسل (تقدير)\n",
        "}\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "\n",
        "        # Initialize the linear layers here\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wo = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Convert input to bfloat16\n",
        "        x = x.type(torch.bfloat16)\n",
        "\n",
        "        # Apply linear transformations\n",
        "        q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.wk(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.wv(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape and apply final projection\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.wo(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "class BLTFFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(config['d_model'], 4 * config['d_model'], bias=False)\n",
        "        self.w2 = nn.Linear(4 * config['d_model'], config['d_model'], bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.gelu(self.w1(x)))\n",
        "\n",
        "class BLTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BLTAttention(config)\n",
        "        self.attention_norm = nn.LayerNorm(config['d_model'])\n",
        "        self.ffn = BLTFFN(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # طبقة الانتباه مع اتصال متبقي\n",
        "        attn_output = self.attention(self.attention_norm(x), mask)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # طبقة FFN مع اتصال متبقي\n",
        "        ffn_output = self.ffn(self.ffn_norm(x))\n",
        "        x = x + ffn_output\n",
        "\n",
        "        return x\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # طبقة التضمين\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "\n",
        "        # طبقات المحول\n",
        "        self.layers = nn.ModuleList([\n",
        "            BLTBlock(config) for _ in range(config['n_layers'])\n",
        "        ])\n",
        "\n",
        "        # طبقة التطبيع النهائية\n",
        "        self.norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "        # رأس التوقع\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "    def _init_weights_from_model(self, weights):\n",
        "        print(\"تهيئة أوزان النموذج من الملف المحمل...\")\n",
        "\n",
        "        # قائمة المفاتيح التي تم تحميلها بنجاح\n",
        "        loaded_keys = []\n",
        "\n",
        "        # تحميل التضمينات\n",
        "        if 'local_encoder.tok_embeddings.weight' in weights:\n",
        "            self.tok_embeddings.weight.data = weights['local_encoder.tok_embeddings.weight'].type(torch.float32)  # Convert to float32\n",
        "            loaded_keys.append('local_encoder.tok_embeddings.weight')\n",
        "\n",
        "        # تحميل طبقات المحول\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            prefix = f'layers.{i}.'\n",
        "\n",
        "            # تحميل أوزان الانتباه\n",
        "            for name, param in [\n",
        "                ('attention.q_proj.weight', layer.attention.wq.weight),  # Updated key\n",
        "                ('attention.k_proj.weight', layer.attention.wk.weight),  # Updated key\n",
        "                ('attention.v_proj.weight', layer.attention.wv.weight),  # Updated key\n",
        "                ('attention.out_proj.weight', layer.attention.wo.weight), # Updated key\n",
        "                ('attention_norm.weight', layer.attention_norm.weight),\n",
        "                ('ffn_norm.weight', layer.ffn_norm.weight)\n",
        "            ]:\n",
        "                key = prefix + name\n",
        "                if key in weights:\n",
        "                    param.data = weights[key].type(torch.float32)  # Convert to float32\n",
        "                    loaded_keys.append(key)\n",
        "\n",
        "            # تحميل أوزان FFN\n",
        "            # قد تختلف الأسماء حسب النموذج الفعلي\n",
        "            ffn_keys = [k for k in weights.keys() if prefix in k and ('feed_forward' in k or 'ffn' in k)]\n",
        "            if len(ffn_keys) >= 2:  # نفترض وجود وزنين على الأقل لطبقة FFN\n",
        "                for key in ffn_keys:\n",
        "                    if 'w1' in key or 'fc1' in key:\n",
        "                        layer.ffn.w1.weight.data = weights[key].type(torch.float32)  # Convert to float32\n",
        "                        loaded_keys.append(key)\n",
        "                    elif 'w2' in key or 'fc2' in key:\n",
        "                        layer.ffn.w2.weight.data = weights[key].type(torch.float32)  # Convert to float32\n",
        "                        loaded_keys.append(key)\n",
        "\n",
        "        # تحميل طبقة التطبيع النهائية\n",
        "        norm_keys = [k for k in weights.keys() if 'norm' in k and not any(x in k for x in ['attention', 'ffn'])]\n",
        "        if norm_keys:\n",
        "            self.norm.weight.data = weights[norm_keys[0]].type(torch.float32)  # Convert to float32\n",
        "            loaded_keys.append(norm_keys[0])\n",
        "\n",
        "        # تحميل رأس الإخراج\n",
        "        output_keys = [k for k in weights.keys() if 'lm_head' in k or 'output' in k]\n",
        "        if output_keys:\n",
        "            self.output.weight.data = weights[output_keys[0]].type(torch.float32)  # Convert to float32\n",
        "            loaded_keys.append(output_keys[0])\n",
        "\n",
        "        print(f\"تم تحميل {len(loaded_keys)} من أصل {len(weights)} مفتاح من النموذج\")\n",
        "        return loaded_keys\n",
        "\n",
        "    # ... (rest of the BLTModel class remains the same)\n",
        "\n",
        "# ... (rest of the code remains the same)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WaKlj5sHCciN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock, BLTModel classes as in the previous response) ...\n",
        "\n",
        "def load_tokenizer(vocab_size=260):\n",
        "    \"\"\"استبدال مؤقت للمحول - تنفيذ بسيط\"\"\"\n",
        "    class SimpleTokenizer:\n",
        "        def __init__(self, vocab_size):\n",
        "            self.vocab_size = vocab_size\n",
        "\n",
        "        def __call__(self, text, return_tensors=\"pt\"):\n",
        "            # هذا مجرد محاكاة، في الواقع تحتاج إلى تنفيذ التحويل الفعلي\n",
        "            # هنا نقوم بتحويل كل حرف إلى رقم بسيط\n",
        "            tokens = [ord(c) % (self.vocab_size - 4) + 4 for c in text]\n",
        "            if return_tensors == \"pt\":\n",
        "                return SimpleNamespace(input_ids=torch.tensor([tokens]))\n",
        "            return tokens\n",
        "\n",
        "        def decode(self, ids):\n",
        "            # تحويل الرموز مرة أخرى إلى نص\n",
        "            if isinstance(ids, torch.Tensor):\n",
        "                ids = ids.tolist()\n",
        "            text = \"\"\n",
        "            for id in ids:\n",
        "                if id >= 4 and id < self.vocab_size:\n",
        "                    text += chr((id - 4) % 26 + 97)  # تحويل بسيط إلى أحرف\n",
        "            return text\n",
        "\n",
        "    return SimpleTokenizer(vocab_size)\n",
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    tokenizer = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0])\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# تشغيل النموذج\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "ZZJg8N2ECoMb",
        "outputId": "2d718300-b2ba-4c7f-e284-15d5735a2c11"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n",
            "تهيئة أوزان النموذج من الملف المحمل...\n",
            "تم تحميل 73 من أصل 386 مفتاح من النموذج\n",
            "تجهيز واختبار المحول...\n",
            "اختبار النموذج مع مثال بسيط...\n",
            "المدخل: 'hi'\n",
            "رموز المدخل: [[108, 109]]\n",
            "بدء التوليد...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'BLTModel' object has no attribute 'generate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5351a32dc11c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# تشغيل النموذج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-5351a32dc11c>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"بدء التوليد...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         output_ids = model.generate(\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BLTModel' object has no attribute 'generate'"
          ]
        }
      ]
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock, BLTModel classes) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    # ... (other parts of BLTModel class remain the same) ...\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"توليد النص باستخدام النموذج\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # الحصول على نتائج النموذج للتسلسل الحالي\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # تطبيق تصفية top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # تطبيق تصفية top-p (nucleus)\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # إزالة الرموز ذات الاحتمال التراكمي فوق العتبة\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # نقل المؤشرات إلى اليمين للحفاظ أيضًا على الرمز الأول فوق العتبة\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # اختيار الرمز التالي\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # إضافة الرمز التالي إلى التسلسل\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # التحقق مما إذا كنا قد أنشأنا رمز EOS\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "# ... (rest of the code remains the same)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-ji935mhD1aU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    tokenizer = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0])\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# تشغيل النموذج\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "HVhTgj01ERx1",
        "outputId": "13d1ec88-f6a7-416e-f23d-666dd5c9b487"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BLTModel.__init__() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cbc315080089>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# تشغيل النموذج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-cbc315080089>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"تهيئة النموذج...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mloaded_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;34mf\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0;34m\" given\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BLTModel.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "source": [
        "#!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer  # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,  # حجم المفردات من التشخيص\n",
        "    'd_model': 1024,    # أبعاد النموذج من التشخيص\n",
        "    'n_layers': 25,     # عدد الطبقات من التشخيص\n",
        "    'n_heads': 16,      # عدد رؤوس الانتباه (تقدير: عادة ما تكون d_model/64)\n",
        "    'max_seq_len': 2048, # الحد الأقصى لطول التسلسل (تقدير)\n",
        "    'eos_token_id': 2  # تعريف رمز EOS\n",
        "}\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock classes remain the same) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    # ... (other parts of BLTModel class remain the same) ...\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"توليد النص باستخدام النموذج\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # الحصول على نتائج النموذج للتسلسل الحالي\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # تطبيق تصفية top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # تطبيق تصفية top-p (nucleus)\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # إزالة الرموز ذات الاحتمال التراكمي فوق العتبة\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # نقل المؤشرات إلى اليمين للحفاظ أيضًا على الرمز الأول فوق العتبة\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # اختيار الرمز التالي\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # إضافة الرمز التالي إلى التسلسل\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # التحقق مما إذا كنا قد أنشأنا رمز EOS\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "# ... (load_tokenizer remains the same) ...\n",
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)  # Decode using Llama tokenizer\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# تشغيل النموذج\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "mnAFzNUAE8e0",
        "outputId": "e9b94271-3eba-4dd7-9824-b2eb36c7e91c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BLTModel.__init__() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-182f6111e6e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# تشغيل النموذج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-182f6111e6e6>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"تهيئة النموذج...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mloaded_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;34mf\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0;34m\" given\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BLTModel.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oq76_tE3ESPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer  # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,  # حجم المفردات من التشخيص\n",
        "    'd_model': 1024,    # أبعاد النموذج من التشخيص\n",
        "    'n_layers': 25,     # عدد الطبقات من التشخيص\n",
        "    'n_heads': 16,      # عدد رؤوس الانتباه (تقدير: عادة ما تكون d_model/64)\n",
        "    'max_seq_len': 2048, # الحد الأقصى لطول التسلسل (تقدير)\n",
        "    'eos_token_id': 2  # تعريف رمز EOS\n",
        "}\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock classes remain the same) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    # ... (other parts of BLTModel class remain the same) ...\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"توليد النص باستخدام النموذج\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # الحصول على نتائج النموذج للتسلسل الحالي\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # تطبيق تصفية top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # تطبيق تصفية top-p (nucleus)\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # إزالة الرموز ذات الاحتمال التراكمي فوق العتبة\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # نقل المؤشرات إلى اليمين للحفاظ أيضًا على الرمز الأول فوق العتبة\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # اختيار الرمز التالي\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # إضافة الرمز التالي إلى التسلسل\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # التحقق مما إذا كنا قد أنشأنا رمز EOS\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "# ... (load_tokenizer remains the same) ...\n",
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)  # Decode using Llama tokenizer\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# تشغيل النموذج\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "q4ilIgH6FADt",
        "outputId": "e8601cad-6493-427d-a3ab-9bedcfc073e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BLTModel.__init__() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-182f6111e6e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# تشغيل النموذج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-182f6111e6e6>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"تهيئة النموذج...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mloaded_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;34mf\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0;34m\" given\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BLTModel.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,  # حجم المفردات من التشخيص\n",
        "    'd_model': 1024,    # أبعاد النموذج من التشخيص\n",
        "    'n_layers': 25,     # عدد الطبقات من التشخيص\n",
        "    'n_heads': 16,      # عدد رؤوس الانتباه (تقدير: عادة ما تكون d_model/64)\n",
        "    'max_seq_len': 2048 # الحد الأقصى لطول التسلسل (تقدير)\n",
        "}\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "\n",
        "        # Initialize the linear layers here\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wo = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Convert input to bfloat16\n",
        "        x = x.type(torch.bfloat16)\n",
        "\n",
        "        # Apply linear transformations\n",
        "        q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.wk(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.wv(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape and apply final projection\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.wo(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# ... (BLTFFN, BLTBlock classes remain the same) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    # ... (other parts of BLTModel class remain the same) ...\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"توليد النص باستخدام النموذج\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # الحصول على نتائج النموذج للتسلسل الحالي\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # تطبيق تصفية top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # تطبيق تصفية top-p (nucleus)\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # إزالة الرموز ذات الاحتمال التراكمي فوق العتبة\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # نقل المؤشرات إلى اليمين للحفاظ أيضًا على الرمز الأول فوق العتبة\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # اختيار الرمز التالي\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # إضافة الرمز التالي إلى التسلسل\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # التحقق مما إذا كنا قد أنشأنا رمز EOS\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "# ... (load_tokenizer, SimpleNamespace, run_model functions remain the same) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "e2qepXhdEZrO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AnZfcCCRFPCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wyno2weXFO_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer  # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,  # حجم المفردات من التشخيص\n",
        "    'd_model': 1024,    # أبعاد النموذج من التشخيص\n",
        "    'n_layers': 25,     # عدد الطبقات من التشخيص\n",
        "    'n_heads': 16,      # عدد رؤوس الانتباه (تقدير: عادة ما تكون d_model/64)\n",
        "    'max_seq_len': 2048, # الحد الأقصى لطول التسلسل (تقدير)\n",
        "    'eos_token_id': 2  # تعريف رمز EOS\n",
        "}\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock classes remain the same) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    # ... (other parts of BLTModel class remain the same) ...\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"توليد النص باستخدام النموذج\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # الحصول على نتائج النموذج للتسلسل الحالي\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # تطبيق تصفية top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # تطبيق تصفية top-p (nucleus)\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # إزالة الرموز ذات الاحتمال التراكمي فوق العتبة\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # نقل المؤشرات إلى اليمين للحفاظ أيضًا على الرمز الأول فوق العتبة\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # اختيار الرمز التالي\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # إضافة الرمز التالي إلى التسلسل\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # التحقق مما إذا كنا قد أنشأنا رمز EOS\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "# ... (load_tokenizer remains the same) ...\n",
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)  # Decode using Llama tokenizer\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# تشغيل النموذج\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "metadata": {
        "id": "CPZVdCCGFO8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer  # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,  # حجم المفردات من التشخيص\n",
        "    'd_model': 1024,    # أبعاد النموذج من التشخيص\n",
        "    'n_layers': 25,     # عدد الطبقات من التشخيص\n",
        "    'n_heads': 16,      # عدد رؤوس الانتباه (تقدير: عادة ما تكون d_model/64)\n",
        "    'max_seq_len': 2048, # الحد الأقصى لطول التسلسل (تقدير)\n",
        "    'eos_token_id': 2  # تعريف رمز EOS\n",
        "}\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock classes remain the same) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    # ... (other parts of BLTModel class remain the same) ...\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"توليد النص باستخدام النموذج\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # الحصول على نتائج النموذج للتسلسل الحالي\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # تطبيق تصفية top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # تطبيق تصفية top-p (nucleus)\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # إزالة الرموز ذات الاحتمال التراكمي فوق العتبة\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # نقل المؤشرات إلى اليمين للحفاظ أيضًا على الرمز الأول فوق العتبة\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # اختيار الرمز التالي\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # إضافة الرمز التالي إلى التسلسل\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # التحقق مما إذا كنا قد أنشأنا رمز EOS\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "# ... (load_tokenizer remains the same) ...\n",
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)  # Decode using Llama tokenizer\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# تشغيل النموذج\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "metadata": {
        "id": "Gj7Ym6qUFdC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VWy3PUGjFj80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer  # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "\n",
        "# تحميل النماذج\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,  # حجم المفردات من التشخيص\n",
        "    'd_model': 1024,    # أبعاد النموذج من التشخيص\n",
        "    'n_layers': 25,     # عدد الطبقات من التشخيص\n",
        "    'n_heads': 16,      # عدد رؤوس الانتباه (تقدير: عادة ما تكون d_model/64)\n",
        "    'max_seq_len': 2048, # الحد الأقصى لطول التسلسل (تقدير)\n",
        "    'eos_token_id': 2  # تعريف رمز EOS\n",
        "}\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock classes remain the same) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    # ... (other parts of BLTModel class remain the same) ...\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"توليد النص باستخدام النموذج\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # الحصول على نتائج النموذج للتسلسل الحالي\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # تطبيق تصفية top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # تطبيق تصفية top-p (nucleus)\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # إزالة الرموز ذات الاحتمال التراكمي فوق العتبة\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    # نقل المؤشرات إلى اليمين للحفاظ أيضًا على الرمز الأول فوق العتبة\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "\n",
        "                # اختيار الرمز التالي\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # إضافة الرمز التالي إلى التسلسل\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "                # التحقق مما إذا كنا قد أنشأنا رمز EOS\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "# ... (load_tokenizer remains the same) ...\n",
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, config):\n",
        "         super().__init__()\n",
        "         self.config = config\n",
        "         # ... (rest of the initialization) ...\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)  # Decode using Llama tokenizer\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# تشغيل النموذج\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "5c1f9b4d-fa4a-45c9-c52f-1a09a130896b",
        "id": "qZv1svsLFkKI"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BLTModel.__init__() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3394a7daa0f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# تشغيل النموذج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3394a7daa0f3>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"تهيئة النموذج...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mloaded_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;34mf\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0;34m\" given\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BLTModel.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "source": [
        "#!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ... (MODEL_CONFIG remains the same) ...\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock classes remain the same) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.layers = nn.ModuleList([BLTBlock(config) for _ in range(config['n_layers'])])\n",
        "        self.norm = nn.LayerNorm(config['d_model'])\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "    def _init_weights_from_model(self, weights):\n",
        "        # ... (This function remains the same as before)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # ... (This function remains the same as before)\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        # ... (This function remains the same as before)\n",
        "\n",
        "# ... (load_tokenizer remains the same) ...\n",
        "\n",
        "# ... (SimpleNamespace remains the same) ...\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)  # Pass MODEL_CONFIG to the constructor\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)  # Load weights after initializing the model\n",
        "\n",
        "    # ... (rest of the run_model function remains the same) ...\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "9Va7SUOpF1z0",
        "outputId": "0259a269-2d6f-4475-be24-21c75ed7d9b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 22 (<ipython-input-6-b06435722a37>, line 25)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-b06435722a37>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    def forward(self, input_ids, attention_mask=None):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lx-PLi82GFgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hU77THL-GFds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oy2M8hDlGFZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQLeEjsaGFWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Md3mjBuGGFSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bzzJktanGFOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,\n",
        "    'd_model': 1024,\n",
        "    'n_layers': 25,\n",
        "    'n_heads': 16,\n",
        "    'max_seq_len': 2048,\n",
        "    'eos_token_id': 2\n",
        "}\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wo = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        x = x.type(torch.bfloat16)\n",
        "        q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.wk(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.wv(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.wo(context)\n",
        "        return output\n",
        "\n",
        "class BLTFFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(config['d_model'], 4 * config['d_model'], bias=False)\n",
        "        self.w2 = nn.Linear(4 * config['d_model'], config['d_model'], bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.gelu(self.w1(x)))\n",
        "\n",
        "class BLTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BLTAttention(config)\n",
        "        self.attention_norm = nn.LayerNorm(config['d_model'])\n",
        "        self.ffn = BLTFFN(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attention(self.attention_norm(x), mask)\n",
        "        x = x + attn_output\n",
        "        ffn_output = self.ffn(self.ffn_norm(x))\n",
        "        x = x + ffn_output\n",
        "        return x\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.layers = nn.ModuleList([BLTBlock(config) for _ in range(config['n_layers'])])\n",
        "        self.norm = nn.LayerNorm(config['d_model'])\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "    def _init_weights_from_model(self, weights):\n",
        "        print(\"تهيئة أوزان النموذج من الملف المحمل...\")\n",
        "        loaded_keys = []\n",
        "        if 'local_encoder.tok_embeddings.weight' in weights:\n",
        "            self.tok_embeddings.weight.data = weights['local_encoder.tok_embeddings.weight'].type(torch.float32)\n",
        "            loaded_keys.append('local_encoder.tok_embeddings.weight')\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            prefix = f'layers.{i}.'\n",
        "            for name, param in [\n",
        "                ('attention.q_proj.weight', layer.attention.wq.weight),\n",
        "                ('attention.k_proj.weight', layer.attention.wk.weight),\n",
        "                ('attention.v_proj.weight', layer.attention.wv.weight),\n",
        "                ('attention.out_proj.weight', layer.attention.wo.weight),\n",
        "                ('attention_norm.weight', layer.attention_norm.weight),\n",
        "                ('ffn_norm.weight', layer.ffn_norm.weight)\n",
        "            ]:\n",
        "                key = prefix + name\n",
        "                if key in weights:\n",
        "                    param.data = weights[key].type(torch.float32)\n",
        "                    loaded_keys.append(key)\n",
        "            ffn_keys = [k for k in weights.keys() if prefix in k and ('feed_forward' in k or 'ffn' in k)]\n",
        "            if len(ffn_keys) >= 2:\n",
        "                for key in ffn_keys:\n",
        "                    if 'w1' in key or 'fc1' in key:\n",
        "                        layer.ffn.w1.weight.data = weights[key].type(torch.float32)\n",
        "                        loaded_keys.append(key)\n",
        "                    elif 'w2' in key or 'fc2' in key:\n",
        "                        layer.ffn.w2.weight.data = weights[key].type(torch.float32)\n",
        "                        loaded_keys.append(key)\n",
        "        norm_keys = [k for k in weights.keys() if 'norm' in k and not any(x in k for x in ['attention', 'ffn'])]\n",
        "        if norm_keys:\n",
        "            self.norm.weight.data = weights[norm_keys[0]].type(torch.float32)\n",
        "            loaded_keys.append(norm_keys[0])\n",
        "        output_keys = [k for k in weights.keys() if 'lm_head' in k or 'output' in k]\n",
        "        if output_keys:\n",
        "            self.output.weight.data = weights[output_keys[0]].type(torch.float32)\n",
        "            loaded_keys.append(output_keys[0])\n",
        "        print(f\"تم تحميل {len(loaded_keys)} من أصل {len(weights)} مفتاح من النموذج\")\n",
        "        return loaded_keys\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "        h = self.tok_embeddings(input_ids)\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones((batch_size, seq_len, seq_len), device=input_ids.device)\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, attention_mask)\n",
        "        h = self.norm(h)\n",
        "        logits = self.output(h)\n",
        "        return logits\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        device = next(self.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        batch_size = input_ids.size(0)\n",
        "        for _ in range(max_length):\n",
        "            with torch.no_grad():\n",
        "                outputs = self(input_ids)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "                    for batch_idx in range(batch_size):\n",
        "                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
        "                        next_token_logits[batch_idx, indices_to_remove] = -float('Inf')\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "                if (next_token == self.config.get('eos_token_id', -1)).any():\n",
        "                    break\n",
        "        return input_ids\n",
        "\n",
        "def load_tokenizer(vocab_size=260):\n",
        "    class SimpleTokenizer:\n",
        "        def __init__(self, vocab_size):\n",
        "            self.vocab_size = vocab_size\n",
        "\n",
        "        def __call__(self, text, return_tensors=\"pt\"):\n",
        "            tokens = [ord(c) % (self.vocab_size - 4) + 4 for c in text]\n",
        "            if return_tensors == \"pt\":\n",
        "                return SimpleNamespace(input_ids=torch.tensor([tokens]))\n",
        "            return tokens\n",
        "\n",
        "        def decode(self, ids):\n",
        "            if isinstance(ids, torch.Tensor):\n",
        "                ids = ids.tolist()\n",
        "            text = \"\"\n",
        "            for id in ids:\n",
        "                if id >= 4 and id < self.vocab_size:\n",
        "                    text += chr((id - 4) % 26 + 97)\n",
        "            return text\n",
        "    return SimpleTokenizer(vocab_size)\n",
        "\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\")\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids=input_ids, max_length=20, temperature=0.7)\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "FkcVVCXiGQBd",
        "outputId": "ee409d8e-bc6a-4807-f48c-3bb7b54db021"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n",
            "تهيئة أوزان النموذج من الملف المحمل...\n",
            "تم تحميل 73 من أصل 386 مفتاح من النموذج\n",
            "تجهيز واختبار المحول...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "اختبار النموذج مع مثال بسيط...\n",
            "المدخل: 'hi'\n",
            "رموز المدخل: [[1, 7251]]\n",
            "بدء التوليد...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a37729250665>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-a37729250665>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"بدء التوليد...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"النص المولد: '{generated_text}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a37729250665>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a37729250665>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,\n",
        "    'd_model': 1024,\n",
        "    'n_layers': 25,\n",
        "    'n_heads': 16,\n",
        "    'max_seq_len': 2048,\n",
        "    'eos_token_id': 2\n",
        "}\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock classes remain the same) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    # ... (other parts of BLTModel class remain the same) ...\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # ... (rest of the forward method)\n",
        "\n",
        "        # Clip input_ids to be within the vocabulary range\n",
        "        input_ids = torch.clamp(input_ids, 0, self.config['vocab_size'] - 1)\n",
        "\n",
        "        # ... (rest of the forward method)\n",
        "\n",
        "    # ... (generate method remains the same) ...\n",
        "\n",
        "# ... (load_tokenizer remains the same) ...\n",
        "\n",
        "# ... (SimpleNamespace remains the same) ...\n",
        "\n",
        "def run_model():\n",
        "    # ... (other parts of run_model function remain the same) ...\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "\n",
        "    # Use AutoTokenizer for Llama-2-7b-chat-hf\n",
        "    tokenizer_llama = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
        "    input_ids_llama = tokenizer_llama(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Use your custom tokenizer for BLTModel\n",
        "    tokenizer_blt = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "    input_ids_blt = tokenizer_blt(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل (Llama): {input_ids_llama.tolist()}\")\n",
        "    print(f\"رموز المدخل (BLT): {input_ids_blt.tolist()}\")\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        # Use input_ids_blt for BLTModel generation\n",
        "        output_ids = model.generate(input_ids=input_ids_blt, max_length=20, temperature=0.7)\n",
        "\n",
        "    # Decode using your custom tokenizer\n",
        "    generated_text = tokenizer_blt.decode(output_ids[0])\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    return model, tokenizer_blt  # Return the BLT tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "qfUjSAWdGexV",
        "outputId": "8870179c-1e2c-4c7a-f832-7c34ac8cefed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "اختبار النموذج مع مثال بسيط...\n",
            "المدخل: 'hi'\n",
            "رموز المدخل (Llama): [[1, 7251]]\n",
            "رموز المدخل (BLT): [[108, 109]]\n",
            "بدء التوليد...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8d7bdc0e627b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-8d7bdc0e627b>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Use input_ids_blt for BLTModel generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids_blt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Decode using your custom tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7WwWDkH1GQ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer, logging\n",
        "logging.set_verbosity_error() # Suppress warnings\n",
        "\n",
        "# ... (MODEL_CONFIG, BLTAttention, BLTFFN, BLTBlock, BLTModel classes remain the same) ...\n",
        "\n",
        "# ... (load_tokenizer, SimpleNamespace remain the same) ...\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    loaded_keys = model._init_weights_from_model(model_weights)\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "\n",
        "    # Use your huggingface token here\n",
        "    tokenizer_llama = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\", use_auth_token=\"YOUR_HUGGINGFACE_TOKEN\")\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids_llama = tokenizer_llama(prompt, return_tensors=\"pt\").input_ids\n",
        "    tokenizer_blt = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "    input_ids_blt = tokenizer_blt(prompt, return_tensors=\"pt\").input_ids\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل (Llama): {input_ids_llama.tolist()}\")\n",
        "    print(f\"رموز المدخل (BLT): {input_ids_blt.tolist()}\")\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids=input_ids_blt, max_length=20, temperature=0.7)\n",
        "    generated_text = tokenizer_blt.decode(output_ids[0])\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "    return model, tokenizer_blt\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "T8Kw2gDcG0OV",
        "outputId": "a50b1691-ce06-48e1-c5ba-d99d71041719"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BLTModel.__init__() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-54299f3101b1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-54299f3101b1>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"تهيئة النموذج...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mloaded_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"تجهيز واختبار المحول...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;34mf\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0;34m\" given\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BLTModel.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAj1aEuQG07L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer, logging\n",
        "\n",
        "logging.set_verbosity_error()  # Suppress warnings\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,\n",
        "    'd_model': 1024,\n",
        "    'n_layers': 25,\n",
        "    'n_heads': 16,\n",
        "    'max_seq_len': 2048,\n",
        "    'eos_token_id': 2\n",
        "}\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock classes remain the same) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.layers = nn.ModuleList([BLTBlock(config) for _ in range(config['n_layers'])])\n",
        "        self.norm = nn.LayerNorm(config['d_model'])\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "        # Load the weights directly in the __init__ method\n",
        "        self._init_weights_from_model(load_file('/content/safetensors/blt_1b/consolidated.safetensors'))\n",
        "\n",
        "    def _init_weights_from_model(self, weights):\n",
        "        # ... (This function remains the same as before) ...\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # ... (This function remains the same as before) ...\n",
        "\n",
        "        # Clip input_ids to be within the vocabulary range\n",
        "        input_ids = torch.clamp(input_ids, 0, self.config['vocab_size'] - 1)\n",
        "\n",
        "        # ... (rest of the forward method) ...\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        # ... (This function remains the same as before) ...\n",
        "\n",
        "# ... (load_tokenizer, SimpleNamespace remain the same) ...\n",
        "\n",
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)  # Pass MODEL_CONFIG to the constructor\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    tokenizer_llama = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\", use_auth_token=\"YOUR_HUGGINGFACE_TOKEN\")\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids_llama = tokenizer_llama(prompt, return_tensors=\"pt\").input_ids\n",
        "    tokenizer_blt = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "    input_ids_blt = tokenizer_blt(prompt, return_tensors=\"pt\").input_ids\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل (Llama): {input_ids_llama.tolist()}\")\n",
        "    print(f\"رموز المدخل (BLT): {input_ids_blt.tolist()}\")\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids=input_ids_blt, max_length=20, temperature=0.7)\n",
        "    generated_text = tokenizer_blt.decode(output_ids[0])\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "    return model, tokenizer_blt\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "xpkR8idwHHFe",
        "outputId": "a366f2d5-3261-47fc-c592-7fc59fc23a7b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 35 (<ipython-input-10-4ba19a2117b5>, line 38)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-4ba19a2117b5>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    def forward(self, input_ids, attention_mask=None):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model():\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)  # Pass MODEL_CONFIG to the constructor\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    tokenizer_llama = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\", use_auth_token=\"YOUR_HUGGINGFACE_TOKEN\")\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    input_ids_llama = tokenizer_llama(prompt, return_tensors=\"pt\").input_ids\n",
        "    tokenizer_blt = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "    input_ids_blt = tokenizer_blt(prompt, return_tensors=\"pt\").input_ids\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل (Llama): {input_ids_llama.tolist()}\")\n",
        "    print(f\"رموز المدخل (BLT): {input_ids_blt.tolist()}\")\n",
        "    print(\"بدء التوليد...\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids=input_ids_blt, max_length=20, temperature=0.7)\n",
        "    generated_text = tokenizer_blt.decode(output_ids[0])\n",
        "    print(f\"النص المولد: '{generated_text}'\")\n",
        "    return model, tokenizer_blt\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "fWoPR9K-HHxr",
        "outputId": "06c9e853-69f8-4212-b480-c048fb8a219d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BLTModel.__init__() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-07c6df6906e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-07c6df6906e5>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"تهيئة النموذج...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass MODEL_CONFIG to the constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"تجهيز واختبار المحول...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer_llama\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NousResearch/Llama-2-7b-chat-hf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"YOUR_HUGGINGFACE_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;34mf\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0;34m\" given\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BLTModel.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pmwjY4ZyHLQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#!pip install safetensors transformers sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer, logging\n",
        "\n",
        "logging.set_verbosity_error()  # Suppress warnings\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,\n",
        "    'd_model': 1024,\n",
        "    'n_layers': 25,\n",
        "    'n_heads': 16,\n",
        "    'max_seq_len': 2048,\n",
        "    'eos_token_id': 2\n",
        "}\n",
        "\n",
        "# ... (BLTAttention, BLTFFN, BLTBlock classes remain the same) ...\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.layers = nn.ModuleList([BLTBlock(config) for _ in range(config['n_layers'])])\n",
        "        self.norm = nn.LayerNorm(config['d_model'])\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "        # Load the weights after initializing the model structure\n",
        "        weights_path = '/content/safetensors/blt_1b/consolidated.safetensors'  # Path to your model weights\n",
        "        self.load_weights(weights_path)\n",
        "\n",
        "    def load_weights(self, weights_path):\n",
        "        loaded_weights = load_file(weights_path)\n",
        "        self._init_weights_from_model(loaded_weights)\n",
        "\n",
        "    def _init_weights_from_model(self, weights):\n",
        "        # ... (This function remains the same as before) ...\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # ... (This function remains the same as before) ...\n",
        "\n",
        "        # Clip input_ids to be within the vocabulary range\n",
        "        input_ids = torch.clamp(input_ids, 0, self.config['vocab_size'] - 1)\n",
        "\n",
        "        # ... (rest of the forward method) ...\n",
        "\n",
        "    def generate(self, input_ids, max_length=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        # ... (This function remains the same as before) ...\n",
        "\n",
        "# ... (load_tokenizer, SimpleNamespace remain the same) ...\n",
        "\n",
        "def run_model():\n",
        "    # ... (rest of the run_model function remains the same) ...\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "uZsd0pQTHsCm",
        "outputId": "dafb8151-ae8a-4cfa-a160-dbc1c8107ca0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 40 (<ipython-input-13-df2a2a253948>, line 43)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-df2a2a253948>\"\u001b[0;36m, line \u001b[0;32m43\u001b[0m\n\u001b[0;31m    def forward(self, input_ids, attention_mask=None):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDuweGMmHXYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tf_fcXeRHXVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jR_iVDiPHXSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPGFUAbWHXPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZnmFkZCHXMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ot2ebQL9HXIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install safetensors transformers sentencepiece torch -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# Assuming safetensors file is loaded correctly elsewhere,\n",
        "# for this example, we'll create dummy weights.\n",
        "# from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings if needed (optional)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,\n",
        "    'd_model': 1024,\n",
        "    'n_layers': 25,\n",
        "    'n_heads': 16,\n",
        "    'max_seq_len': 2048,\n",
        "    'eos_token_id': 2 # Example EOS token ID, adjust if needed\n",
        "}\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        if self.d_model % self.n_heads != 0:\n",
        "             raise ValueError(f\"d_model ({self.d_model}) must be divisible by n_heads ({self.n_heads})\")\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wo = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        # Use causal mask for autoregressive generation\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config['max_seq_len'], config['max_seq_len']))\n",
        "                                     .view(1, 1, config['max_seq_len'], config['max_seq_len']))\n",
        "\n",
        "\n",
        "    def forward(self, x): # Removed mask argument, use causal mask internally\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        # x = x.type(torch.bfloat16) # Apply dtype conversion higher up if needed for performance\n",
        "        q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.wk(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.wv(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Use the causal mask\n",
        "        causal_mask = self.mask[:, :, :seq_len, :seq_len] # Select appropriate part of the mask\n",
        "\n",
        "        # Calculate attention scores\n",
        "        # Use bfloat16 for matmul if desired and supported\n",
        "        # with torch.autocast(device_type=x.device.type, dtype=torch.bfloat16):\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        scores = scores.masked_fill(causal_mask == 0, float('-inf')) # Apply causal mask\n",
        "        attn_weights = F.softmax(scores, dim=-1).type_as(x) # Cast back to input type if needed\n",
        "\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.wo(context)\n",
        "        return output\n",
        "\n",
        "class BLTFFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        hidden_dim = 4 * config['d_model'] # Standard practice\n",
        "        self.w1 = nn.Linear(config['d_model'], hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, config['d_model'], bias=False)\n",
        "        # Consider adding activation like GELU or SiLU\n",
        "        self.activation = nn.GELU() # Or nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply activation\n",
        "        return self.w2(self.activation(self.w1(x)))\n",
        "\n",
        "class BLTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BLTAttention(config)\n",
        "        self.attention_norm = nn.LayerNorm(config['d_model'])\n",
        "        self.ffn = BLTFFN(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "    def forward(self, x): # Removed mask argument\n",
        "        # Pre-normalization is common\n",
        "        attn_output = self.attention(self.attention_norm(x))\n",
        "        x = x + attn_output\n",
        "        ffn_output = self.ffn(self.ffn_norm(x))\n",
        "        x = x + ffn_output\n",
        "        return x\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.layers = nn.ModuleList([BLTBlock(config) for _ in range(config['n_layers'])])\n",
        "        self.norm = nn.LayerNorm(config['d_model'])\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "        # Optional: Tie weights\n",
        "        # self.output.weight = self.tok_embeddings.weight\n",
        "\n",
        "        # Initialize weights (example)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def _init_weights_from_safetensors(self, weights):\n",
        "        print(\"تهيئة أوزان النموذج من الملف المحمل...\")\n",
        "        loaded_count = 0\n",
        "        total_count = len(weights)\n",
        "        model_dict = self.state_dict()\n",
        "        processed_keys = set()\n",
        "\n",
        "        # --- Direct Mapping (adjust keys based on actual safetensors file) ---\n",
        "        key_map = {\n",
        "            'local_encoder.tok_embeddings.weight': 'tok_embeddings.weight',\n",
        "            # Add mappings for attention layers (example for layer 0)\n",
        "            'layers.0.attention.q_proj.weight': 'layers.0.attention.wq.weight',\n",
        "            'layers.0.attention.k_proj.weight': 'layers.0.attention.wk.weight',\n",
        "            'layers.0.attention.v_proj.weight': 'layers.0.attention.wv.weight',\n",
        "            'layers.0.attention.out_proj.weight': 'layers.0.attention.wo.weight',\n",
        "            'layers.0.attention_norm.weight': 'layers.0.attention_norm.weight',\n",
        "            'layers.0.attention_norm.bias': 'layers.0.attention_norm.bias', # Add bias if LayerNorm has it\n",
        "            # Add mappings for FFN layers (example for layer 0)\n",
        "            'layers.0.feed_forward.w1.weight': 'layers.0.ffn.w1.weight', # Adjust based on actual key in file\n",
        "            'layers.0.feed_forward.w2.weight': 'layers.0.ffn.w2.weight', # Adjust based on actual key in file\n",
        "            'layers.0.ffn_norm.weight': 'layers.0.ffn_norm.weight',\n",
        "            'layers.0.ffn_norm.bias': 'layers.0.ffn_norm.bias', # Add bias if LayerNorm has it\n",
        "            # Add mappings for final norm and output\n",
        "            'norm.weight': 'norm.weight', # Adjust based on actual key\n",
        "            'norm.bias': 'norm.bias',     # Add bias if LayerNorm has it\n",
        "            'lm_head.weight': 'output.weight' # Adjust based on actual key\n",
        "        }\n",
        "\n",
        "        # --- Dynamic Mapping for Layers ---\n",
        "        for i in range(self.config['n_layers']):\n",
        "            key_map.update({\n",
        "                f'layers.{i}.attention.q_proj.weight': f'layers.{i}.attention.wq.weight',\n",
        "                f'layers.{i}.attention.k_proj.weight': f'layers.{i}.attention.wk.weight',\n",
        "                f'layers.{i}.attention.v_proj.weight': f'layers.{i}.attention.wv.weight',\n",
        "                f'layers.{i}.attention.out_proj.weight': f'layers.{i}.attention.wo.weight',\n",
        "                f'layers.{i}.attention_norm.weight': f'layers.{i}.attention_norm.weight',\n",
        "                f'layers.{i}.attention_norm.bias': f'layers.{i}.attention_norm.bias',\n",
        "                # Adjust FFN keys based on the actual names in the safetensors file\n",
        "                f'layers.{i}.feed_forward.w1.weight': f'layers.{i}.ffn.w1.weight',\n",
        "                f'layers.{i}.feed_forward.w3.weight': f'layers.{i}.ffn.w3.weight', # Example if gated FFN (like SwiGLU)\n",
        "                f'layers.{i}.feed_forward.w2.weight': f'layers.{i}.ffn.w2.weight',\n",
        "                f'layers.{i}.ffn_norm.weight': f'layers.{i}.ffn_norm.weight',\n",
        "                f'layers.{i}.ffn_norm.bias': f'layers.{i}.ffn_norm.bias',\n",
        "            })\n",
        "\n",
        "        # --- Load Weights ---\n",
        "        new_state_dict = {}\n",
        "        for safetensors_key, tensor in weights.items():\n",
        "            if safetensors_key in key_map:\n",
        "                model_key = key_map[safetensors_key]\n",
        "                if model_key in model_dict:\n",
        "                    if model_dict[model_key].shape == tensor.shape:\n",
        "                        # new_state_dict[model_key] = tensor.to(model_dict[model_key].dtype).to(model_dict[model_key].device)\n",
        "                        new_state_dict[model_key] = tensor.float() # Load as float32 for now\n",
        "                        loaded_count += 1\n",
        "                        processed_keys.add(safetensors_key)\n",
        "                    else:\n",
        "                        print(f\"  Skipping {safetensors_key} -> {model_key}: Shape mismatch! Expected {model_dict[model_key].shape}, got {tensor.shape}\")\n",
        "                else:\n",
        "                     print(f\"  Skipping {safetensors_key}: Corresponding key {model_key} not found in model state_dict.\")\n",
        "            # else:\n",
        "            #     print(f\"  Skipping {safetensors_key}: No mapping defined.\") # Optional: print unmapped keys\n",
        "\n",
        "        missing_keys, unexpected_keys = self.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "        print(f\"تم تحميل {loaded_count} من أصل {total_count} مفتاح معرف من الملف.\")\n",
        "        if missing_keys:\n",
        "            print(f\"مفاتيح مفقودة في النموذج (لم يتم العثور عليها في الملف أو تم تخطيها): {missing_keys}\")\n",
        "        if unexpected_keys:\n",
        "             print(f\"مفاتيح غير متوقعة (موجودة في الملف ولكن لا تطابق النموذج أو لا يوجد لها تعيين): {unexpected_keys}\") # This shouldn't happen with strict=False and loading mapped keys\n",
        "\n",
        "        # Print some unmapped keys for debugging\n",
        "        unmapped_safetensors = set(weights.keys()) - processed_keys\n",
        "        if unmapped_safetensors:\n",
        "             print(f\"بعض المفاتيح من ملف safetensors لم يتم تعيينها أو استخدامها:\")\n",
        "             for k in list(unmapped_safetensors)[:10]: # Print first 10\n",
        "                 print(f\"  - {k}\")\n",
        "\n",
        "\n",
        "    def forward(self, input_ids): # Removed attention_mask argument\n",
        "        # input_ids: (batch_size, seq_len)\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        # Ensure input_ids are within vocab size\n",
        "        if torch.any(input_ids >= self.config['vocab_size']) or torch.any(input_ids < 0):\n",
        "             raise ValueError(f\"Input IDs contain values outside the valid range [0, {self.config['vocab_size']-1}]\")\n",
        "\n",
        "        h = self.tok_embeddings(input_ids) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Pass through layers\n",
        "        for layer in self.layers:\n",
        "            h = layer(h) # Each layer handles its own causal masking\n",
        "\n",
        "        h = self.norm(h)\n",
        "        logits = self.output(h) # (batch_size, seq_len, vocab_size)\n",
        "        return logits.float() # Ensure output is float32 for stability with loss functions\n",
        "\n",
        "    # --- Generation Method (Simplified Sampling) ---\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids, max_new_tokens=50, temperature=0.8, top_k=40):\n",
        "        self.eval() # Set model to evaluation mode\n",
        "        device = input_ids.device\n",
        "        ids = input_ids\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop input_ids if they exceed max_seq_len\n",
        "            ids_cond = ids if ids.size(1) <= self.config['max_seq_len'] else ids[:, -self.config['max_seq_len']:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = self(ids_cond) # Get logits for the *entire* sequence\n",
        "            logits = logits[:, -1, :] / temperature # Focus on the last token prediction, apply temperature\n",
        "\n",
        "            # Optional: Top-k filtering\n",
        "            if top_k is not None and top_k > 0:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf') # Apply top-k\n",
        "\n",
        "            # Sample\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
        "\n",
        "            # Append sampled token\n",
        "            ids = torch.cat((ids, next_id), dim=1)\n",
        "\n",
        "            # Check for EOS token\n",
        "            if self.config.get('eos_token_id') is not None and (next_id == self.config['eos_token_id']).all():\n",
        "                break\n",
        "\n",
        "        self.train() # Set model back to train mode if needed later\n",
        "        return ids\n",
        "\n",
        "\n",
        "# --- Simple Tokenizer ---\n",
        "# Helper class to mimic Hugging Face tokenizer output structure\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "def load_tokenizer(vocab_size=260):\n",
        "    class SimpleTokenizer:\n",
        "        def __init__(self, vocab_size):\n",
        "            self.vocab_size = vocab_size\n",
        "            # Define special tokens if needed, ensuring they are within vocab_size\n",
        "            self.pad_token_id = 0\n",
        "            self.unk_token_id = 1\n",
        "            self.eos_token_id = 2\n",
        "            self.bos_token_id = 3\n",
        "            self.vocab = {i: chr(((i - 4) % 26) + 97) for i in range(4, vocab_size)} # Map IDs back to chars\n",
        "            self.reverse_vocab = {v: k for k, v in self.vocab.items()} # Map chars to IDs\n",
        "\n",
        "        def __call__(self, text, return_tensors=\"pt\", add_special_tokens=True):\n",
        "            # Simple character-to-ID mapping, offset by 4 for special tokens\n",
        "            # Ensure generated IDs are within the valid range [4, vocab_size - 1]\n",
        "            tokens = []\n",
        "            if add_special_tokens and self.bos_token_id is not None:\n",
        "                 tokens.append(self.bos_token_id) # Optional: Add BOS token\n",
        "\n",
        "            for char in text.lower(): # Example: lowercase\n",
        "                # Map known characters, use UNK for others\n",
        "                token_id = self.reverse_vocab.get(char, self.unk_token_id)\n",
        "                # Ensure the ID is valid before adding\n",
        "                if 4 <= token_id < self.vocab_size:\n",
        "                     tokens.append(token_id)\n",
        "                else:\n",
        "                     tokens.append(self.unk_token_id) # Fallback to UNK\n",
        "\n",
        "            # Optional: Add EOS token if needed by model training/generation\n",
        "            # if add_special_tokens and self.eos_token_id is not None:\n",
        "            #     tokens.append(self.eos_token_id)\n",
        "\n",
        "            if return_tensors == \"pt\":\n",
        "                # Return a structure similar to Hugging Face tokenizers\n",
        "                return SimpleNamespace(input_ids=torch.tensor([tokens], dtype=torch.long))\n",
        "            return tokens\n",
        "\n",
        "        def decode(self, ids, skip_special_tokens=True):\n",
        "            if isinstance(ids, torch.Tensor):\n",
        "                ids = ids.squeeze().tolist() # Remove batch dim if present\n",
        "            if isinstance(ids, int): # Handle single ID case\n",
        "                ids = [ids]\n",
        "\n",
        "            text = \"\"\n",
        "            for token_id in ids:\n",
        "                if skip_special_tokens and token_id in [self.pad_token_id, self.unk_token_id, self.eos_token_id, self.bos_token_id]:\n",
        "                    continue\n",
        "                # Map valid character IDs back to characters\n",
        "                char = self.vocab.get(token_id)\n",
        "                if char:\n",
        "                    text += char\n",
        "                # else:\n",
        "                #     text += '?' # Represent unknown or special tokens if not skipped\n",
        "            return text\n",
        "\n",
        "    return SimpleTokenizer(vocab_size)\n",
        "\n",
        "# --- Dummy Weights for Testing ---\n",
        "# Create dummy weights if no safetensors file is available\n",
        "def create_dummy_weights(model):\n",
        "    print(\"Creating dummy weights for testing (no safetensors file loaded).\")\n",
        "    dummy_weights = {}\n",
        "    state_dict = model.state_dict()\n",
        "    # Create tensors with the correct shape but random/zero values\n",
        "    # This requires knowing the expected keys from the safetensors file\n",
        "    # Example using the key_map defined in _init_weights_from_safetensors\n",
        "    # Reverse the key_map to go from model key -> safetensors key\n",
        "    reverse_key_map = {v: k for k, v in model._init_weights_from_safetensors.__defaults__[0].items()} # Hacky way to get default key_map\n",
        "\n",
        "    # A more robust way is to define the expected safetensors keys directly\n",
        "    expected_safetensors_keys = [\n",
        "        'local_encoder.tok_embeddings.weight',\n",
        "        'norm.weight', 'norm.bias',\n",
        "        'lm_head.weight'\n",
        "    ]\n",
        "    for i in range(model.config['n_layers']):\n",
        "        expected_safetensors_keys.extend([\n",
        "            f'layers.{i}.attention.q_proj.weight',\n",
        "            f'layers.{i}.attention.k_proj.weight',\n",
        "            f'layers.{i}.attention.v_proj.weight',\n",
        "            f'layers.{i}.attention.out_proj.weight',\n",
        "            f'layers.{i}.attention_norm.weight', f'layers.{i}.attention_norm.bias',\n",
        "            f'layers.{i}.feed_forward.w1.weight',\n",
        "            f'layers.{i}.feed_forward.w2.weight',\n",
        "            # f'layers.{i}.feed_forward.w3.weight', # If using SwiGLU etc.\n",
        "            f'layers.{i}.ffn_norm.weight', f'layers.{i}.ffn_norm.bias',\n",
        "        ])\n",
        "\n",
        "    # Find corresponding model keys and shapes\n",
        "    key_map_for_dummy = {\n",
        "        'local_encoder.tok_embeddings.weight': 'tok_embeddings.weight',\n",
        "        'norm.weight': 'norm.weight', 'norm.bias': 'norm.bias',\n",
        "        'lm_head.weight': 'output.weight'\n",
        "    }\n",
        "    for i in range(model.config['n_layers']):\n",
        "        key_map_for_dummy.update({\n",
        "            f'layers.{i}.attention.q_proj.weight': f'layers.{i}.attention.wq.weight',\n",
        "            f'layers.{i}.attention.k_proj.weight': f'layers.{i}.attention.wk.weight',\n",
        "            f'layers.{i}.attention.v_proj.weight': f'layers.{i}.attention.wv.weight',\n",
        "            f'layers.{i}.attention.out_proj.weight': f'layers.{i}.attention.wo.weight',\n",
        "            f'layers.{i}.attention_norm.weight': f'layers.{i}.attention_norm.weight',\n",
        "            f'layers.{i}.attention_norm.bias': f'layers.{i}.attention_norm.bias',\n",
        "            f'layers.{i}.feed_forward.w1.weight': f'layers.{i}.ffn.w1.weight',\n",
        "            f'layers.{i}.feed_forward.w2.weight': f'layers.{i}.ffn.w2.weight',\n",
        "            f'layers.{i}.ffn_norm.weight': f'layers.{i}.ffn_norm.weight',\n",
        "            f'layers.{i}.ffn_norm.bias': f'layers.{i}.ffn_norm.bias',\n",
        "        })\n",
        "\n",
        "    for skey in expected_safetensors_keys:\n",
        "        mkey = key_map_for_dummy.get(skey)\n",
        "        if mkey and mkey in state_dict:\n",
        "            dummy_weights[skey] = torch.zeros_like(state_dict[mkey])\n",
        "        # else:\n",
        "        #     print(f\"Warning: Cannot create dummy weight for {skey}, model key {mkey} not found or mapped.\")\n",
        "\n",
        "    print(f\"Created {len(dummy_weights)} dummy weight tensors.\")\n",
        "    return dummy_weights\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "def run_model(weights_path=None):\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    model.eval() # Set to evaluation mode\n",
        "\n",
        "    # Load weights if path is provided\n",
        "    if weights_path:\n",
        "        try:\n",
        "            # model_weights = load_file(weights_path) # Uncomment if you have the file\n",
        "            # print(f\"تم تحميل ملف الأوزان من: {weights_path}\")\n",
        "            # model._init_weights_from_safetensors(model_weights) # Use the specific loading method\n",
        "\n",
        "            # --- Placeholder for loading ---\n",
        "            print(f\"محاكاة تحميل الأوزان من: {weights_path} (استخدام أوزان وهمية)\")\n",
        "            model_weights = create_dummy_weights(model) # Create dummy weights matching expected keys\n",
        "            model._init_weights_from_safetensors(model_weights)\n",
        "            # --- End Placeholder ---\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"خطأ: لم يتم العثور على ملف الأوزان في {weights_path}. استخدام الأوزان الأولية.\")\n",
        "            model_weights = create_dummy_weights(model) # Create dummy weights if file not found\n",
        "            model._init_weights_from_safetensors(model_weights) # Attempt loading with dummy weights\n",
        "        except Exception as e:\n",
        "            print(f\"خطأ أثناء تحميل الأوزان: {e}. استخدام الأوزان الأولية.\")\n",
        "            model_weights = create_dummy_weights(model) # Create dummy weights on other errors\n",
        "            model._init_weights_from_safetensors(model_weights) # Attempt loading with dummy weights\n",
        "    else:\n",
        "        print(\"لم يتم توفير مسار للأوزان. استخدام الأوزان الأولية العشوائية.\")\n",
        "        # Create dummy weights to simulate the loading process structure if needed\n",
        "        model_weights = create_dummy_weights(model)\n",
        "        model._init_weights_from_safetensors(model_weights) # Attempt loading with dummy weights\n",
        "\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    # *** Use the SimpleTokenizer that matches the model's vocab size ***\n",
        "    tokenizer = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "    print(f\"تم استخدام SimpleTokenizer مع حجم مفردات: {tokenizer.vocab_size}\")\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    # Use the correct tokenizer\n",
        "    tokenized_output = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_output.input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\") # Should be small numbers now\n",
        "\n",
        "    # Check if input IDs are valid before generation\n",
        "    if torch.any(input_ids >= MODEL_CONFIG['vocab_size']) or torch.any(input_ids < 0):\n",
        "         print(f\"خطأ: رموز المدخل {input_ids.tolist()} خارج النطاق [0, {MODEL_CONFIG['vocab_size']-1}]\")\n",
        "         return model, tokenizer # Stop before generation\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    try:\n",
        "        # Move model and inputs to the same device (e.g., CPU or GPU)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        # Generate text\n",
        "        output_ids = model.generate(input_ids=input_ids, max_new_tokens=20, temperature=0.7, top_k=10) # Reduced top_k for small vocab\n",
        "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Decode the input prompt correctly using the same tokenizer\n",
        "        decoded_prompt = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        print(f\"النص المدخل (بعد فك الترميز): '{decoded_prompt}'\") # Show how the simple tokenizer sees the input\n",
        "        print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    except IndexError as e:\n",
        "        print(f\"حدث خطأ IndexError أثناء التوليد: {e}\")\n",
        "        print(\"Check model configuration, tokenizer alignment, and input IDs.\")\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ غير متوقع أثناء التوليد: {e}\")\n",
        "\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Provide the path to your .safetensors file if you have one\n",
        "    # weights_file_path = \"path/to/your/model.safetensors\"\n",
        "    weights_file_path = None # Set to None to use dummy weights for testing structure\n",
        "\n",
        "    run_model(weights_path=weights_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "to5FlMX0HXEx",
        "outputId": "5d4c21e8-06ca-4db7-b8ca-8e54d17d1916"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n",
            "لم يتم توفير مسار للأوزان. استخدام الأوزان الأولية العشوائية.\n",
            "Creating dummy weights for testing (no safetensors file loaded).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ac7d46fe283a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0mweights_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# Set to None to use dummy weights for testing structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-ac7d46fe283a>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(weights_path)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"لم يتم توفير مسار للأوزان. استخدام الأوزان الأولية العشوائية.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# Create dummy weights to simulate the loading process structure if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dummy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights_from_safetensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Attempt loading with dummy weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ac7d46fe283a>\u001b[0m in \u001b[0;36mcreate_dummy_weights\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;31m# Example using the key_map defined in _init_weights_from_safetensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# Reverse the key_map to go from model key -> safetensors key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0mreverse_key_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights_from_safetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__defaults__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# Hacky way to get default key_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# A more robust way is to define the expected safetensors keys directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install safetensors transformers sentencepiece torch -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# Assuming safetensors file is loaded correctly elsewhere,\n",
        "# for this example, we'll create dummy weights.\n",
        "# from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings if needed (optional)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,\n",
        "    'd_model': 1024,\n",
        "    'n_layers': 25,\n",
        "    'n_heads': 16,\n",
        "    'max_seq_len': 2048,\n",
        "    'eos_token_id': 2 # Example EOS token ID, adjust if needed\n",
        "}\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        if self.d_model % self.n_heads != 0:\n",
        "             raise ValueError(f\"d_model ({self.d_model}) must be divisible by n_heads ({self.n_heads})\")\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wo = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        # Use causal mask for autoregressive generation\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config['max_seq_len'], config['max_seq_len']))\n",
        "                                     .view(1, 1, config['max_seq_len'], config['max_seq_len']))\n",
        "\n",
        "\n",
        "    def forward(self, x): # Removed mask argument, use causal mask internally\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        # x = x.type(torch.bfloat16) # Apply dtype conversion higher up if needed for performance\n",
        "        q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.wk(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.wv(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Use the causal mask\n",
        "        causal_mask = self.mask[:, :, :seq_len, :seq_len] # Select appropriate part of the mask\n",
        "\n",
        "        # Calculate attention scores\n",
        "        # Use bfloat16 for matmul if desired and supported\n",
        "        # with torch.autocast(device_type=x.device.type, dtype=torch.bfloat16):\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        scores = scores.masked_fill(causal_mask == 0, float('-inf')) # Apply causal mask\n",
        "        attn_weights = F.softmax(scores, dim=-1).type_as(x) # Cast back to input type if needed\n",
        "\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.wo(context)\n",
        "        return output\n",
        "\n",
        "class BLTFFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        hidden_dim = 4 * config['d_model'] # Standard practice\n",
        "        self.w1 = nn.Linear(config['d_model'], hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, config['d_model'], bias=False)\n",
        "        # Consider adding activation like GELU or SiLU\n",
        "        self.activation = nn.GELU() # Or nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply activation\n",
        "        return self.w2(self.activation(self.w1(x)))\n",
        "\n",
        "class BLTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BLTAttention(config)\n",
        "        self.attention_norm = nn.LayerNorm(config['d_model'])\n",
        "        self.ffn = BLTFFN(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config['d_model'])\n",
        "\n",
        "    def forward(self, x): # Removed mask argument\n",
        "        # Pre-normalization is common\n",
        "        attn_output = self.attention(self.attention_norm(x))\n",
        "        x = x + attn_output\n",
        "        ffn_output = self.ffn(self.ffn_norm(x))\n",
        "        x = x + ffn_output\n",
        "        return x\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.layers = nn.ModuleList([BLTBlock(config) for _ in range(config['n_layers'])])\n",
        "        self.norm = nn.LayerNorm(config['d_model'])\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "        # Optional: Tie weights\n",
        "        # self.output.weight = self.tok_embeddings.weight\n",
        "\n",
        "        # Initialize weights (example)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def _init_weights_from_safetensors(self, weights):\n",
        "        print(\"تهيئة أوزان النموذج من الملف المحمل...\")\n",
        "        loaded_count = 0\n",
        "        total_count = len(weights)\n",
        "        model_dict = self.state_dict()\n",
        "        processed_keys = set()\n",
        "\n",
        "        # --- Direct Mapping (adjust keys based on actual safetensors file) ---\n",
        "        key_map = {\n",
        "            'local_encoder.tok_embeddings.weight': 'tok_embeddings.weight',\n",
        "            # Add mappings for attention layers (example for layer 0)\n",
        "            'layers.0.attention.q_proj.weight': 'layers.0.attention.wq.weight',\n",
        "            'layers.0.attention.k_proj.weight': 'layers.0.attention.wk.weight',\n",
        "            'layers.0.attention.v_proj.weight': 'layers.0.attention.wv.weight',\n",
        "            'layers.0.attention.out_proj.weight': 'layers.0.attention.wo.weight',\n",
        "            'layers.0.attention_norm.weight': 'layers.0.attention_norm.weight',\n",
        "            'layers.0.attention_norm.bias': 'layers.0.attention_norm.bias', # Add bias if LayerNorm has it\n",
        "            # Add mappings for FFN layers (example for layer 0)\n",
        "            'layers.0.feed_forward.w1.weight': 'layers.0.ffn.w1.weight', # Adjust based on actual key in file\n",
        "            'layers.0.feed_forward.w2.weight': 'layers.0.ffn.w2.weight', # Adjust based on actual key in file\n",
        "            'layers.0.ffn_norm.weight': 'layers.0.ffn_norm.weight',\n",
        "            'layers.0.ffn_norm.bias': 'layers.0.ffn_norm.bias', # Add bias if LayerNorm has it\n",
        "            # Add mappings for final norm and output\n",
        "            'norm.weight': 'norm.weight', # Adjust based on actual key\n",
        "            'norm.bias': 'norm.bias',     # Add bias if LayerNorm has it\n",
        "            'lm_head.weight': 'output.weight' # Adjust based on actual key\n",
        "        }\n",
        "\n",
        "        # --- Dynamic Mapping for Layers ---\n",
        "        for i in range(self.config['n_layers']):\n",
        "            key_map.update({\n",
        "                f'layers.{i}.attention.q_proj.weight': f'layers.{i}.attention.wq.weight',\n",
        "                f'layers.{i}.attention.k_proj.weight': f'layers.{i}.attention.wk.weight',\n",
        "                f'layers.{i}.attention.v_proj.weight': f'layers.{i}.attention.wv.weight',\n",
        "                f'layers.{i}.attention.out_proj.weight': f'layers.{i}.attention.wo.weight',\n",
        "                f'layers.{i}.attention_norm.weight': f'layers.{i}.attention_norm.weight',\n",
        "                f'layers.{i}.attention_norm.bias': f'layers.{i}.attention_norm.bias',\n",
        "                # Adjust FFN keys based on the actual names in the safetensors file\n",
        "                f'layers.{i}.feed_forward.w1.weight': f'layers.{i}.ffn.w1.weight',\n",
        "                f'layers.{i}.feed_forward.w3.weight': f'layers.{i}.ffn.w3.weight', # Example if gated FFN (like SwiGLU)\n",
        "                f'layers.{i}.feed_forward.w2.weight': f'layers.{i}.ffn.w2.weight',\n",
        "                f'layers.{i}.ffn_norm.weight': f'layers.{i}.ffn_norm.weight',\n",
        "                f'layers.{i}.ffn_norm.bias': f'layers.{i}.ffn_norm.bias',\n",
        "            })\n",
        "\n",
        "        # --- Load Weights ---\n",
        "        new_state_dict = {}\n",
        "        for safetensors_key, tensor in weights.items():\n",
        "            if safetensors_key in key_map:\n",
        "                model_key = key_map[safetensors_key]\n",
        "                if model_key in model_dict:\n",
        "                    if model_dict[model_key].shape == tensor.shape:\n",
        "                        # new_state_dict[model_key] = tensor.to(model_dict[model_key].dtype).to(model_dict[model_key].device)\n",
        "                        new_state_dict[model_key] = tensor.float() # Load as float32 for now\n",
        "                        loaded_count += 1\n",
        "                        processed_keys.add(safetensors_key)\n",
        "                    else:\n",
        "                        print(f\"  Skipping {safetensors_key} -> {model_key}: Shape mismatch! Expected {model_dict[model_key].shape}, got {tensor.shape}\")\n",
        "                else:\n",
        "                     print(f\"  Skipping {safetensors_key}: Corresponding key {model_key} not found in model state_dict.\")\n",
        "            # else:\n",
        "            #     print(f\"  Skipping {safetensors_key}: No mapping defined.\") # Optional: print unmapped keys\n",
        "\n",
        "        missing_keys, unexpected_keys = self.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "        print(f\"تم تحميل {loaded_count} من أصل {total_count} مفتاح معرف من الملف.\")\n",
        "        if missing_keys:\n",
        "            print(f\"مفاتيح مفقودة في النموذج (لم يتم العثور عليها في الملف أو تم تخطيها): {missing_keys}\")\n",
        "        if unexpected_keys:\n",
        "             print(f\"مفاتيح غير متوقعة (موجودة في الملف ولكن لا تطابق النموذج أو لا يوجد لها تعيين): {unexpected_keys}\") # This shouldn't happen with strict=False and loading mapped keys\n",
        "\n",
        "        # Print some unmapped keys for debugging\n",
        "        unmapped_safetensors = set(weights.keys()) - processed_keys\n",
        "        if unmapped_safetensors:\n",
        "             print(f\"بعض المفاتيح من ملف safetensors لم يتم تعيينها أو استخدامها:\")\n",
        "             for k in list(unmapped_safetensors)[:10]: # Print first 10\n",
        "                 print(f\"  - {k}\")\n",
        "\n",
        "\n",
        "    def forward(self, input_ids): # Removed attention_mask argument\n",
        "        # input_ids: (batch_size, seq_len)\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        # Ensure input_ids are within vocab size\n",
        "        if torch.any(input_ids >= self.config['vocab_size']) or torch.any(input_ids < 0):\n",
        "             raise ValueError(f\"Input IDs contain values outside the valid range [0, {self.config['vocab_size']-1}]\")\n",
        "\n",
        "        h = self.tok_embeddings(input_ids) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Pass through layers\n",
        "        for layer in self.layers:\n",
        "            h = layer(h) # Each layer handles its own causal masking\n",
        "\n",
        "        h = self.norm(h)\n",
        "        logits = self.output(h) # (batch_size, seq_len, vocab_size)\n",
        "        return logits.float() # Ensure output is float32 for stability with loss functions\n",
        "\n",
        "    # --- Generation Method (Simplified Sampling) ---\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids, max_new_tokens=50, temperature=0.8, top_k=40):\n",
        "        self.eval() # Set model to evaluation mode\n",
        "        device = input_ids.device\n",
        "        ids = input_ids\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop input_ids if they exceed max_seq_len\n",
        "            ids_cond = ids if ids.size(1) <= self.config['max_seq_len'] else ids[:, -self.config['max_seq_len']:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = self(ids_cond) # Get logits for the *entire* sequence\n",
        "            logits = logits[:, -1, :] / temperature # Focus on the last token prediction, apply temperature\n",
        "\n",
        "            # Optional: Top-k filtering\n",
        "            if top_k is not None and top_k > 0:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf') # Apply top-k\n",
        "\n",
        "            # Sample\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
        "\n",
        "            # Append sampled token\n",
        "            ids = torch.cat((ids, next_id), dim=1)\n",
        "\n",
        "            # Check for EOS token\n",
        "            if self.config.get('eos_token_id') is not None and (next_id == self.config['eos_token_id']).all():\n",
        "                break\n",
        "\n",
        "        self.train() # Set model back to train mode if needed later\n",
        "        return ids\n",
        "\n",
        "\n",
        "# --- Simple Tokenizer ---\n",
        "# Helper class to mimic Hugging Face tokenizer output structure\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "def load_tokenizer(vocab_size=260):\n",
        "    class SimpleTokenizer:\n",
        "        def __init__(self, vocab_size):\n",
        "            self.vocab_size = vocab_size\n",
        "            # Define special tokens if needed, ensuring they are within vocab_size\n",
        "            self.pad_token_id = 0\n",
        "            self.unk_token_id = 1\n",
        "            self.eos_token_id = 2\n",
        "            self.bos_token_id = 3\n",
        "            self.vocab = {i: chr(((i - 4) % 26) + 97) for i in range(4, vocab_size)} # Map IDs back to chars\n",
        "            self.reverse_vocab = {v: k for k, v in self.vocab.items()} # Map chars to IDs\n",
        "\n",
        "        def __call__(self, text, return_tensors=\"pt\", add_special_tokens=True):\n",
        "            # Simple character-to-ID mapping, offset by 4 for special tokens\n",
        "            # Ensure generated IDs are within the valid range [4, vocab_size - 1]\n",
        "            tokens = []\n",
        "            if add_special_tokens and self.bos_token_id is not None:\n",
        "                 tokens.append(self.bos_token_id) # Optional: Add BOS token\n",
        "\n",
        "            for char in text.lower(): # Example: lowercase\n",
        "                # Map known characters, use UNK for others\n",
        "                token_id = self.reverse_vocab.get(char, self.unk_token_id)\n",
        "                # Ensure the ID is valid before adding\n",
        "                if 4 <= token_id < self.vocab_size:\n",
        "                     tokens.append(token_id)\n",
        "                else:\n",
        "                     tokens.append(self.unk_token_id) # Fallback to UNK\n",
        "\n",
        "            # Optional: Add EOS token if needed by model training/generation\n",
        "            # if add_special_tokens and self.eos_token_id is not None:\n",
        "            #     tokens.append(self.eos_token_id)\n",
        "\n",
        "            if return_tensors == \"pt\":\n",
        "                # Return a structure similar to Hugging Face tokenizers\n",
        "                return SimpleNamespace(input_ids=torch.tensor([tokens], dtype=torch.long))\n",
        "            return tokens\n",
        "\n",
        "        def decode(self, ids, skip_special_tokens=True):\n",
        "            if isinstance(ids, torch.Tensor):\n",
        "                ids = ids.squeeze().tolist() # Remove batch dim if present\n",
        "            if isinstance(ids, int): # Handle single ID case\n",
        "                ids = [ids]\n",
        "\n",
        "            text = \"\"\n",
        "            for token_id in ids:\n",
        "                if skip_special_tokens and token_id in [self.pad_token_id, self.unk_token_id, self.eos_token_id, self.bos_token_id]:\n",
        "                    continue\n",
        "                # Map valid character IDs back to characters\n",
        "                char = self.vocab.get(token_id)\n",
        "                if char:\n",
        "                    text += char\n",
        "                # else:\n",
        "                #     text += '?' # Represent unknown or special tokens if not skipped\n",
        "            return text\n",
        "\n",
        "    return SimpleTokenizer(vocab_size)\n",
        "\n",
        "# --- Dummy Weights for Testing ---\n",
        "# Create dummy weights if no safetensors file is available\n",
        "def create_dummy_weights(model):\n",
        "    print(\"Creating dummy weights for testing (no safetensors file loaded).\")\n",
        "    dummy_weights = {}\n",
        "    state_dict = model.state_dict()\n",
        "    # Create tensors with the correct shape but random/zero values\n",
        "    # This requires knowing the expected keys from the safetensors file\n",
        "    # Example using the key_map defined in _init_weights_from_safetensors\n",
        "    # Reverse the key_map to go from model key -> safetensors key\n",
        "    reverse_key_map = {v: k for k, v in model._init_weights_from_safetensors.__defaults__[0].items()} # Hacky way to get default key_map\n",
        "\n",
        "    # A more robust way is to define the expected safetensors keys directly\n",
        "    expected_safetensors_keys = [\n",
        "        'local_encoder.tok_embeddings.weight',\n",
        "        'norm.weight', 'norm.bias',\n",
        "        'lm_head.weight'\n",
        "    ]\n",
        "    for i in range(model.config['n_layers']):\n",
        "        expected_safetensors_keys.extend([\n",
        "            f'layers.{i}.attention.q_proj.weight',\n",
        "            f'layers.{i}.attention.k_proj.weight',\n",
        "            f'layers.{i}.attention.v_proj.weight',\n",
        "            f'layers.{i}.attention.out_proj.weight',\n",
        "            f'layers.{i}.attention_norm.weight', f'layers.{i}.attention_norm.bias',\n",
        "            f'layers.{i}.feed_forward.w1.weight',\n",
        "            f'layers.{i}.feed_forward.w2.weight',\n",
        "            # f'layers.{i}.feed_forward.w3.weight', # If using SwiGLU etc.\n",
        "            f'layers.{i}.ffn_norm.weight', f'layers.{i}.ffn_norm.bias',\n",
        "        ])\n",
        "\n",
        "    # Find corresponding model keys and shapes\n",
        "    key_map_for_dummy = {\n",
        "        'local_encoder.tok_embeddings.weight': 'tok_embeddings.weight',\n",
        "        'norm.weight': 'norm.weight', 'norm.bias': 'norm.bias',\n",
        "        'lm_head.weight': 'output.weight'\n",
        "    }\n",
        "    for i in range(model.config['n_layers']):\n",
        "        key_map_for_dummy.update({\n",
        "            f'layers.{i}.attention.q_proj.weight': f'layers.{i}.attention.wq.weight',\n",
        "            f'layers.{i}.attention.k_proj.weight': f'layers.{i}.attention.wk.weight',\n",
        "            f'layers.{i}.attention.v_proj.weight': f'layers.{i}.attention.wv.weight',\n",
        "            f'layers.{i}.attention.out_proj.weight': f'layers.{i}.attention.wo.weight',\n",
        "            f'layers.{i}.attention_norm.weight': f'layers.{i}.attention_norm.weight',\n",
        "            f'layers.{i}.attention_norm.bias': f'layers.{i}.attention_norm.bias',\n",
        "            f'layers.{i}.feed_forward.w1.weight': f'layers.{i}.ffn.w1.weight',\n",
        "            f'layers.{i}.feed_forward.w2.weight': f'layers.{i}.ffn.w2.weight',\n",
        "            f'layers.{i}.ffn_norm.weight': f'layers.{i}.ffn_norm.weight',\n",
        "            f'layers.{i}.ffn_norm.bias': f'layers.{i}.ffn_norm.bias',\n",
        "        })\n",
        "\n",
        "    for skey in expected_safetensors_keys:\n",
        "        mkey = key_map_for_dummy.get(skey)\n",
        "        if mkey and mkey in state_dict:\n",
        "            dummy_weights[skey] = torch.zeros_like(state_dict[mkey])\n",
        "        # else:\n",
        "        #     print(f\"Warning: Cannot create dummy weight for {skey}, model key {mkey} not found or mapped.\")\n",
        "\n",
        "    print(f\"Created {len(dummy_weights)} dummy weight tensors.\")\n",
        "    return dummy_weights\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "def run_model(weights_path=None):\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    model.eval() # Set to evaluation mode\n",
        "\n",
        "    # Load weights if path is provided\n",
        "    if weights_path:\n",
        "        try:\n",
        "            # model_weights = load_file(weights_path) # Uncomment if you have the file\n",
        "            # print(f\"تم تحميل ملف الأوزان من: {weights_path}\")\n",
        "            # model._init_weights_from_safetensors(model_weights) # Use the specific loading method\n",
        "\n",
        "            # --- Placeholder for loading ---\n",
        "            print(f\"محاكاة تحميل الأوزان من: {weights_path} (استخدام أوزان وهمية)\")\n",
        "            model_weights = create_dummy_weights(model) # Create dummy weights matching expected keys\n",
        "            model._init_weights_from_safetensors(model_weights)\n",
        "            # --- End Placeholder ---\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"خطأ: لم يتم العثور على ملف الأوزان في {weights_path}. استخدام الأوزان الأولية.\")\n",
        "            model_weights = create_dummy_weights(model) # Create dummy weights if file not found\n",
        "            model._init_weights_from_safetensors(model_weights) # Attempt loading with dummy weights\n",
        "        except Exception as e:\n",
        "            print(f\"خطأ أثناء تحميل الأوزان: {e}. استخدام الأوزان الأولية.\")\n",
        "            model_weights = create_dummy_weights(model) # Create dummy weights on other errors\n",
        "            model._init_weights_from_safetensors(model_weights) # Attempt loading with dummy weights\n",
        "    else:\n",
        "        print(\"لم يتم توفير مسار للأوزان. استخدام الأوزان الأولية العشوائية.\")\n",
        "        # Create dummy weights to simulate the loading process structure if needed\n",
        "        model_weights = create_dummy_weights(model)\n",
        "        model._init_weights_from_safetensors(model_weights) # Attempt loading with dummy weights\n",
        "\n",
        "\n",
        "    print(\"تجهيز واختبار المحول...\")\n",
        "    # *** Use the SimpleTokenizer that matches the model's vocab size ***\n",
        "    tokenizer = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "    print(f\"تم استخدام SimpleTokenizer مع حجم مفردات: {tokenizer.vocab_size}\")\n",
        "\n",
        "    print(\"اختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    # Use the correct tokenizer\n",
        "    tokenized_output = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_output.input_ids\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    print(f\"رموز المدخل: {input_ids.tolist()}\") # Should be small numbers now\n",
        "\n",
        "    # Check if input IDs are valid before generation\n",
        "    if torch.any(input_ids >= MODEL_CONFIG['vocab_size']) or torch.any(input_ids < 0):\n",
        "         print(f\"خطأ: رموز المدخل {input_ids.tolist()} خارج النطاق [0, {MODEL_CONFIG['vocab_size']-1}]\")\n",
        "         return model, tokenizer # Stop before generation\n",
        "\n",
        "    print(\"بدء التوليد...\")\n",
        "    try:\n",
        "        # Move model and inputs to the same device (e.g., CPU or GPU)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        # Generate text\n",
        "        output_ids = model.generate(input_ids=input_ids, max_new_tokens=20, temperature=0.7, top_k=10) # Reduced top_k for small vocab\n",
        "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Decode the input prompt correctly using the same tokenizer\n",
        "        decoded_prompt = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        print(f\"النص المدخل (بعد فك الترميز): '{decoded_prompt}'\") # Show how the simple tokenizer sees the input\n",
        "        print(f\"النص المولد: '{generated_text}'\")\n",
        "\n",
        "    except IndexError as e:\n",
        "        print(f\"حدث خطأ IndexError أثناء التوليد: {e}\")\n",
        "        print(\"Check model configuration, tokenizer alignment, and input IDs.\")\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ غير متوقع أثناء التوليد: {e}\")\n",
        "\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Provide the path to your .safetensors file if you have one\n",
        "    # weights_file_path = \"path/to/your/model.safetensors\"\n",
        "    weights_file_path = None # Set to None to use dummy weights for testing structure\n",
        "\n",
        "    run_model(weights_path=weights_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "x66P18sbHZt0",
        "outputId": "27c14f35-a260-4bb2-f53a-533fb9e508be"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n",
            "لم يتم توفير مسار للأوزان. استخدام الأوزان الأولية العشوائية.\n",
            "Creating dummy weights for testing (no safetensors file loaded).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ac7d46fe283a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0mweights_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# Set to None to use dummy weights for testing structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-ac7d46fe283a>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(weights_path)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"لم يتم توفير مسار للأوزان. استخدام الأوزان الأولية العشوائية.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# Create dummy weights to simulate the loading process structure if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dummy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights_from_safetensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Attempt loading with dummy weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-ac7d46fe283a>\u001b[0m in \u001b[0;36mcreate_dummy_weights\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;31m# Example using the key_map defined in _init_weights_from_safetensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# Reverse the key_map to go from model key -> safetensors key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0mreverse_key_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights_from_safetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__defaults__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# Hacky way to get default key_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# A more robust way is to define the expected safetensors keys directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install safetensors transformers sentencepiece torch -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from safetensors.torch import load_file # Make sure this is imported\n",
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "import os # Import os for path checking\n",
        "\n",
        "# Suppress specific warnings if needed (optional)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "\n",
        "# تعريف التكوين بناءً على المعلومات المستخرجة\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': 260,\n",
        "    'd_model': 1024,\n",
        "    'n_layers': 25,\n",
        "    'n_heads': 16,\n",
        "    'max_seq_len': 2048,\n",
        "    'eos_token_id': 2 # Example EOS token ID, adjust if needed\n",
        "}\n",
        "\n",
        "class BLTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_model = config['d_model']\n",
        "        self.n_heads = config['n_heads']\n",
        "        if self.d_model % self.n_heads != 0:\n",
        "             raise ValueError(f\"d_model ({self.d_model}) must be divisible by n_heads ({self.n_heads})\")\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wo = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        # Use causal mask for autoregressive generation\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config['max_seq_len'], config['max_seq_len']))\n",
        "                                     .view(1, 1, config['max_seq_len'], config['max_seq_len']))\n",
        "\n",
        "\n",
        "    def forward(self, x): # Removed mask argument, use causal mask internally\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        # x = x.type(torch.bfloat16) # Apply dtype conversion higher up if needed for performance\n",
        "        q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.wk(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.wv(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Use the causal mask\n",
        "        causal_mask = self.mask[:, :, :seq_len, :seq_len].to(x.device) # Select appropriate part of the mask and move to correct device\n",
        "\n",
        "        # Calculate attention scores\n",
        "        # Use bfloat16 for matmul if desired and supported\n",
        "        # with torch.autocast(device_type=x.device.type, dtype=torch.bfloat16):\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        scores = scores.masked_fill(causal_mask == 0, float('-inf')) # Apply causal mask\n",
        "        attn_weights = F.softmax(scores, dim=-1).type_as(x) # Cast back to input type if needed\n",
        "\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.wo(context)\n",
        "        return output\n",
        "\n",
        "class BLTFFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        hidden_dim = 4 * config['d_model'] # Standard practice\n",
        "        self.w1 = nn.Linear(config['d_model'], hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, config['d_model'], bias=False)\n",
        "        # Consider adding activation like GELU or SiLU\n",
        "        self.activation = nn.GELU() # Or nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply activation\n",
        "        return self.w2(self.activation(self.w1(x)))\n",
        "\n",
        "class BLTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BLTAttention(config)\n",
        "        # Use bias=False for LayerNorm if the original model did, otherwise keep True (default)\n",
        "        self.attention_norm = nn.LayerNorm(config['d_model'], elementwise_affine=True)\n",
        "        self.ffn = BLTFFN(config)\n",
        "        self.ffn_norm = nn.LayerNorm(config['d_model'], elementwise_affine=True)\n",
        "\n",
        "    def forward(self, x): # Removed mask argument\n",
        "        # Pre-normalization is common\n",
        "        attn_input = self.attention_norm(x)\n",
        "        attn_output = self.attention(attn_input)\n",
        "        x = x + attn_output\n",
        "\n",
        "        ffn_input = self.ffn_norm(x)\n",
        "        ffn_output = self.ffn(ffn_input)\n",
        "        x = x + ffn_output\n",
        "        return x\n",
        "\n",
        "class BLTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.layers = nn.ModuleList([BLTBlock(config) for _ in range(config['n_layers'])])\n",
        "        # Use bias=False for LayerNorm if the original model did\n",
        "        self.norm = nn.LayerNorm(config['d_model'], elementwise_affine=True)\n",
        "        self.output = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "\n",
        "        # Optional: Tie weights IF the original model did\n",
        "        # self.output.weight = self.tok_embeddings.weight\n",
        "\n",
        "        # Initialize weights (example - will be overwritten by loaded weights)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def _init_weights_from_safetensors(self, weights):\n",
        "        \"\"\"\n",
        "        Loads weights from a dictionary (loaded from safetensors).\n",
        "        *** CRITICAL: The key_map MUST match the names in your specific safetensors file ***\n",
        "        You might need to inspect the keys in your file first.\n",
        "        Example inspection:\n",
        "            loaded_weights = load_file('path/to/your/consolidated.safetensors')\n",
        "            print(list(loaded_weights.keys()))\n",
        "        \"\"\"\n",
        "        print(\"تهيئة أوزان النموذج من الملف المحمل...\")\n",
        "        loaded_count = 0\n",
        "        total_count = len(weights)\n",
        "        model_dict = self.state_dict()\n",
        "        processed_keys = set() # Keep track of keys from the file that were processed\n",
        "\n",
        "        # --- Define the Mapping from Safetensors Key -> Model Key ---\n",
        "        # !!! ADJUST THIS MAP BASED ON THE ACTUAL KEYS IN YOUR FILE !!!\n",
        "        key_map = {\n",
        "            # Embedding - Adjust 'local_encoder.tok_embeddings.weight' if needed\n",
        "            'local_encoder.tok_embeddings.weight': 'tok_embeddings.weight',\n",
        "\n",
        "            # Final LayerNorm - Adjust 'norm.weight'/'norm.bias' if needed\n",
        "            'norm.weight': 'norm.weight',\n",
        "            'norm.bias': 'norm.bias', # Add/remove bias based on your LayerNorm definition and file\n",
        "\n",
        "            # Output Head - Adjust 'lm_head.weight' if needed\n",
        "            'lm_head.weight': 'output.weight'\n",
        "        }\n",
        "\n",
        "        # --- Dynamic Mapping for Layers ---\n",
        "        for i in range(self.config['n_layers']):\n",
        "            layer_prefix_file = f'layers.{i}.' # Assumed prefix in file\n",
        "            layer_prefix_model = f'layers.{i}.' # Prefix in this model definition\n",
        "\n",
        "            key_map.update({\n",
        "                # Attention Weights\n",
        "                layer_prefix_file + 'attention.q_proj.weight': layer_prefix_model + 'attention.wq.weight',\n",
        "                layer_prefix_file + 'attention.k_proj.weight': layer_prefix_model + 'attention.wk.weight',\n",
        "                layer_prefix_file + 'attention.v_proj.weight': layer_prefix_model + 'attention.wv.weight',\n",
        "                layer_prefix_file + 'attention.out_proj.weight': layer_prefix_model + 'attention.wo.weight',\n",
        "\n",
        "                # Attention LayerNorm\n",
        "                layer_prefix_file + 'attention_norm.weight': layer_prefix_model + 'attention_norm.weight',\n",
        "                layer_prefix_file + 'attention_norm.bias': layer_prefix_model + 'attention_norm.bias', # Add/remove bias mapping\n",
        "\n",
        "                # FFN Weights - Adjust 'feed_forward' and 'w1'/'w2' names if needed\n",
        "                layer_prefix_file + 'feed_forward.w1.weight': layer_prefix_model + 'ffn.w1.weight',\n",
        "                # layer_prefix_file + 'feed_forward.w3.weight': layer_prefix_model + 'ffn.w3.weight', # Example if gated FFN (like SwiGLU)\n",
        "                layer_prefix_file + 'feed_forward.w2.weight': layer_prefix_model + 'ffn.w2.weight',\n",
        "\n",
        "                # FFN LayerNorm\n",
        "                layer_prefix_file + 'ffn_norm.weight': layer_prefix_model + 'ffn_norm.weight',\n",
        "                layer_prefix_file + 'ffn_norm.bias': layer_prefix_model + 'ffn_norm.bias', # Add/remove bias mapping\n",
        "            })\n",
        "\n",
        "        # --- Load Weights ---\n",
        "        new_state_dict = {}\n",
        "        skipped_mismatch = []\n",
        "        skipped_missing_model_key = []\n",
        "        skipped_unmapped = []\n",
        "\n",
        "        for safetensors_key, tensor in weights.items():\n",
        "            processed_keys.add(safetensors_key)\n",
        "            if safetensors_key in key_map:\n",
        "                model_key = key_map[safetensors_key]\n",
        "                if model_key in model_dict:\n",
        "                    # Check shape compatibility BEFORE assigning\n",
        "                    if model_dict[model_key].shape == tensor.shape:\n",
        "                        # Load as float32 by default, ensure device matches model\n",
        "                        target_device = model_dict[model_key].device\n",
        "                        new_state_dict[model_key] = tensor.float().to(target_device)\n",
        "                        loaded_count += 1\n",
        "                        # print(f\"  Mapped: {safetensors_key} -> {model_key} (Shape: {tensor.shape})\") # Verbose\n",
        "                    else:\n",
        "                        # print(f\"  Skipping {safetensors_key} -> {model_key}: Shape mismatch! Expected {model_dict[model_key].shape}, got {tensor.shape}\")\n",
        "                        skipped_mismatch.append(f\"{safetensors_key} (Shape: {tensor.shape}) -> {model_key} (Expected: {model_dict[model_key].shape})\")\n",
        "                else:\n",
        "                     # This case means the key_map points to a layer/parameter name that doesn't exist in *this* BLTModel definition\n",
        "                     # print(f\"  Skipping {safetensors_key}: Mapped key '{model_key}' not found in current model state_dict.\")\n",
        "                     skipped_missing_model_key.append(f\"{safetensors_key} -> {model_key}\")\n",
        "            else:\n",
        "                # This case means a key exists in the safetensors file but is not included in the key_map\n",
        "                # print(f\"  Skipping {safetensors_key}: No mapping defined in key_map.\")\n",
        "                skipped_unmapped.append(safetensors_key)\n",
        "\n",
        "        # Load the mapped weights into the model\n",
        "        # `strict=False` allows the model to load even if some weights are missing (e.g., bias terms if LayerNorms don't have them)\n",
        "        # or if `new_state_dict` doesn't contain weights for all parameters in the model (they keep their initial values).\n",
        "        missing_keys, unexpected_keys = self.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "        print(f\"--- Weight Loading Summary ---\")\n",
        "        print(f\"تم تحميل {loaded_count} من أصل {total_count} مفتاح معرف من الملف بنجاح.\")\n",
        "\n",
        "        if skipped_mismatch:\n",
        "            print(f\"\\nSkipped {len(skipped_mismatch)} keys due to SHAPE MISMATCH:\")\n",
        "            for item in skipped_mismatch[:5]: print(f\"  - {item}\") # Print first 5\n",
        "            if len(skipped_mismatch) > 5: print(\"  ...\")\n",
        "\n",
        "        if skipped_missing_model_key:\n",
        "            print(f\"\\nSkipped {len(skipped_missing_model_key)} keys due to MAPPED MODEL KEY NOT FOUND:\")\n",
        "            for item in skipped_missing_model_key[:5]: print(f\"  - {item}\") # Print first 5\n",
        "            if len(skipped_missing_model_key) > 5: print(\"  ...\")\n",
        "\n",
        "        if skipped_unmapped:\n",
        "            print(f\"\\nSkipped {len(skipped_unmapped)} keys due to NO MAPPING DEFINED in key_map:\")\n",
        "            for item in skipped_unmapped[:5]: print(f\"  - {item}\") # Print first 5\n",
        "            if len(skipped_unmapped) > 5: print(\"  ...\")\n",
        "\n",
        "        # Report keys from the model that were NOT loaded from the file\n",
        "        if missing_keys:\n",
        "            print(f\"\\nModel parameters NOT found in the loaded weights (using initial values):\")\n",
        "            for key in missing_keys[:10]: print(f\"  - {key}\") # Print first 10\n",
        "            if len(missing_keys) > 10: print(\"  ...\")\n",
        "\n",
        "        # `unexpected_keys` should be empty with strict=False when loading from a subset dict.\n",
        "        # If it's not empty, it indicates an issue with load_state_dict itself.\n",
        "        if unexpected_keys:\n",
        "             print(f\"\\nUnexpected keys during load_state_dict (internal issue?): {unexpected_keys}\")\n",
        "        print(\"--- End Summary ---\")\n",
        "\n",
        "\n",
        "    def forward(self, input_ids): # Removed attention_mask argument\n",
        "        # input_ids: (batch_size, seq_len)\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        # Ensure input_ids are within vocab size\n",
        "        max_id = torch.max(input_ids)\n",
        "        min_id = torch.min(input_ids)\n",
        "        if max_id >= self.config['vocab_size'] or min_id < 0:\n",
        "             raise ValueError(f\"Input IDs contain values outside the valid range [0, {self.config['vocab_size']-1}]. Found min={min_id}, max={max_id}\")\n",
        "\n",
        "        h = self.tok_embeddings(input_ids) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Pass through layers\n",
        "        for layer in self.layers:\n",
        "            h = layer(h) # Each layer handles its own causal masking\n",
        "\n",
        "        h = self.norm(h)\n",
        "        logits = self.output(h) # (batch_size, seq_len, vocab_size)\n",
        "        return logits.float() # Ensure output is float32 for stability with loss functions\n",
        "\n",
        "    # --- Generation Method (Simplified Sampling) ---\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids, max_new_tokens=50, temperature=0.8, top_k=40):\n",
        "        self.eval() # Set model to evaluation mode\n",
        "        device = input_ids.device # Use the device of the input_ids\n",
        "        ids = input_ids\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop input_ids if they exceed max_seq_len for the forward pass\n",
        "            # Keep only the last max_seq_len tokens relevant for predicting the next one\n",
        "            seq_len = ids.size(1)\n",
        "            ids_cond = ids if seq_len <= self.config['max_seq_len'] else ids[:, -self.config['max_seq_len']:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = self(ids_cond) # Get logits for the (potentially cropped) sequence\n",
        "            # Always focus on the prediction for the *very last* token position\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Optional: Top-k filtering\n",
        "            if top_k is not None and top_k > 0:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf') # Apply top-k\n",
        "\n",
        "            # Sample\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
        "\n",
        "            # Append sampled token\n",
        "            ids = torch.cat((ids, next_id), dim=1)\n",
        "\n",
        "            # Check for EOS token\n",
        "            if self.config.get('eos_token_id') is not None and (next_id == self.config['eos_token_id']).all():\n",
        "                print(f\"\\n[Generation stopped: EOS token {self.config['eos_token_id']} reached]\")\n",
        "                break\n",
        "            # Optional: Check if max length (including prompt) is reached\n",
        "            if ids.size(1) >= self.config['max_seq_len']:\n",
        "                 print(f\"\\n[Generation stopped: Reached max_seq_len {self.config['max_seq_len']}]\")\n",
        "                 break\n",
        "\n",
        "\n",
        "        # self.train() # Only set back to train mode if you intend to continue training\n",
        "        return ids\n",
        "\n",
        "\n",
        "# --- Simple Tokenizer ---\n",
        "# Helper class to mimic Hugging Face tokenizer output structure\n",
        "class SimpleNamespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "def load_tokenizer(vocab_size=260):\n",
        "    class SimpleTokenizer:\n",
        "        def __init__(self, vocab_size):\n",
        "            self.vocab_size = vocab_size\n",
        "            # Define special tokens if needed, ensuring they are within vocab_size\n",
        "            self.pad_token_id = 0\n",
        "            self.unk_token_id = 1\n",
        "            self.eos_token_id = 2 # Make sure this matches MODEL_CONFIG['eos_token_id']\n",
        "            self.bos_token_id = 3\n",
        "            # Character mapping starts after special tokens\n",
        "            self.first_char_token_id = 4\n",
        "            num_char_tokens = vocab_size - self.first_char_token_id\n",
        "            # Simple a-z mapping, wrapping around if needed\n",
        "            self.vocab = {i: chr(((i - self.first_char_token_id) % 26) + 97)\n",
        "                          for i in range(self.first_char_token_id, vocab_size)}\n",
        "            self.reverse_vocab = {v: k for k, v in self.vocab.items()} # Map chars to IDs\n",
        "\n",
        "            # Add special tokens to vocab for decoding\n",
        "            self.vocab[self.pad_token_id] = \"[PAD]\"\n",
        "            self.vocab[self.unk_token_id] = \"[UNK]\"\n",
        "            self.vocab[self.eos_token_id] = \"[EOS]\"\n",
        "            self.vocab[self.bos_token_id] = \"[BOS]\"\n",
        "\n",
        "\n",
        "        def __call__(self, text, return_tensors=\"pt\", add_special_tokens=True):\n",
        "            tokens = []\n",
        "            if add_special_tokens and self.bos_token_id is not None:\n",
        "                 # Ensure BOS token ID is valid before adding\n",
        "                 if 0 <= self.bos_token_id < self.vocab_size:\n",
        "                     tokens.append(self.bos_token_id)\n",
        "\n",
        "            for char in text.lower(): # Example: lowercase\n",
        "                token_id = self.reverse_vocab.get(char, self.unk_token_id)\n",
        "                # Ensure the ID is valid before adding (should be caught by .get default)\n",
        "                if 0 <= token_id < self.vocab_size:\n",
        "                     tokens.append(token_id)\n",
        "                else:\n",
        "                     # This case should ideally not happen with the .get default\n",
        "                     tokens.append(self.unk_token_id)\n",
        "\n",
        "            # Optional: Add EOS token if needed by model training/generation\n",
        "            # if add_special_tokens and self.eos_token_id is not None:\n",
        "            #     if 0 <= self.eos_token_id < self.vocab_size:\n",
        "            #          tokens.append(self.eos_token_id)\n",
        "\n",
        "            if return_tensors == \"pt\":\n",
        "                return SimpleNamespace(input_ids=torch.tensor([tokens], dtype=torch.long))\n",
        "            return tokens\n",
        "\n",
        "        def decode(self, ids, skip_special_tokens=True):\n",
        "            if isinstance(ids, torch.Tensor):\n",
        "                ids = ids.squeeze().tolist() # Remove batch dim if present\n",
        "            if isinstance(ids, int): # Handle single ID case\n",
        "                ids = [ids]\n",
        "\n",
        "            text = \"\"\n",
        "            for token_id in ids:\n",
        "                if skip_special_tokens and token_id in [self.pad_token_id, self.unk_token_id, self.eos_token_id, self.bos_token_id]:\n",
        "                    continue\n",
        "                # Map valid IDs back to characters/special token strings\n",
        "                char = self.vocab.get(token_id, \"[UNK]\") # Use UNK string if ID is somehow invalid\n",
        "                text += char\n",
        "            return text\n",
        "\n",
        "    return SimpleTokenizer(vocab_size)\n",
        "\n",
        "# --- Dummy Weights for Testing (Fallback) ---\n",
        "# This function is now less critical if you always provide the real path,\n",
        "# but kept as a fallback or for testing without the file.\n",
        "def create_dummy_weights(model):\n",
        "    print(\"Creating dummy weights (fallback or testing without file).\")\n",
        "    dummy_weights = {}\n",
        "    state_dict = model.state_dict()\n",
        "\n",
        "    # --- Define the Mapping from Safetensors Key -> Model Key ---\n",
        "    # *** This map MUST be consistent with the one in _init_weights_from_safetensors ***\n",
        "    key_map_for_dummy = {\n",
        "        'local_encoder.tok_embeddings.weight': 'tok_embeddings.weight',\n",
        "        'norm.weight': 'norm.weight', 'norm.bias': 'norm.bias',\n",
        "        'lm_head.weight': 'output.weight'\n",
        "    }\n",
        "    for i in range(model.config['n_layers']):\n",
        "        layer_prefix_file = f'layers.{i}.'\n",
        "        layer_prefix_model = f'layers.{i}.'\n",
        "        key_map_for_dummy.update({\n",
        "            layer_prefix_file + 'attention.q_proj.weight': layer_prefix_model + 'attention.wq.weight',\n",
        "            layer_prefix_file + 'attention.k_proj.weight': layer_prefix_model + 'attention.wk.weight',\n",
        "            layer_prefix_file + 'attention.v_proj.weight': layer_prefix_model + 'attention.wv.weight',\n",
        "            layer_prefix_file + 'attention.out_proj.weight': layer_prefix_model + 'attention.wo.weight',\n",
        "            layer_prefix_file + 'attention_norm.weight': layer_prefix_model + 'attention_norm.weight',\n",
        "            layer_prefix_file + 'attention_norm.bias': layer_prefix_model + 'attention_norm.bias',\n",
        "            layer_prefix_file + 'feed_forward.w1.weight': layer_prefix_model + 'ffn.w1.weight',\n",
        "            layer_prefix_file + 'feed_forward.w2.weight': layer_prefix_model + 'ffn.w2.weight',\n",
        "            layer_prefix_file + 'ffn_norm.weight': layer_prefix_model + 'ffn_norm.weight',\n",
        "            layer_prefix_file + 'ffn_norm.bias': layer_prefix_model + 'ffn_norm.bias',\n",
        "        })\n",
        "\n",
        "    # Create dummy tensors based on the *model's* state dict shapes,\n",
        "    # using the *safetensors keys* expected by the loading function.\n",
        "    expected_safetensors_keys = list(key_map_for_dummy.keys())\n",
        "\n",
        "    for skey in expected_safetensors_keys:\n",
        "        mkey = key_map_for_dummy.get(skey)\n",
        "        if mkey and mkey in state_dict:\n",
        "            # Create a zero tensor with the correct shape and type\n",
        "            dummy_weights[skey] = torch.zeros_like(state_dict[mkey])\n",
        "        # else:\n",
        "        #     print(f\"Warning: Cannot create dummy weight for {skey}, model key '{mkey}' not found or mapped.\")\n",
        "\n",
        "    print(f\"Created {len(dummy_weights)} dummy weight tensors based on expected keys.\")\n",
        "    return dummy_weights\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "def run_model(weights_path=None):\n",
        "    print(\"تهيئة النموذج...\")\n",
        "    # Determine device early\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = BLTModel(MODEL_CONFIG)\n",
        "    # Move model to device BEFORE loading weights if weights need to match device\n",
        "    model.to(device)\n",
        "    model.eval() # Set to evaluation mode\n",
        "\n",
        "    model_weights = None\n",
        "    # Load weights if path is provided and exists\n",
        "    if weights_path and os.path.exists(weights_path):\n",
        "        try:\n",
        "            print(f\"محاولة تحميل الأوزان من: {weights_path}\")\n",
        "            model_weights = load_file(weights_path, device=str(device)) # Load directly to target device\n",
        "            print(f\"تم تحميل ملف الأوزان بنجاح.\")\n",
        "            model._init_weights_from_safetensors(model_weights) # Use the specific loading method\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"خطأ أثناء تحميل الأوزان من {weights_path}: {e}\")\n",
        "            print(\"!!! فشل تحميل الأوزان الحقيقية. سيتم استخدام الأوزان الأولية الوهمية !!!\")\n",
        "            model_weights = None # Ensure we fallback to dummy weights\n",
        "\n",
        "    # Fallback to dummy weights if path not provided, file not found, or loading failed\n",
        "    if model_weights is None:\n",
        "        if weights_path and not os.path.exists(weights_path):\n",
        "             print(f\"لم يتم العثور على ملف الأوزان في المسار المحدد: {weights_path}\")\n",
        "        elif not weights_path:\n",
        "             print(\"لم يتم توفير مسار للأوزان.\")\n",
        "        print(\"استخدام الأوزان الأولية الوهمية.\")\n",
        "        dummy_weights = create_dummy_weights(model)\n",
        "        # Need to move dummy weights to the correct device as well\n",
        "        dummy_weights_on_device = {k: v.to(device) for k, v in dummy_weights.items()}\n",
        "        model._init_weights_from_safetensors(dummy_weights_on_device)\n",
        "\n",
        "\n",
        "    print(\"\\nتجهيز واختبار المحول...\")\n",
        "    tokenizer = load_tokenizer(MODEL_CONFIG['vocab_size'])\n",
        "    print(f\"تم استخدام SimpleTokenizer مع حجم مفردات: {tokenizer.vocab_size}\")\n",
        "\n",
        "    print(\"\\nاختبار النموذج مع مثال بسيط...\")\n",
        "    prompt = \"hi\"\n",
        "    tokenized_output = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True) # Add BOS\n",
        "    input_ids = tokenized_output.input_ids.to(device) # Move input to the correct device\n",
        "\n",
        "    print(f\"المدخل: '{prompt}'\")\n",
        "    # Decode the input using the tokenizer to see how it's represented\n",
        "    decoded_input_check = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
        "    print(f\"المدخل المرمز (مع الرموز الخاصة): {decoded_input_check}\")\n",
        "    print(f\"رموز المدخل (Tensor): {input_ids.tolist()}\")\n",
        "\n",
        "    # Check if input IDs are valid before generation\n",
        "    if torch.any(input_ids >= MODEL_CONFIG['vocab_size']) or torch.any(input_ids < 0):\n",
        "         print(f\"خطأ فادح: رموز المدخل {input_ids.tolist()} خارج النطاق [0, {MODEL_CONFIG['vocab_size']-1}]\")\n",
        "         return model, tokenizer # Stop before generation\n",
        "\n",
        "    print(\"\\nبدء التوليد...\")\n",
        "    try:\n",
        "        # Generate text\n",
        "        output_ids = model.generate(input_ids=input_ids, max_new_tokens=30, temperature=0.7, top_k=10) # Reduced top_k for small vocab\n",
        "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True) # Skip special tokens for final output\n",
        "\n",
        "        # Decode the full generated sequence including prompt and special tokens for inspection\n",
        "        full_decoded_sequence = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
        "        print(f\"\\nالتسلسل الكامل المولد (مع الرموز الخاصة): {full_decoded_sequence}\")\n",
        "        print(f\"النص المولد النهائي (بدون الرموز الخاصة): '{generated_text}'\")\n",
        "\n",
        "    except IndexError as e:\n",
        "        print(f\"حدث خطأ IndexError أثناء التوليد: {e}\")\n",
        "        print(\"Check model configuration, tokenizer alignment, and input IDs.\")\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ غير متوقع أثناء التوليد: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print detailed traceback for unexpected errors\n",
        "\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # *** USE THE CORRECT PATH TO YOUR FILE HERE ***\n",
        "    # Make sure the directory 'safetensors/blt_1b/' exists relative to your script\n",
        "    # or provide an absolute path.\n",
        "    weights_file_path = 'safetensors/blt_1b/consolidated.safetensors'\n",
        "\n",
        "    # Optional: Inspect keys first if loading fails\n",
        "    # try:\n",
        "    #     print(\"Inspecting keys in safetensors file...\")\n",
        "    #     inspect_weights = load_file(weights_file_path)\n",
        "    #     print(f\"Found {len(inspect_weights)} keys. First 20 keys:\")\n",
        "    #     keys_list = list(inspect_weights.keys())\n",
        "    #     for i, key in enumerate(keys_list[:20]):\n",
        "    #         print(f\"  {i}: {key} (Shape: {inspect_weights[key].shape})\")\n",
        "    #     if len(keys_list) > 20: print(\"  ...\")\n",
        "    #     del inspect_weights # Free memory\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Could not inspect keys: {e}\")\n",
        "\n",
        "    run_model(weights_path=weights_file_path)\n",
        "\n",
        "    # Note: The entropy_weights are loaded but not used by this script.\n",
        "    # entropy_weights = load_file('safetensors/entropy_model/consolidated.safetensors')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixy9WEkIIMsF",
        "outputId": "fb07c4c8-f6ce-4e0a-d42a-bba2d6640d08"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تهيئة النموذج...\n",
            "Using device: cpu\n",
            "محاولة تحميل الأوزان من: safetensors/blt_1b/consolidated.safetensors\n",
            "تم تحميل ملف الأوزان بنجاح.\n",
            "تهيئة أوزان النموذج من الملف المحمل...\n",
            "--- Weight Loading Summary ---\n",
            "تم تحميل 1 من أصل 386 مفتاح معرف من الملف بنجاح.\n",
            "\n",
            "Skipped 385 keys due to NO MAPPING DEFINED in key_map:\n",
            "  - encoder_hash_tok_embedding.0.weight\n",
            "  - encoder_hash_tok_embedding.1.weight\n",
            "  - encoder_hash_tok_embedding.2.weight\n",
            "  - encoder_hash_tok_embedding.3.weight\n",
            "  - encoder_hash_tok_embedding.4.weight\n",
            "  ...\n",
            "\n",
            "Model parameters NOT found in the loaded weights (using initial values):\n",
            "  - layers.0.attention.mask\n",
            "  - layers.0.attention.wq.weight\n",
            "  - layers.0.attention.wk.weight\n",
            "  - layers.0.attention.wv.weight\n",
            "  - layers.0.attention.wo.weight\n",
            "  - layers.0.attention_norm.weight\n",
            "  - layers.0.attention_norm.bias\n",
            "  - layers.0.ffn.w1.weight\n",
            "  - layers.0.ffn.w2.weight\n",
            "  - layers.0.ffn_norm.weight\n",
            "  ...\n",
            "--- End Summary ---\n",
            "\n",
            "تجهيز واختبار المحول...\n",
            "تم استخدام SimpleTokenizer مع حجم مفردات: 260\n",
            "\n",
            "اختبار النموذج مع مثال بسيط...\n",
            "المدخل: 'hi'\n",
            "المدخل المرمز (مع الرموز الخاصة): [BOS]hi\n",
            "رموز المدخل (Tensor): [[3, 245, 246]]\n",
            "\n",
            "بدء التوليد...\n",
            "\n",
            "التسلسل الكامل المولد (مع الرموز الخاصة): [BOS]hiedqeydomdioyyeisoowudyeszowybi\n",
            "النص المولد النهائي (بدون الرموز الخاصة): 'hiedqeydomdioyyeisoowudyeszowybi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure you are in the correct directory after cloning and installing\n",
        "# %cd /content/blt\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "# --- Make sure the bytelatent library is installed and importable ---\n",
        "try:\n",
        "    from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "    from bytelatent.generate_blt import generate_nocache\n",
        "    from bytelatent.data.patcher import PatcherArgs # Needed for patcher\n",
        "    print(\"Successfully imported from bytelatent library.\")\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing from bytelatent: {e}\")\n",
        "    print(\"Please ensure you have cloned the 'blt' repository and installed it correctly:\")\n",
        "    print(\"1. !git clone https://github.com/facebookresearch/blt.git\")\n",
        "    print(\"2. %cd blt\")\n",
        "    print(\"3. !pip install -e .\")\n",
        "    exit() # Stop execution if library not found\n",
        "\n",
        "# --- Configuration ---\n",
        "# 1) Point this to the FOLDER containing the safetensors files\n",
        "#    It should contain:\n",
        "#    - consolidated.safetensors (for the main model)\n",
        "#    - entropy_model/consolidated.safetensors\n",
        "#    - Ideally, also the train_config.yaml or similar config file if needed by the loading function\n",
        "checkpoint_dir = \"/content/safetensors/blt_1b\" # Directory containing the main model file\n",
        "entropy_model_dir = os.path.join(checkpoint_dir, \"entropy_model\") # Standard location\n",
        "\n",
        "# Verify paths\n",
        "print(f\"Checking main model directory: {checkpoint_dir}\")\n",
        "if not os.path.exists(os.path.join(checkpoint_dir, \"consolidated.safetensors\")):\n",
        "    print(f\"ERROR: Main model file not found in {checkpoint_dir}\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"✓ Main model file found.\")\n",
        "\n",
        "print(f\"Checking entropy model directory: {entropy_model_dir}\")\n",
        "if not os.path.exists(os.path.join(entropy_model_dir, \"consolidated.safetensors\")):\n",
        "    print(f\"ERROR: Entropy model file not found in {entropy_model_dir}\")\n",
        "    # Decide if you want to proceed without it or exit\n",
        "    print(\"Warning: Proceeding without entropy model patching.\")\n",
        "    use_entropy_model = False\n",
        "    # exit() # Uncomment to stop if entropy model is required\n",
        "else:\n",
        "    print(\"✓ Entropy model file found.\")\n",
        "    use_entropy_model = True\n",
        "\n",
        "\n",
        "# --- Load Model and Tokenizer using the Library Function ---\n",
        "print(\"\\nLoading model, tokenizer, and config using bytelatent library...\")\n",
        "try:\n",
        "    # This function should handle loading the config, model, tokenizer, and weights\n",
        "    # It might look for a config file within checkpoint_dir\n",
        "    model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(checkpoint_dir)\n",
        "    print(\"✓ Model, tokenizer, and config loaded successfully.\")\n",
        "\n",
        "    # Move model to appropriate device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval() # Set to evaluation mode\n",
        "    print(f\"Model moved to device: {device}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "     print(f\"ERROR loading model: {e}\")\n",
        "     print(\"Ensure the checkpoint directory contains necessary files (like config).\")\n",
        "     exit()\n",
        "except Exception as e:\n",
        "     print(f\"An unexpected error occurred during model loading: {e}\")\n",
        "     import traceback\n",
        "     traceback.print_exc()\n",
        "     exit()\n",
        "\n",
        "# --- Build the Patcher ---\n",
        "print(\"\\nBuilding patcher...\")\n",
        "try:\n",
        "    patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "    patcher_args.realtime_patching = True # Use realtime entropy patching\n",
        "\n",
        "    if use_entropy_model:\n",
        "        patcher_args.entropy_model_checkpoint_dir = entropy_model_dir\n",
        "        print(f\"Patcher will use entropy model from: {entropy_model_dir}\")\n",
        "    else:\n",
        "         patcher_args.entropy_model_checkpoint_dir = None\n",
        "         print(\"Patcher will NOT use entropy model.\")\n",
        "\n",
        "    patcher = patcher_args.build()\n",
        "    print(\"✓ Patcher built successfully.\")\n",
        "\n",
        "    # If using entropy model, move its sub-model to the device too\n",
        "    if use_entropy_model and hasattr(patcher, 'entropy_model') and patcher.entropy_model is not None:\n",
        "        patcher.entropy_model.to(device)\n",
        "        patcher.entropy_model.eval()\n",
        "        print(\"Entropy model moved to device.\")\n",
        "\n",
        "except AttributeError as e:\n",
        "    print(f\"Error accessing patcher configuration: {e}\")\n",
        "    print(\"The loaded 'train_cfg' might not have the expected structure.\")\n",
        "    print(\"Setting patcher to None.\")\n",
        "    patcher = None\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during patcher building: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"Setting patcher to None.\")\n",
        "    patcher = None\n",
        "\n",
        "\n",
        "# --- Generate Text ---\n",
        "print(\"\\nStarting generation...\")\n",
        "prompt = \"hi\" # Your simple prompt\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "\n",
        "# The generate_nocache function likely expects a list of prompts\n",
        "prompts = [prompt]\n",
        "\n",
        "try:\n",
        "    # Note: generate_nocache might handle tokenization internally, or you might need to tokenize first.\n",
        "    # Check the function signature or examples in the blt repo.\n",
        "    # Assuming it handles tokenization:\n",
        "    outputs = generate_nocache(\n",
        "        prompts,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer, # Pass the loaded tokenizer\n",
        "        patcher=patcher,     # Pass the patcher\n",
        "        max_new_tokens=30,   # Generate fewer tokens for testing\n",
        "        temperature=0.7,\n",
        "        top_k=40,\n",
        "        device=device        # Specify the device\n",
        "    )\n",
        "    print(\"✓ Generation complete.\")\n",
        "\n",
        "    # --- Decode and Print ---\n",
        "    print(\"\\nGenerated Output:\")\n",
        "    for i, out_ids in enumerate(outputs):\n",
        "        # The output might be on GPU, move to CPU for decoding if needed\n",
        "        out_ids_cpu = out_ids.cpu()\n",
        "        decoded_text = tokenizer.decode(out_ids_cpu)\n",
        "        print(f\"Output {i+1}: {decoded_text}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during generation: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fLiDvAaIOHV",
        "outputId": "a38ce7ef-5d52-446d-f265-9a133c20cb67"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error importing from bytelatent: No module named 'omegaconf'\n",
            "Please ensure you have cloned the 'blt' repository and installed it correctly:\n",
            "1. !git clone https://github.com/facebookresearch/blt.git\n",
            "2. %cd blt\n",
            "3. !pip install -e .\n",
            "Checking main model directory: /content/safetensors/blt_1b\n",
            "✓ Main model file found.\n",
            "Checking entropy model directory: /content/safetensors/blt_1b/entropy_model\n",
            "ERROR: Entropy model file not found in /content/safetensors/blt_1b/entropy_model\n",
            "Warning: Proceeding without entropy model patching.\n",
            "\n",
            "Loading model, tokenizer, and config using bytelatent library...\n",
            "An unexpected error occurred during model loading: name 'load_consolidated_model_and_tokenizer' is not defined\n",
            "\n",
            "Building patcher...\n",
            "An unexpected error occurred during patcher building: name 'train_cfg' is not defined\n",
            "Setting patcher to None.\n",
            "\n",
            "Starting generation...\n",
            "Prompt: 'hi'\n",
            "An error occurred during generation: name 'generate_nocache' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-16-5332dde3b46b>\", line 56, in <cell line: 0>\n",
            "    model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(checkpoint_dir)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "NameError: name 'load_consolidated_model_and_tokenizer' is not defined\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-16-5332dde3b46b>\", line 78, in <cell line: 0>\n",
            "    patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
            "                   ^^^^^^^^^\n",
            "NameError: name 'train_cfg' is not defined\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-16-5332dde3b46b>\", line 122, in <cell line: 0>\n",
            "    outputs = generate_nocache(\n",
            "              ^^^^^^^^^^^^^^^^\n",
            "NameError: name 'generate_nocache' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJB20smjJCAj",
        "outputId": "063fc0d9-448e-4042-f493-615069f7f443"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Installation (Run this cell first) ---\n",
        "!pip install omegaconf --quiet # Install the missing dependency\n",
        "# Ensure the bytelatent library is installed (assuming you've already run this)\n",
        "# If not, uncomment and run these lines in the correct order:\n",
        "# !git clone https://github.com/facebookresearch/blt.git\n",
        "# %cd blt\n",
        "# !pip install -e . --quiet\n",
        "# %cd .. # Go back to the parent directory (/content)\n",
        "\n",
        "# --- Main Script ---\n",
        "import torch\n",
        "import os\n",
        "from safetensors.torch import load_file\n",
        "import warnings\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# Suppress specific warnings if needed (optional)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "# --- Attempt to import from bytelatent library AFTER installing omegaconf ---\n",
        "try:\n",
        "    from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "    from bytelatent.generate_blt import generate_nocache\n",
        "    from bytelatent.data.patcher import PatcherArgs # Needed for patcher\n",
        "    print(\"Successfully imported from bytelatent library.\")\n",
        "    imports_successful = True\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing from bytelatent: {e}\")\n",
        "    print(\"Please ensure you have cloned the 'blt' repository and installed it correctly:\")\n",
        "    print(\"1. !git clone https://github.com/facebookresearch/blt.git\")\n",
        "    print(\"2. %cd blt\")\n",
        "    print(\"3. !pip install -e .\")\n",
        "    print(\"4. Also ensure 'omegaconf' is installed: !pip install omegaconf\")\n",
        "    imports_successful = False\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during import: {e}\")\n",
        "    traceback.print_exc()\n",
        "    imports_successful = False\n",
        "\n",
        "# Proceed only if imports were successful\n",
        "if imports_successful:\n",
        "    # --- Configuration ---\n",
        "    checkpoint_dir = \"/content/safetensors/blt_1b\" # Directory containing the main model file\n",
        "    # *** CORRECTED PATH for entropy model ***\n",
        "    entropy_model_dir = \"/content/safetensors/entropy_model\" # Direct path to the entropy model folder\n",
        "\n",
        "    # Verify paths\n",
        "    print(f\"\\nChecking main model directory: {checkpoint_dir}\")\n",
        "    main_model_file = os.path.join(checkpoint_dir, \"consolidated.safetensors\")\n",
        "    if not os.path.exists(main_model_file):\n",
        "        print(f\"ERROR: Main model file not found: {main_model_file}\")\n",
        "        exit()\n",
        "    else:\n",
        "        print(\"✓ Main model file found.\")\n",
        "\n",
        "    print(f\"Checking entropy model directory: {entropy_model_dir}\")\n",
        "    entropy_model_file = os.path.join(entropy_model_dir, \"consolidated.safetensors\")\n",
        "    if not os.path.exists(entropy_model_file):\n",
        "        print(f\"ERROR: Entropy model file not found: {entropy_model_file}\")\n",
        "        print(\"Warning: Proceeding without entropy model patching.\")\n",
        "        use_entropy_model = False\n",
        "    else:\n",
        "        print(\"✓ Entropy model file found.\")\n",
        "        use_entropy_model = True\n",
        "\n",
        "    # --- Load Model and Tokenizer using the Library Function ---\n",
        "    print(\"\\nLoading model, tokenizer, and config using bytelatent library...\")\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    train_cfg = None\n",
        "    try:\n",
        "        # This function should handle loading the config, model, tokenizer, and weights\n",
        "        # It might look for a config file within checkpoint_dir\n",
        "        model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(checkpoint_dir)\n",
        "        print(\"✓ Model, tokenizer, and config loaded successfully.\")\n",
        "\n",
        "        # Move model to appropriate device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        model.eval() # Set to evaluation mode\n",
        "        print(f\"Model moved to device: {device}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "         print(f\"ERROR loading model: {e}\")\n",
        "         print(\"Ensure the checkpoint directory contains necessary files (like config.yaml or train_config.yaml).\")\n",
        "         print(\"You might need to download the config file from the ttj/blt repo.\")\n",
        "         exit()\n",
        "    except Exception as e:\n",
        "         print(f\"An unexpected error occurred during model loading: {e}\")\n",
        "         traceback.print_exc()\n",
        "         exit()\n",
        "\n",
        "    # --- Build the Patcher ---\n",
        "    print(\"\\nBuilding patcher...\")\n",
        "    patcher = None # Initialize patcher to None\n",
        "    if train_cfg is not None: # Proceed only if config was loaded\n",
        "        try:\n",
        "            # Check if patcher config exists\n",
        "            if hasattr(train_cfg, 'data') and hasattr(train_cfg.data, 'patcher_args'):\n",
        "                patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "                patcher_args.realtime_patching = True # Use realtime entropy patching\n",
        "\n",
        "                if use_entropy_model:\n",
        "                    patcher_args.entropy_model_checkpoint_dir = entropy_model_dir\n",
        "                    print(f\"Patcher will use entropy model from: {entropy_model_dir}\")\n",
        "                else:\n",
        "                     patcher_args.entropy_model_checkpoint_dir = None\n",
        "                     print(\"Patcher will NOT use entropy model.\")\n",
        "\n",
        "                patcher = patcher_args.build()\n",
        "                print(\"✓ Patcher built successfully.\")\n",
        "\n",
        "                # If using entropy model, move its sub-model to the device too\n",
        "                if use_entropy_model and hasattr(patcher, 'entropy_model') and patcher.entropy_model is not None:\n",
        "                    patcher.entropy_model.to(device)\n",
        "                    patcher.entropy_model.eval()\n",
        "                    print(\"Entropy model moved to device.\")\n",
        "            else:\n",
        "                print(\"Patcher configuration not found in train_cfg.data.patcher_args. Skipping patcher.\")\n",
        "\n",
        "        except AttributeError as e:\n",
        "            print(f\"Error accessing patcher configuration: {e}\")\n",
        "            print(\"The loaded 'train_cfg' might not have the expected structure.\")\n",
        "            print(\"Skipping patcher.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during patcher building: {e}\")\n",
        "            traceback.print_exc()\n",
        "            print(\"Skipping patcher.\")\n",
        "    else:\n",
        "        print(\"train_cfg not loaded, cannot build patcher.\")\n",
        "\n",
        "\n",
        "    # --- Generate Text ---\n",
        "    print(\"\\nStarting generation...\")\n",
        "    prompt = \"hi\" # Your simple prompt\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "\n",
        "    # The generate_nocache function likely expects a list of prompts\n",
        "    prompts = [prompt]\n",
        "\n",
        "    if model is not None and tokenizer is not None: # Proceed only if model/tokenizer loaded\n",
        "        try:\n",
        "            outputs = generate_nocache(\n",
        "                prompts,\n",
        "                model=model,\n",
        "                tokenizer=tokenizer, # Pass the loaded tokenizer\n",
        "                patcher=patcher,     # Pass the patcher (can be None)\n",
        "                max_new_tokens=30,   # Generate fewer tokens for testing\n",
        "                temperature=0.7,\n",
        "                top_k=40,\n",
        "                device=device        # Specify the device\n",
        "            )\n",
        "            print(\"✓ Generation complete.\")\n",
        "\n",
        "            # --- Decode and Print ---\n",
        "            print(\"\\nGenerated Output:\")\n",
        "            for i, out_ids in enumerate(outputs):\n",
        "                # The output might be on GPU, move to CPU for decoding if needed\n",
        "                out_ids_cpu = out_ids.cpu()\n",
        "                # Use the tokenizer's decode method\n",
        "                decoded_text = tokenizer.decode(out_ids_cpu)\n",
        "                print(f\"Output {i+1}: {decoded_text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during generation: {e}\")\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"Model or tokenizer not loaded, cannot generate.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nExiting due to import errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLby0dtEIx7k",
        "outputId": "f5b6293c-aa68-4117-a4b1-289af0491d31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error importing from bytelatent: No module named 's3fs'\n",
            "Please ensure you have cloned the 'blt' repository and installed it correctly:\n",
            "1. !git clone https://github.com/facebookresearch/blt.git\n",
            "2. %cd blt\n",
            "3. !pip install -e .\n",
            "4. Also ensure 'omegaconf' is installed: !pip install omegaconf\n",
            "\n",
            "Exiting due to import errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Installation (Run this cell first) ---\n",
        "!pip install omegaconf --quiet # Install the missing dependency\n",
        "# Ensure the bytelatent library is installed (assuming you've already run this)\n",
        "# If not, uncomment and run these lines in the correct order:\n",
        "# !git clone https://github.com/facebookresearch/blt.git\n",
        "# %cd blt\n",
        "# !pip install -e . --quiet\n",
        "# %cd .. # Go back to the parent directory (/content)\n",
        "\n",
        "# --- Main Script ---\n",
        "import torch\n",
        "import os\n",
        "from safetensors.torch import load_file\n",
        "import warnings\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# Suppress specific warnings if needed (optional)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "# --- Attempt to import from bytelatent library AFTER installing omegaconf ---\n",
        "try:\n",
        "    from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "    from bytelatent.generate_blt import generate_nocache\n",
        "    from bytelatent.data.patcher import PatcherArgs # Needed for patcher\n",
        "    print(\"Successfully imported from bytelatent library.\")\n",
        "    imports_successful = True\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing from bytelatent: {e}\")\n",
        "    print(\"Please ensure you have cloned the 'blt' repository and installed it correctly:\")\n",
        "    print(\"1. !git clone https://github.com/facebookresearch/blt.git\")\n",
        "    print(\"2. %cd blt\")\n",
        "    print(\"3. !pip install -e .\")\n",
        "    print(\"4. Also ensure 'omegaconf' is installed: !pip install omegaconf\")\n",
        "    imports_successful = False\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during import: {e}\")\n",
        "    traceback.print_exc()\n",
        "    imports_successful = False\n",
        "\n",
        "# Proceed only if imports were successful\n",
        "if imports_successful:\n",
        "    # --- Configuration ---\n",
        "    checkpoint_dir = \"/content/safetensors/blt_1b\" # Directory containing the main model file\n",
        "    # *** CORRECTED PATH for entropy model ***\n",
        "    entropy_model_dir = \"/content/safetensors/entropy_model\" # Direct path to the entropy model folder\n",
        "\n",
        "    # Verify paths\n",
        "    print(f\"\\nChecking main model directory: {checkpoint_dir}\")\n",
        "    main_model_file = os.path.join(checkpoint_dir, \"consolidated.safetensors\")\n",
        "    if not os.path.exists(main_model_file):\n",
        "        print(f\"ERROR: Main model file not found: {main_model_file}\")\n",
        "        exit()\n",
        "    else:\n",
        "        print(\"✓ Main model file found.\")\n",
        "\n",
        "    print(f\"Checking entropy model directory: {entropy_model_dir}\")\n",
        "    entropy_model_file = os.path.join(entropy_model_dir, \"consolidated.safetensors\")\n",
        "    if not os.path.exists(entropy_model_file):\n",
        "        print(f\"ERROR: Entropy model file not found: {entropy_model_file}\")\n",
        "        print(\"Warning: Proceeding without entropy model patching.\")\n",
        "        use_entropy_model = False\n",
        "    else:\n",
        "        print(\"✓ Entropy model file found.\")\n",
        "        use_entropy_model = True\n",
        "\n",
        "    # --- Load Model and Tokenizer using the Library Function ---\n",
        "    print(\"\\nLoading model, tokenizer, and config using bytelatent library...\")\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    train_cfg = None\n",
        "    try:\n",
        "        # This function should handle loading the config, model, tokenizer, and weights\n",
        "        # It might look for a config file within checkpoint_dir\n",
        "        model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(checkpoint_dir)\n",
        "        print(\"✓ Model, tokenizer, and config loaded successfully.\")\n",
        "\n",
        "        # Move model to appropriate device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        model.eval() # Set to evaluation mode\n",
        "        print(f\"Model moved to device: {device}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "         print(f\"ERROR loading model: {e}\")\n",
        "         print(\"Ensure the checkpoint directory contains necessary files (like config.yaml or train_config.yaml).\")\n",
        "         print(\"You might need to download the config file from the ttj/blt repo.\")\n",
        "         exit()\n",
        "    except Exception as e:\n",
        "         print(f\"An unexpected error occurred during model loading: {e}\")\n",
        "         traceback.print_exc()\n",
        "         exit()\n",
        "\n",
        "    # --- Build the Patcher ---\n",
        "    print(\"\\nBuilding patcher...\")\n",
        "    patcher = None # Initialize patcher to None\n",
        "    if train_cfg is not None: # Proceed only if config was loaded\n",
        "        try:\n",
        "            # Check if patcher config exists\n",
        "            if hasattr(train_cfg, 'data') and hasattr(train_cfg.data, 'patcher_args'):\n",
        "                patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "                patcher_args.realtime_patching = True # Use realtime entropy patching\n",
        "\n",
        "                if use_entropy_model:\n",
        "                    patcher_args.entropy_model_checkpoint_dir = entropy_model_dir\n",
        "                    print(f\"Patcher will use entropy model from: {entropy_model_dir}\")\n",
        "                else:\n",
        "                     patcher_args.entropy_model_checkpoint_dir = None\n",
        "                     print(\"Patcher will NOT use entropy model.\")\n",
        "\n",
        "                patcher = patcher_args.build()\n",
        "                print(\"✓ Patcher built successfully.\")\n",
        "\n",
        "                # If using entropy model, move its sub-model to the device too\n",
        "                if use_entropy_model and hasattr(patcher, 'entropy_model') and patcher.entropy_model is not None:\n",
        "                    patcher.entropy_model.to(device)\n",
        "                    patcher.entropy_model.eval()\n",
        "                    print(\"Entropy model moved to device.\")\n",
        "            else:\n",
        "                print(\"Patcher configuration not found in train_cfg.data.patcher_args. Skipping patcher.\")\n",
        "\n",
        "        except AttributeError as e:\n",
        "            print(f\"Error accessing patcher configuration: {e}\")\n",
        "            print(\"The loaded 'train_cfg' might not have the expected structure.\")\n",
        "            print(\"Skipping patcher.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during patcher building: {e}\")\n",
        "            traceback.print_exc()\n",
        "            print(\"Skipping patcher.\")\n",
        "    else:\n",
        "        print(\"train_cfg not loaded, cannot build patcher.\")\n",
        "\n",
        "\n",
        "    # --- Generate Text ---\n",
        "    print(\"\\nStarting generation...\")\n",
        "    prompt = \"hi\" # Your simple prompt\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "\n",
        "    # The generate_nocache function likely expects a list of prompts\n",
        "    prompts = [prompt]\n",
        "\n",
        "    if model is not None and tokenizer is not None: # Proceed only if model/tokenizer loaded\n",
        "        try:\n",
        "            outputs = generate_nocache(\n",
        "                prompts,\n",
        "                model=model,\n",
        "                tokenizer=tokenizer, # Pass the loaded tokenizer\n",
        "                patcher=patcher,     # Pass the patcher (can be None)\n",
        "                max_new_tokens=30,   # Generate fewer tokens for testing\n",
        "                temperature=0.7,\n",
        "                top_k=40,\n",
        "                device=device        # Specify the device\n",
        "            )\n",
        "            print(\"✓ Generation complete.\")\n",
        "\n",
        "            # --- Decode and Print ---\n",
        "            print(\"\\nGenerated Output:\")\n",
        "            for i, out_ids in enumerate(outputs):\n",
        "                # The output might be on GPU, move to CPU for decoding if needed\n",
        "                out_ids_cpu = out_ids.cpu()\n",
        "                # Use the tokenizer's decode method\n",
        "                decoded_text = tokenizer.decode(out_ids_cpu)\n",
        "                print(f\"Output {i+1}: {decoded_text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during generation: {e}\")\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"Model or tokenizer not loaded, cannot generate.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nExiting due to import errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNJgKPpRJJUs",
        "outputId": "9d19e20d-fe4b-4405-d615-e81137cf94d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error importing from bytelatent: No module named 's3fs'\n",
            "Please ensure you have cloned the 'blt' repository and installed it correctly:\n",
            "1. !git clone https://github.com/facebookresearch/blt.git\n",
            "2. %cd blt\n",
            "3. !pip install -e .\n",
            "4. Also ensure 'omegaconf' is installed: !pip install omegaconf\n",
            "\n",
            "Exiting due to import errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Script ---\n",
        "import torch\n",
        "import os\n",
        "from safetensors.torch import load_file\n",
        "import warnings\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# Suppress specific warnings if needed (optional)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "# --- Attempt to import from bytelatent library AFTER installing dependencies ---\n",
        "try:\n",
        "    from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "    from bytelatent.generate_blt import generate_nocache\n",
        "    from bytelatent.data.patcher import PatcherArgs # Needed for patcher\n",
        "    print(\"Successfully imported from bytelatent library.\")\n",
        "    imports_successful = True\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing from bytelatent: {e}\")\n",
        "    print(\"Please ensure you have cloned the 'blt' repository and installed it correctly:\")\n",
        "    print(\"1. !git clone https://github.com/facebookresearch/blt.git\")\n",
        "    print(\"2. %cd blt\")\n",
        "    print(\"3. !pip install -e .\")\n",
        "    print(\"4. Also ensure dependencies are installed: !pip install omegaconf s3fs\")\n",
        "    imports_successful = False\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during import: {e}\")\n",
        "    traceback.print_exc()\n",
        "    imports_successful = False\n",
        "\n",
        "# Proceed only if imports were successful\n",
        "if imports_successful:\n",
        "    # --- Configuration ---\n",
        "    checkpoint_dir = \"/content/safetensors/blt_1b\" # Directory containing the main model file\n",
        "    entropy_model_dir = \"/content/safetensors/entropy_model\" # Direct path to the entropy model folder\n",
        "\n",
        "    # Verify paths\n",
        "    print(f\"\\nChecking main model directory: {checkpoint_dir}\")\n",
        "    main_model_file = os.path.join(checkpoint_dir, \"consolidated.safetensors\")\n",
        "    if not os.path.exists(main_model_file):\n",
        "        print(f\"ERROR: Main model file not found: {main_model_file}\")\n",
        "        exit()\n",
        "    else:\n",
        "        print(\"✓ Main model file found.\")\n",
        "\n",
        "    print(f\"Checking entropy model directory: {entropy_model_dir}\")\n",
        "    entropy_model_file = os.path.join(entropy_model_dir, \"consolidated.safetensors\")\n",
        "    if not os.path.exists(entropy_model_file):\n",
        "        print(f\"ERROR: Entropy model file not found: {entropy_model_file}\")\n",
        "        print(\"Warning: Proceeding without entropy model patching.\")\n",
        "        use_entropy_model = False\n",
        "    else:\n",
        "        print(\"✓ Entropy model file found.\")\n",
        "        use_entropy_model = True\n",
        "\n",
        "    # --- Load Model and Tokenizer using the Library Function ---\n",
        "    print(\"\\nLoading model, tokenizer, and config using bytelatent library...\")\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    train_cfg = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device early\n",
        "    try:\n",
        "        # This function should handle loading the config, model, tokenizer, and weights\n",
        "        model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(checkpoint_dir)\n",
        "        print(\"✓ Model, tokenizer, and config loaded successfully.\")\n",
        "\n",
        "        # Move model to appropriate device\n",
        "        model.to(device)\n",
        "        model.eval() # Set to evaluation mode\n",
        "        print(f\"Model moved to device: {device}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "         print(f\"ERROR loading model: {e}\")\n",
        "         print(f\"Could not find a configuration file (e.g., config.yaml, train_config.yaml) in {checkpoint_dir}.\")\n",
        "         print(\"Please download the necessary config file from the ttj/blt Hugging Face repository and place it in that directory.\")\n",
        "         exit()\n",
        "    except Exception as e:\n",
        "         print(f\"An unexpected error occurred during model loading: {e}\")\n",
        "         traceback.print_exc()\n",
        "         exit()\n",
        "\n",
        "    # --- Build the Patcher ---\n",
        "    print(\"\\nBuilding patcher...\")\n",
        "    patcher = None # Initialize patcher to None\n",
        "    if train_cfg is not None: # Proceed only if config was loaded\n",
        "        try:\n",
        "            # Check if patcher config exists\n",
        "            if hasattr(train_cfg, 'data') and hasattr(train_cfg.data, 'patcher_args'):\n",
        "                patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "                patcher_args.realtime_patching = True # Use realtime entropy patching\n",
        "\n",
        "                if use_entropy_model:\n",
        "                    patcher_args.entropy_model_checkpoint_dir = entropy_model_dir\n",
        "                    print(f\"Patcher will use entropy model from: {entropy_model_dir}\")\n",
        "                else:\n",
        "                     patcher_args.entropy_model_checkpoint_dir = None\n",
        "                     print(\"Patcher will NOT use entropy model.\")\n",
        "\n",
        "                patcher = patcher_args.build()\n",
        "                print(\"✓ Patcher built successfully.\")\n",
        "\n",
        "                # If using entropy model, move its sub-model to the device too\n",
        "                if use_entropy_model and hasattr(patcher, 'entropy_model') and patcher.entropy_model is not None:\n",
        "                    patcher.entropy_model.to(device)\n",
        "                    patcher.entropy_model.eval()\n",
        "                    print(\"Entropy model moved to device.\")\n",
        "            else:\n",
        "                print(\"Patcher configuration not found in train_cfg.data.patcher_args. Skipping patcher.\")\n",
        "\n",
        "        except AttributeError as e:\n",
        "            print(f\"Error accessing patcher configuration: {e}\")\n",
        "            print(\"The loaded 'train_cfg' might not have the expected structure.\")\n",
        "            print(\"Skipping patcher.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during patcher building: {e}\")\n",
        "            traceback.print_exc()\n",
        "            print(\"Skipping patcher.\")\n",
        "    else:\n",
        "        print(\"train_cfg not loaded, cannot build patcher.\")\n",
        "\n",
        "\n",
        "    # --- Generate Text ---\n",
        "    print(\"\\nStarting generation...\")\n",
        "    prompt = \"hi\" # Your simple prompt\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "\n",
        "    # The generate_nocache function likely expects a list of prompts\n",
        "    prompts = [prompt]\n",
        "\n",
        "    if model is not None and tokenizer is not None: # Proceed only if model/tokenizer loaded\n",
        "        try:\n",
        "            outputs = generate_nocache(\n",
        "                prompts,\n",
        "                model=model,\n",
        "                tokenizer=tokenizer, # Pass the loaded tokenizer\n",
        "                patcher=patcher,     # Pass the patcher (can be None)\n",
        "                max_new_tokens=30,   # Generate fewer tokens for testing\n",
        "                temperature=0.7,\n",
        "                top_k=40,\n",
        "                device=device        # Specify the device\n",
        "            )\n",
        "            print(\"✓ Generation complete.\")\n",
        "\n",
        "            # --- Decode and Print ---\n",
        "            print(\"\\nGenerated Output:\")\n",
        "            for i, out_ids in enumerate(outputs):\n",
        "                # The output might be on GPU, move to CPU for decoding if needed\n",
        "                out_ids_cpu = out_ids.cpu()\n",
        "                # Use the tokenizer's decode method\n",
        "                decoded_text = tokenizer.decode(out_ids_cpu)\n",
        "                print(f\"Output {i+1}: {decoded_text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during generation: {e}\")\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"Model or tokenizer not loaded, cannot generate.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nExiting due to import errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsX_02efJaBE",
        "outputId": "db797ba6-2f08-4ff9-9b47-22dbd83aa299"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An unexpected error occurred during import: '_OpNamespace' 'xformers' object has no attribute 'efficient_attention_forward_cutlass'\n",
            "\n",
            "Exiting due to import errors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-5-addbc327f047>\", line 13, in <cell line: 0>\n",
            "    from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bytelatent/generate.py\", line 13, in <module>\n",
            "    from bytelatent.args import EvalArgs, PackedCausalTransformerGeneratorArgs, TrainArgs\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bytelatent/args.py\", line 10, in <module>\n",
            "    from bytelatent.checkpoint import CONSOLIDATE_FOLDER, CheckpointArgs\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bytelatent/checkpoint.py\", line 26, in <module>\n",
            "    from bytelatent.distributed import get_is_master\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bytelatent/distributed.py\", line 54, in <module>\n",
            "    torch.ops.xformers.efficient_attention_forward_cutlass.default\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1232, in __getattr__\n",
            "    raise AttributeError(\n",
            "AttributeError: '_OpNamespace' 'xformers' object has no attribute 'efficient_attention_forward_cutlass'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf s3fs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfj_iQsLJ7YM",
        "outputId": "ec41d692-93be-4aed-c196-3ac46d2d76f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Collecting s3fs\n",
            "  Downloading s3fs-2025.3.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf) (6.0.2)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs)\n",
            "  Downloading aiobotocore-2.21.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: fsspec==2025.3.2.* in /usr/local/lib/python3.11/dist-packages (from s3fs) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from s3fs) (3.11.15)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting botocore<1.37.2,>=1.37.0 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
            "  Downloading botocore-1.37.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.4.3)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.6.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.37.2,>=1.37.0->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
            "Downloading s3fs-2025.3.2-py3-none-any.whl (30 kB)\n",
            "Downloading aiobotocore-2.21.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading botocore-1.37.1-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: jmespath, aioitertools, botocore, aiobotocore, s3fs\n",
            "Successfully installed aiobotocore-2.21.1 aioitertools-0.12.0 botocore-1.37.1 jmespath-1.0.1 s3fs-2025.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab_size(self) -> int:\n",
        "    return self.n_words\n",
        "\n",
        "def encode(\n",
        "    self, text: str, add_bos: bool | None = None, add_eos: bool | None = None\n",
        "):\n",
        "    if add_bos is None:\n",
        "        add_bos = self.add_bos\n",
        "    if add_eos is None:\n",
        "        add_eos = self.add_eos\n",
        "\n",
        "    if self.bpe_delim:\n",
        "        tokens = text2bytes_bpe_delims(\n",
        "            text,\n",
        "            bpe_tokenizer=self.bpe_tokenizer,\n",
        "            bpe_id=self.bpe_id,\n",
        "            offsetting_special_char=self.offsetting_special_char,\n",
        "            add_bos=False,\n",
        "            add_eos=False,\n",
        "        )\n",
        "    else:\n",
        "        tokens = bytes(text, encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    # Offsetting\n",
        "    tokens = [int(unit) + self.offsetting_special_char for unit in tokens]\n",
        "\n",
        "    if add_bos:\n",
        "        tokens.insert(0, self.bos_id)\n",
        "    if add_eos:\n",
        "        tokens.append(self.eos_id)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def decode(self, tokens: list[int], cut_at_eos: bool = False):\n",
        "    if cut_at_eos:\n",
        "        for k, t in enumerate(tokens):\n",
        "            if t == self.eos_id:\n",
        "                tokens = tokens[: k + 1]\n",
        "                break\n",
        "    return bytes(\n",
        "        [\n",
        "            tok - self.offsetting_special_char\n",
        "            for tok in tokens\n",
        "            if tok - self.offsetting_special_char >= 0\n",
        "        ]\n",
        "    ).decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "def get_token_offsets(self, text: str, tokens: list[int] | None = None):\n",
        "    # TODO: Figure out what this does\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "hGMn0x1aJ_gL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Script ---\n",
        "import torch\n",
        "import os\n",
        "from safetensors.torch import load_file\n",
        "import warnings\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# Suppress specific warnings if needed (optional)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "# --- Attempt to import from bytelatent library AFTER reinstalling xformers ---\n",
        "try:\n",
        "    from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "    from bytelatent.generate_blt import generate_nocache\n",
        "    from bytelatent.data.patcher import PatcherArgs # Needed for patcher\n",
        "    print(\"Successfully imported from bytelatent library.\")\n",
        "    imports_successful = True\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing from bytelatent: {e}\")\n",
        "    print(\"Please ensure you have cloned the 'blt' repository and installed it correctly:\")\n",
        "    print(\"1. !git clone https://github.com/facebookresearch/blt.git\")\n",
        "    print(\"2. %cd blt\")\n",
        "    print(\"3. !pip install -e .\")\n",
        "    print(\"4. Also ensure dependencies are installed: !pip install omegaconf s3fs xformers\")\n",
        "    imports_successful = False\n",
        "except AttributeError as e: # Catch the specific error\n",
        "     print(f\"AttributeError during import: {e}\")\n",
        "     print(\"This often indicates an issue with the xformers installation or version compatibility.\")\n",
        "     print(\"Ensure PyTorch and xformers versions are compatible as required by bytelatent.\")\n",
        "     imports_successful = False\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during import: {e}\")\n",
        "    traceback.print_exc()\n",
        "    imports_successful = False\n",
        "\n",
        "# Proceed only if imports were successful\n",
        "if imports_successful:\n",
        "    # --- Configuration ---\n",
        "    checkpoint_dir = \"/content/safetensors/blt_1b\" # Directory containing the main model file\n",
        "    entropy_model_dir = \"/content/safetensors/entropy_model\" # Direct path to the entropy model folder\n",
        "\n",
        "    # Verify paths\n",
        "    print(f\"\\nChecking main model directory: {checkpoint_dir}\")\n",
        "    main_model_file = os.path.join(checkpoint_dir, \"consolidated.safetensors\")\n",
        "    if not os.path.exists(main_model_file):\n",
        "        print(f\"ERROR: Main model file not found: {main_model_file}\")\n",
        "        exit()\n",
        "    else:\n",
        "        print(\"✓ Main model file found.\")\n",
        "\n",
        "    print(f\"Checking entropy model directory: {entropy_model_dir}\")\n",
        "    entropy_model_file = os.path.join(entropy_model_dir, \"consolidated.safetensors\")\n",
        "    if not os.path.exists(entropy_model_file):\n",
        "        print(f\"ERROR: Entropy model file not found: {entropy_model_file}\")\n",
        "        print(\"Warning: Proceeding without entropy model patching.\")\n",
        "        use_entropy_model = False\n",
        "    else:\n",
        "        print(\"✓ Entropy model file found.\")\n",
        "        use_entropy_model = True\n",
        "\n",
        "    # --- Load Model and Tokenizer using the Library Function ---\n",
        "    print(\"\\nLoading model, tokenizer, and config using bytelatent library...\")\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    train_cfg = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device early\n",
        "    try:\n",
        "        # This function should handle loading the config, model, tokenizer, and weights\n",
        "        model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(checkpoint_dir)\n",
        "        print(\"✓ Model, tokenizer, and config loaded successfully.\")\n",
        "\n",
        "        # Move model to appropriate device\n",
        "        model.to(device)\n",
        "        model.eval() # Set to evaluation mode\n",
        "        print(f\"Model moved to device: {device}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "         print(f\"ERROR loading model: {e}\")\n",
        "         print(f\"Could not find a configuration file (e.g., config.yaml, train_config.yaml) in {checkpoint_dir}.\")\n",
        "         print(\"Please download the necessary config file from the ttj/blt Hugging Face repository and place it in that directory.\")\n",
        "         exit()\n",
        "    except Exception as e:\n",
        "         print(f\"An unexpected error occurred during model loading: {e}\")\n",
        "         traceback.print_exc()\n",
        "         exit()\n",
        "\n",
        "    # --- Build the Patcher ---\n",
        "    print(\"\\nBuilding patcher...\")\n",
        "    patcher = None # Initialize patcher to None\n",
        "    if train_cfg is not None: # Proceed only if config was loaded\n",
        "        try:\n",
        "            # Check if patcher config exists\n",
        "            if hasattr(train_cfg, 'data') and hasattr(train_cfg.data, 'patcher_args'):\n",
        "                patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "                patcher_args.realtime_patching = True # Use realtime entropy patching\n",
        "\n",
        "                if use_entropy_model:\n",
        "                    patcher_args.entropy_model_checkpoint_dir = entropy_model_dir\n",
        "                    print(f\"Patcher will use entropy model from: {entropy_model_dir}\")\n",
        "                else:\n",
        "                     patcher_args.entropy_model_checkpoint_dir = None\n",
        "                     print(\"Patcher will NOT use entropy model.\")\n",
        "\n",
        "                patcher = patcher_args.build()\n",
        "                print(\"✓ Patcher built successfully.\")\n",
        "\n",
        "                # If using entropy model, move its sub-model to the device too\n",
        "                if use_entropy_model and hasattr(patcher, 'entropy_model') and patcher.entropy_model is not None:\n",
        "                    patcher.entropy_model.to(device)\n",
        "                    patcher.entropy_model.eval()\n",
        "                    print(\"Entropy model moved to device.\")\n",
        "            else:\n",
        "                print(\"Patcher configuration not found in train_cfg.data.patcher_args. Skipping patcher.\")\n",
        "\n",
        "        except AttributeError as e:\n",
        "            print(f\"Error accessing patcher configuration: {e}\")\n",
        "            print(\"The loaded 'train_cfg' might not have the expected structure.\")\n",
        "            print(\"Skipping patcher.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during patcher building: {e}\")\n",
        "            traceback.print_exc()\n",
        "            print(\"Skipping patcher.\")\n",
        "    else:\n",
        "        print(\"train_cfg not loaded, cannot build patcher.\")\n",
        "\n",
        "\n",
        "    # --- Generate Text ---\n",
        "    print(\"\\nStarting generation...\")\n",
        "    prompt = \"hi\" # Your simple prompt\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "\n",
        "    # The generate_nocache function likely expects a list of prompts\n",
        "    prompts = [prompt]\n",
        "\n",
        "    if model is not None and tokenizer is not None: # Proceed only if model/tokenizer loaded\n",
        "        try:\n",
        "            outputs = generate_nocache(\n",
        "                prompts,\n",
        "                model=model,\n",
        "                tokenizer=tokenizer, # Pass the loaded tokenizer\n",
        "                patcher=patcher,     # Pass the patcher (can be None)\n",
        "                max_new_tokens=30,   # Generate fewer tokens for testing\n",
        "                temperature=0.7,\n",
        "                top_k=40,\n",
        "                device=device        # Specify the device\n",
        "            )\n",
        "            print(\"✓ Generation complete.\")\n",
        "\n",
        "            # --- Decode and Print ---\n",
        "            print(\"\\nGenerated Output:\")\n",
        "            for i, out_ids in enumerate(outputs):\n",
        "                # The output might be on GPU, move to CPU for decoding if needed\n",
        "                out_ids_cpu = out_ids.cpu()\n",
        "                # Use the tokenizer's decode method\n",
        "                decoded_text = tokenizer.decode(out_ids_cpu)\n",
        "                print(f\"Output {i+1}: {decoded_text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during generation: {e}\")\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"Model or tokenizer not loaded, cannot generate.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nExiting due to import errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhyGEwU5KJoE",
        "outputId": "c2fcbc3f-bb2e-4e0f-c204-16ef37e2146a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AttributeError during import: '_OpNamespace' 'xformers' object has no attribute 'efficient_attention_forward_cutlass'\n",
            "This often indicates an issue with the xformers installation or version compatibility.\n",
            "Ensure PyTorch and xformers versions are compatible as required by bytelatent.\n",
            "\n",
            "Exiting due to import errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall xformers -y --quiet"
      ],
      "metadata": {
        "id": "bZUlh8f_Kc4p"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers==0.0.29.post3 --quiet"
      ],
      "metadata": {
        "id": "_1uwTg5gKnmc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ny64J4mBMWjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
      ],
      "metadata": {
        "id": "6UgfSGtAMWdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RyGjhMmM9j_",
        "outputId": "3aa9b0ae-0b23-491b-b2d7-8003b35f7e7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# Load BLT-1B model\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "\n",
        "# Load model directly\n",
        "\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model_weights.to(device)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "#model_weights.eval()\n",
        "\n",
        "# Define a prompt\n",
        "prompt = \"Write a short story about a robot learning to paint.\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate text\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=200,  # Maximum length of generated text\n",
        "    num_return_sequences=1,  # Number of sequences to generate\n",
        "    temperature=0.7,  # Controls randomness (lower = more deterministic)\n",
        "    top_p=0.9,  # Nucleus sampling\n",
        "    do_sample=True  # Enable sampling for diverse outputs\n",
        ")\n",
        "\n",
        "# Decode the generated tokens to text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the result\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "b0ae4939977341399a76fa780f7e63fb",
            "0a30b25786814e68a7c51913b9902e3e",
            "bc2e466a076f417f860c43f76b4d7f26",
            "3d27d9ddb3d848d48fc2cbc303eb15a0",
            "9c084413c9b748d08228e0237ac1b778",
            "f7dc2e6de0ab4052b29882bb28b6785c",
            "c9f77932a9464f93b4c98fcbc90d3476",
            "7490c67cb55f4195a9803f90dd48b0ca",
            "ab7d7cd06c0a4e13ba0de7fdca5983b2",
            "2daceb510b7b4acc80cbbbd96c399589",
            "5bef16207e064604b746295514cb6cbd",
            "25871037f82a40deb0f3027763dea018",
            "36b71313f9bb49d7928782f966458998",
            "a2c901403f7a4b5b80847e6a9361ab87",
            "b22b081e62a94f68942a75f51e33ac67",
            "5ede96c8b82a4792b80e4cd086d7ceaf",
            "4ada8aef79fd49f7acf2665bef60dc5d",
            "0eb4b17f1b94483faabdb27d593d5a08",
            "7d0d3e3871254c9192f759884800f82c",
            "ca81f1edf526409caa8765aed4afeecd",
            "11b294389b464d3b84f43f0f1db244f0",
            "c259b6bf45c640a1a913af9c18d75d68",
            "4c82b89744e24e28826cf4f30948b240",
            "c1f8544698634c57aab76209e64b6bcf",
            "66122e84e67f4edfbd61384c7eee3e0c",
            "e2fa66e2621a40f8a41ba95cfe7b807a",
            "afe03352a56e4d8f96f0d1fc3a1bae58",
            "1511121b50d346898499244ed54ae996",
            "2237752bf5ff46fb862042bfb6eb14cb",
            "9ef5541fed014f66882c76948e37c619",
            "b6594e7d234e48adb293945588558c26",
            "0a6cb283eb6d4877ad950f9c3441a30f",
            "dfbbec7018ed47388bb9e464dc318a91"
          ]
        },
        "id": "LbaJDxv9MWgY",
        "outputId": "415eeda4-9784-4f48-865e-8377995729a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0ae4939977341399a76fa780f7e63fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25871037f82a40deb0f3027763dea018"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c82b89744e24e28826cf4f30948b240"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "# 1. قم بتحميل هيكل النموذج المناسب (هنا نستخدم نموذج مشابه لـ LLaMA كمثال)\n",
        "# يجب أن يكون هذا مطابقاً لهيكل النموذج الذي تم حفظ أوزانه\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # أو أي نموذج مشابه لهيكل BLT-1B\n",
        "\n",
        "# 2. تحميل التكوين (config) وتعديله إذا لزم الأمر\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "# 3. إنشاء النموذج الفارغ\n",
        "model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "# 4. تحميل الأوزان من ملف safetensors\n",
        "model_weights = load_file('safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# 5. تحميل الأوزان إلى النموذج\n",
        "missing_keys, unexpected_keys = model.load_state_dict(model_weights, strict=False)\n",
        "print(f\"المفاتيح المفقودة: {missing_keys}\")\n",
        "print(f\"المفاتيح غير المتوقعة: {unexpected_keys}\")\n",
        "\n",
        "# 6. تحميل tokenizer (يفضل استخدام الـ tokenizer الخاص بالنموذج الأصلي)\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"path_to_your_tokenizer\")  # إذا كان لديك tokenizer مخصص\n",
        "except:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)  # استخدام tokenizer افتراضي\n",
        "\n",
        "# 7. نقل النموذج إلى GPU إذا كان متاحاً\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 8. توليد النص\n",
        "prompt = \"Write a short story about a robot learning to paint.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=200,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2dba159d4bee482abb85df06543ab9e5",
            "d244f4a618d0467ea9e5f2c08f3a7a21",
            "42019ee54418479bbac6afaed04c3574",
            "af2c989ba8f74ff7a14e3d61be626a3a",
            "4ae5055c3bb7482195319c32b4528d2a",
            "a0b525755b17445bb66bd47692f986a5",
            "e4fdee063c8c4af6a5f864faa1778ad3",
            "6b69384152dd4be997aec9799348ec46",
            "2af5c08f6b08419994bc612258b1e90e",
            "5e5f5ca15fec4d24853943af5e953d90",
            "6c91e96b5dd5407db32582cf663c79e2"
          ]
        },
        "id": "ZQjlKOMqNxmB",
        "outputId": "3d31ea5a-e0a4-4a5a-fbd8-6bfadb175822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2dba159d4bee482abb85df06543ab9e5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BltTokenizer(bpe_tokenizer_path=\"path/to/llama_v2.tokenizer.model\")"
      ],
      "metadata": {
        "id": "qa34aV5WO1ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# لعرض المفاتيح غير المتطابقة\n",
        "missing, unexpected = model.load_state_dict(model_weights, strict=False)\n",
        "print(\"Missing keys:\", missing)\n",
        "print(\"Unexpected keys:\", unexpected)"
      ],
      "metadata": {
        "id": "FcaU7fF7O3mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "from blt_tokenizer import BltTokenizer  # تأكد من أن المسار صحيح\n",
        "\n",
        "# 1. تهيئة الـ Tokenizer\n",
        "tokenizer = BltTokenizer(\n",
        "    vocab_size_unit_1=256,  # BYTE_UNITS افتراضياً 256\n",
        "    bpe_delim=False,       # هل تستخدم محددات BPE؟\n",
        "    bpe_tokenizer_path=\"/content/my_tokenizer\",  # تحديث المسار\n",
        "    add_bos=True,          # إضافة Beginning Of Sequence token\n",
        "    add_eos=True           # إضافة End Of Sequence token\n",
        ")\n",
        "\n",
        "# 2. تحميل هيكل النموذج (تعديل الإعدادات حسب مواصفات BLT-1B)\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",  # أو أي نموذج قريب من هيكل BLT-1B\n",
        "    vocab_size=tokenizer.vocab_size,  # تأكد من تطابق حجم المفردات\n",
        "    hidden_size=4096,        # مثال - تعديل حسب مواصفات BLT-1B\n",
        "    num_attention_heads=32,  # مثال - تعديل حسب المواصفات\n",
        "    num_hidden_layers=32     # مثال - تعديل حسب المواصفات\n",
        ")\n",
        "\n",
        "# 3. إنشاء النموذج\n",
        "model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "# 4. تحميل الأوزان\n",
        "model_weights = load_file('safetensors/blt_1b/consolidated.safetensors')\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# 5. إعداد الجهاز\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 6. توليد النص\n",
        "prompt = \"Write a short story about a robot learning to paint.\"\n",
        "\n",
        "# ترميز النص مع معالجة خاصة للبايتات إذا لزم الأمر\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=200,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,  # إذا كان موجوداً\n",
        "        eos_token_id=tokenizer.eos_token_id,  # إذا كان موجوداً\n",
        "        bos_token_id=tokenizer.bos_token_id   # إذا كان موجوداً\n",
        "    )\n",
        "\n",
        "# فك الترميز\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "hVCWpTMIObN9",
        "outputId": "0e44a323-e9ab-4ea4-f6de-0a1ec27ba171"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'blt_tokenizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2539b6f57ca1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msafetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mblt_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBltTokenizer\u001b[0m  \u001b[0;31m# تأكد من أن المسار صحيح\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 1. تهيئة الـ Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'blt_tokenizer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "source": [
        "!pip install transformers"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLb5SF2RPFVv",
        "outputId": "91214264-ecd8-4767-f59d-a8fa5f4c28a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "   # تحديد اسم النموذج المطلوب\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "   # تحميل التوكنزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "   # تحديد مسار المجلد المطلوب لحفظ التوكنزر\n",
        "save_directory = \"/content/my_tokenizer\"\n",
        "\n",
        "   # حفظ التوكنزر في المجلد المحدد\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356,
          "referenced_widgets": [
            "3c2b29af0bbe406abd1154c7ba696d7c",
            "0619c282554043a69c89c72cd13e5bf2",
            "7d28cc00f4a34d1c9e750cee37240cc3",
            "190d722850f04a06af016d1237c6d5be",
            "d0e0673cac9c471b8bbbc9c53694733c",
            "26c1091978424bc2b48007a7b2c131d4",
            "c5a2f0cd42ef4cbb9c39bf526f91fcf3",
            "7046a9e3e64445c084a6d3f03ca3be4e",
            "b088c755560643148e921906c2879902",
            "3a79df47eef04296bb64547053a76e08",
            "2e95bd4b76244acc9ed63ed6de3a5d01",
            "1bff724cf473448b8d0f188ee1e6b852",
            "7f8f336c4a5545c19d633c6c093647c3",
            "3d2313bb7228465f959e6f7eb1705e7c",
            "57bd868ae47c4012861169ec3756ae49",
            "0455e2474ff749ea903cca13831e883c",
            "37263b3ca8a94edead998064de9106f1",
            "5163f5e16dea490098fb2979c008b1d2",
            "b4dd2fb1b6ec4598912391c015ec4d42",
            "2c8088be0927470ab9ba0034527e0d0b",
            "0a8de725dce54d7fbfb71fc3c99426fa",
            "9f9c645349b449c09d51bc5e3f548a36",
            "74f59c85892e448cb381be50fbb7c69a",
            "6ffb6aeb32e1495d8f83f64095b8e3e4",
            "2f447a5b1d2c48cf8bddf2df46fef2c3",
            "2509783a69ca448eaccda5088ba70c7a",
            "7ef18a76ab9348f5999832bb7c861969",
            "c4b15c95036c430b949b176a83a0a8d6",
            "15108c870974487a9b5613eafd45f12b",
            "b082c57e41ee4c35b9e2cb23912ebb9a",
            "61a7f15089d047dd93a982aa5fb82576",
            "2e7f781dca7745f3a7bc0f95c5c9cbbc",
            "025cacee72624352b391e046ab01f6b2",
            "6a709be84d3541708f45859a4f1067fc",
            "ecf8518fc7424dba9f260c261064ebab",
            "cf2ec075a4e54093af6a942022ec1fc8",
            "d5be8d7aa1004c6f9ab60513a26aed97",
            "ca4b6c447aa84aedabcafcf0159195c2",
            "59bcfee2cda048759fcb152238a01f9a",
            "bded4ea3b8754bbe849df94a8caea119",
            "ebc493be87534b22bdb050739571bf54",
            "c6ab8ff16e844d059710a64c76e35858",
            "cbc37aa39f92408dbcca6df34d497fb9",
            "d1fe2a3aee2a429c9698d301ac4293ad"
          ]
        },
        "id": "q962vm1EPFwY",
        "outputId": "6ab13938-3dfe-48fb-eff6-746a4f598235"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c2b29af0bbe406abd1154c7ba696d7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bff724cf473448b8d0f188ee1e6b852"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74f59c85892e448cb381be50fbb7c69a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a709be84d3541708f45859a4f1067fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/my_tokenizer/tokenizer_config.json',\n",
              " '/content/my_tokenizer/special_tokens_map.json',\n",
              " '/content/my_tokenizer/tokenizer.model',\n",
              " '/content/my_tokenizer/added_tokens.json',\n",
              " '/content/my_tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "# 1. قم بتحميل هيكل النموذج المناسب (هنا نستخدم نموذج مشابه لـ LLaMA كمثال)\n",
        "# يجب أن يكون هذا مطابقاً لهيكل النموذج الذي تم حفظ أوزانه\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # أو أي نموذج مشابه لهيكل BLT-1B\n",
        "\n",
        "# 2. تحميل التكوين (config) وتعديله إذا لزم الأمر\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "# 3. إنشاء النموذج الفارغ\n",
        "model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "# 4. تحميل الأوزان من ملف safetensors\n",
        "model_weights = load_file('safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# 5. تحميل الأوزان إلى النموذج\n",
        "missing_keys, unexpected_keys = model.load_state_dict(model_weights, strict=False)\n",
        "print(f\"المفاتيح المفقودة: {missing_keys}\")\n",
        "print(f\"المفاتيح غير المتوقعة: {unexpected_keys}\")\n",
        "\n",
        "# 6. تحميل tokenizer (يفضل استخدام الـ tokenizer الخاص بالنموذج الأصلي)\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"/content/my_tokenizer\")  # إذا كان لديك tokenizer مخصص\n",
        "except:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)  # استخدام tokenizer افتراضي\n",
        "\n",
        "# 7. نقل النموذج إلى GPU إذا كان متاحاً\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 8. توليد النص\n",
        "prompt = \"Write a short story about a robot learning to paint.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=200,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg0JElrQONmN",
        "outputId": "ede3775d-bfd7-462d-b3fc-4223f4f7566c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KtaYI4EBQDrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHczxmxcZUVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bl3bV1HYZUSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yqgVoQlzZUQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8BQEwfcfZUMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "import torch\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():  # Disable gradient calculation during inference\n",
        "    output = model.generate(input_ids, max_new_tokens=50)  # Adjust max_new_tokens as needed\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8A3mIh2ZUIi",
        "outputId": "338f0fac-4590-4b06-ce3f-fd2b03223a78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using a model of type opt to instantiate a model of type bloom. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of BloomForCausalLM were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['lm_head.weight', 'transformer.h.0.input_layernorm.bias', 'transformer.h.0.input_layernorm.weight', 'transformer.h.0.mlp.dense_4h_to_h.bias', 'transformer.h.0.mlp.dense_4h_to_h.weight', 'transformer.h.0.mlp.dense_h_to_4h.bias', 'transformer.h.0.mlp.dense_h_to_4h.weight', 'transformer.h.0.post_attention_layernorm.bias', 'transformer.h.0.post_attention_layernorm.weight', 'transformer.h.0.self_attention.dense.bias', 'transformer.h.0.self_attention.dense.weight', 'transformer.h.0.self_attention.query_key_value.bias', 'transformer.h.0.self_attention.query_key_value.weight', 'transformer.h.1.input_layernorm.bias', 'transformer.h.1.input_layernorm.weight', 'transformer.h.1.mlp.dense_4h_to_h.bias', 'transformer.h.1.mlp.dense_4h_to_h.weight', 'transformer.h.1.mlp.dense_h_to_4h.bias', 'transformer.h.1.mlp.dense_h_to_4h.weight', 'transformer.h.1.post_attention_layernorm.bias', 'transformer.h.1.post_attention_layernorm.weight', 'transformer.h.1.self_attention.dense.bias', 'transformer.h.1.self_attention.dense.weight', 'transformer.h.1.self_attention.query_key_value.bias', 'transformer.h.1.self_attention.query_key_value.weight', 'transformer.h.10.input_layernorm.bias', 'transformer.h.10.input_layernorm.weight', 'transformer.h.10.mlp.dense_4h_to_h.bias', 'transformer.h.10.mlp.dense_4h_to_h.weight', 'transformer.h.10.mlp.dense_h_to_4h.bias', 'transformer.h.10.mlp.dense_h_to_4h.weight', 'transformer.h.10.post_attention_layernorm.bias', 'transformer.h.10.post_attention_layernorm.weight', 'transformer.h.10.self_attention.dense.bias', 'transformer.h.10.self_attention.dense.weight', 'transformer.h.10.self_attention.query_key_value.bias', 'transformer.h.10.self_attention.query_key_value.weight', 'transformer.h.11.input_layernorm.bias', 'transformer.h.11.input_layernorm.weight', 'transformer.h.11.mlp.dense_4h_to_h.bias', 'transformer.h.11.mlp.dense_4h_to_h.weight', 'transformer.h.11.mlp.dense_h_to_4h.bias', 'transformer.h.11.mlp.dense_h_to_4h.weight', 'transformer.h.11.post_attention_layernorm.bias', 'transformer.h.11.post_attention_layernorm.weight', 'transformer.h.11.self_attention.dense.bias', 'transformer.h.11.self_attention.dense.weight', 'transformer.h.11.self_attention.query_key_value.bias', 'transformer.h.11.self_attention.query_key_value.weight', 'transformer.h.12.input_layernorm.bias', 'transformer.h.12.input_layernorm.weight', 'transformer.h.12.mlp.dense_4h_to_h.bias', 'transformer.h.12.mlp.dense_4h_to_h.weight', 'transformer.h.12.mlp.dense_h_to_4h.bias', 'transformer.h.12.mlp.dense_h_to_4h.weight', 'transformer.h.12.post_attention_layernorm.bias', 'transformer.h.12.post_attention_layernorm.weight', 'transformer.h.12.self_attention.dense.bias', 'transformer.h.12.self_attention.dense.weight', 'transformer.h.12.self_attention.query_key_value.bias', 'transformer.h.12.self_attention.query_key_value.weight', 'transformer.h.13.input_layernorm.bias', 'transformer.h.13.input_layernorm.weight', 'transformer.h.13.mlp.dense_4h_to_h.bias', 'transformer.h.13.mlp.dense_4h_to_h.weight', 'transformer.h.13.mlp.dense_h_to_4h.bias', 'transformer.h.13.mlp.dense_h_to_4h.weight', 'transformer.h.13.post_attention_layernorm.bias', 'transformer.h.13.post_attention_layernorm.weight', 'transformer.h.13.self_attention.dense.bias', 'transformer.h.13.self_attention.dense.weight', 'transformer.h.13.self_attention.query_key_value.bias', 'transformer.h.13.self_attention.query_key_value.weight', 'transformer.h.14.input_layernorm.bias', 'transformer.h.14.input_layernorm.weight', 'transformer.h.14.mlp.dense_4h_to_h.bias', 'transformer.h.14.mlp.dense_4h_to_h.weight', 'transformer.h.14.mlp.dense_h_to_4h.bias', 'transformer.h.14.mlp.dense_h_to_4h.weight', 'transformer.h.14.post_attention_layernorm.bias', 'transformer.h.14.post_attention_layernorm.weight', 'transformer.h.14.self_attention.dense.bias', 'transformer.h.14.self_attention.dense.weight', 'transformer.h.14.self_attention.query_key_value.bias', 'transformer.h.14.self_attention.query_key_value.weight', 'transformer.h.15.input_layernorm.bias', 'transformer.h.15.input_layernorm.weight', 'transformer.h.15.mlp.dense_4h_to_h.bias', 'transformer.h.15.mlp.dense_4h_to_h.weight', 'transformer.h.15.mlp.dense_h_to_4h.bias', 'transformer.h.15.mlp.dense_h_to_4h.weight', 'transformer.h.15.post_attention_layernorm.bias', 'transformer.h.15.post_attention_layernorm.weight', 'transformer.h.15.self_attention.dense.bias', 'transformer.h.15.self_attention.dense.weight', 'transformer.h.15.self_attention.query_key_value.bias', 'transformer.h.15.self_attention.query_key_value.weight', 'transformer.h.16.input_layernorm.bias', 'transformer.h.16.input_layernorm.weight', 'transformer.h.16.mlp.dense_4h_to_h.bias', 'transformer.h.16.mlp.dense_4h_to_h.weight', 'transformer.h.16.mlp.dense_h_to_4h.bias', 'transformer.h.16.mlp.dense_h_to_4h.weight', 'transformer.h.16.post_attention_layernorm.bias', 'transformer.h.16.post_attention_layernorm.weight', 'transformer.h.16.self_attention.dense.bias', 'transformer.h.16.self_attention.dense.weight', 'transformer.h.16.self_attention.query_key_value.bias', 'transformer.h.16.self_attention.query_key_value.weight', 'transformer.h.17.input_layernorm.bias', 'transformer.h.17.input_layernorm.weight', 'transformer.h.17.mlp.dense_4h_to_h.bias', 'transformer.h.17.mlp.dense_4h_to_h.weight', 'transformer.h.17.mlp.dense_h_to_4h.bias', 'transformer.h.17.mlp.dense_h_to_4h.weight', 'transformer.h.17.post_attention_layernorm.bias', 'transformer.h.17.post_attention_layernorm.weight', 'transformer.h.17.self_attention.dense.bias', 'transformer.h.17.self_attention.dense.weight', 'transformer.h.17.self_attention.query_key_value.bias', 'transformer.h.17.self_attention.query_key_value.weight', 'transformer.h.18.input_layernorm.bias', 'transformer.h.18.input_layernorm.weight', 'transformer.h.18.mlp.dense_4h_to_h.bias', 'transformer.h.18.mlp.dense_4h_to_h.weight', 'transformer.h.18.mlp.dense_h_to_4h.bias', 'transformer.h.18.mlp.dense_h_to_4h.weight', 'transformer.h.18.post_attention_layernorm.bias', 'transformer.h.18.post_attention_layernorm.weight', 'transformer.h.18.self_attention.dense.bias', 'transformer.h.18.self_attention.dense.weight', 'transformer.h.18.self_attention.query_key_value.bias', 'transformer.h.18.self_attention.query_key_value.weight', 'transformer.h.19.input_layernorm.bias', 'transformer.h.19.input_layernorm.weight', 'transformer.h.19.mlp.dense_4h_to_h.bias', 'transformer.h.19.mlp.dense_4h_to_h.weight', 'transformer.h.19.mlp.dense_h_to_4h.bias', 'transformer.h.19.mlp.dense_h_to_4h.weight', 'transformer.h.19.post_attention_layernorm.bias', 'transformer.h.19.post_attention_layernorm.weight', 'transformer.h.19.self_attention.dense.bias', 'transformer.h.19.self_attention.dense.weight', 'transformer.h.19.self_attention.query_key_value.bias', 'transformer.h.19.self_attention.query_key_value.weight', 'transformer.h.2.input_layernorm.bias', 'transformer.h.2.input_layernorm.weight', 'transformer.h.2.mlp.dense_4h_to_h.bias', 'transformer.h.2.mlp.dense_4h_to_h.weight', 'transformer.h.2.mlp.dense_h_to_4h.bias', 'transformer.h.2.mlp.dense_h_to_4h.weight', 'transformer.h.2.post_attention_layernorm.bias', 'transformer.h.2.post_attention_layernorm.weight', 'transformer.h.2.self_attention.dense.bias', 'transformer.h.2.self_attention.dense.weight', 'transformer.h.2.self_attention.query_key_value.bias', 'transformer.h.2.self_attention.query_key_value.weight', 'transformer.h.20.input_layernorm.bias', 'transformer.h.20.input_layernorm.weight', 'transformer.h.20.mlp.dense_4h_to_h.bias', 'transformer.h.20.mlp.dense_4h_to_h.weight', 'transformer.h.20.mlp.dense_h_to_4h.bias', 'transformer.h.20.mlp.dense_h_to_4h.weight', 'transformer.h.20.post_attention_layernorm.bias', 'transformer.h.20.post_attention_layernorm.weight', 'transformer.h.20.self_attention.dense.bias', 'transformer.h.20.self_attention.dense.weight', 'transformer.h.20.self_attention.query_key_value.bias', 'transformer.h.20.self_attention.query_key_value.weight', 'transformer.h.21.input_layernorm.bias', 'transformer.h.21.input_layernorm.weight', 'transformer.h.21.mlp.dense_4h_to_h.bias', 'transformer.h.21.mlp.dense_4h_to_h.weight', 'transformer.h.21.mlp.dense_h_to_4h.bias', 'transformer.h.21.mlp.dense_h_to_4h.weight', 'transformer.h.21.post_attention_layernorm.bias', 'transformer.h.21.post_attention_layernorm.weight', 'transformer.h.21.self_attention.dense.bias', 'transformer.h.21.self_attention.dense.weight', 'transformer.h.21.self_attention.query_key_value.bias', 'transformer.h.21.self_attention.query_key_value.weight', 'transformer.h.22.input_layernorm.bias', 'transformer.h.22.input_layernorm.weight', 'transformer.h.22.mlp.dense_4h_to_h.bias', 'transformer.h.22.mlp.dense_4h_to_h.weight', 'transformer.h.22.mlp.dense_h_to_4h.bias', 'transformer.h.22.mlp.dense_h_to_4h.weight', 'transformer.h.22.post_attention_layernorm.bias', 'transformer.h.22.post_attention_layernorm.weight', 'transformer.h.22.self_attention.dense.bias', 'transformer.h.22.self_attention.dense.weight', 'transformer.h.22.self_attention.query_key_value.bias', 'transformer.h.22.self_attention.query_key_value.weight', 'transformer.h.23.input_layernorm.bias', 'transformer.h.23.input_layernorm.weight', 'transformer.h.23.mlp.dense_4h_to_h.bias', 'transformer.h.23.mlp.dense_4h_to_h.weight', 'transformer.h.23.mlp.dense_h_to_4h.bias', 'transformer.h.23.mlp.dense_h_to_4h.weight', 'transformer.h.23.post_attention_layernorm.bias', 'transformer.h.23.post_attention_layernorm.weight', 'transformer.h.23.self_attention.dense.bias', 'transformer.h.23.self_attention.dense.weight', 'transformer.h.23.self_attention.query_key_value.bias', 'transformer.h.23.self_attention.query_key_value.weight', 'transformer.h.3.input_layernorm.bias', 'transformer.h.3.input_layernorm.weight', 'transformer.h.3.mlp.dense_4h_to_h.bias', 'transformer.h.3.mlp.dense_4h_to_h.weight', 'transformer.h.3.mlp.dense_h_to_4h.bias', 'transformer.h.3.mlp.dense_h_to_4h.weight', 'transformer.h.3.post_attention_layernorm.bias', 'transformer.h.3.post_attention_layernorm.weight', 'transformer.h.3.self_attention.dense.bias', 'transformer.h.3.self_attention.dense.weight', 'transformer.h.3.self_attention.query_key_value.bias', 'transformer.h.3.self_attention.query_key_value.weight', 'transformer.h.4.input_layernorm.bias', 'transformer.h.4.input_layernorm.weight', 'transformer.h.4.mlp.dense_4h_to_h.bias', 'transformer.h.4.mlp.dense_4h_to_h.weight', 'transformer.h.4.mlp.dense_h_to_4h.bias', 'transformer.h.4.mlp.dense_h_to_4h.weight', 'transformer.h.4.post_attention_layernorm.bias', 'transformer.h.4.post_attention_layernorm.weight', 'transformer.h.4.self_attention.dense.bias', 'transformer.h.4.self_attention.dense.weight', 'transformer.h.4.self_attention.query_key_value.bias', 'transformer.h.4.self_attention.query_key_value.weight', 'transformer.h.5.input_layernorm.bias', 'transformer.h.5.input_layernorm.weight', 'transformer.h.5.mlp.dense_4h_to_h.bias', 'transformer.h.5.mlp.dense_4h_to_h.weight', 'transformer.h.5.mlp.dense_h_to_4h.bias', 'transformer.h.5.mlp.dense_h_to_4h.weight', 'transformer.h.5.post_attention_layernorm.bias', 'transformer.h.5.post_attention_layernorm.weight', 'transformer.h.5.self_attention.dense.bias', 'transformer.h.5.self_attention.dense.weight', 'transformer.h.5.self_attention.query_key_value.bias', 'transformer.h.5.self_attention.query_key_value.weight', 'transformer.h.6.input_layernorm.bias', 'transformer.h.6.input_layernorm.weight', 'transformer.h.6.mlp.dense_4h_to_h.bias', 'transformer.h.6.mlp.dense_4h_to_h.weight', 'transformer.h.6.mlp.dense_h_to_4h.bias', 'transformer.h.6.mlp.dense_h_to_4h.weight', 'transformer.h.6.post_attention_layernorm.bias', 'transformer.h.6.post_attention_layernorm.weight', 'transformer.h.6.self_attention.dense.bias', 'transformer.h.6.self_attention.dense.weight', 'transformer.h.6.self_attention.query_key_value.bias', 'transformer.h.6.self_attention.query_key_value.weight', 'transformer.h.7.input_layernorm.bias', 'transformer.h.7.input_layernorm.weight', 'transformer.h.7.mlp.dense_4h_to_h.bias', 'transformer.h.7.mlp.dense_4h_to_h.weight', 'transformer.h.7.mlp.dense_h_to_4h.bias', 'transformer.h.7.mlp.dense_h_to_4h.weight', 'transformer.h.7.post_attention_layernorm.bias', 'transformer.h.7.post_attention_layernorm.weight', 'transformer.h.7.self_attention.dense.bias', 'transformer.h.7.self_attention.dense.weight', 'transformer.h.7.self_attention.query_key_value.bias', 'transformer.h.7.self_attention.query_key_value.weight', 'transformer.h.8.input_layernorm.bias', 'transformer.h.8.input_layernorm.weight', 'transformer.h.8.mlp.dense_4h_to_h.bias', 'transformer.h.8.mlp.dense_4h_to_h.weight', 'transformer.h.8.mlp.dense_h_to_4h.bias', 'transformer.h.8.mlp.dense_h_to_4h.weight', 'transformer.h.8.post_attention_layernorm.bias', 'transformer.h.8.post_attention_layernorm.weight', 'transformer.h.8.self_attention.dense.bias', 'transformer.h.8.self_attention.dense.weight', 'transformer.h.8.self_attention.query_key_value.bias', 'transformer.h.8.self_attention.query_key_value.weight', 'transformer.h.9.input_layernorm.bias', 'transformer.h.9.input_layernorm.weight', 'transformer.h.9.mlp.dense_4h_to_h.bias', 'transformer.h.9.mlp.dense_4h_to_h.weight', 'transformer.h.9.mlp.dense_h_to_4h.bias', 'transformer.h.9.mlp.dense_h_to_4h.weight', 'transformer.h.9.post_attention_layernorm.bias', 'transformer.h.9.post_attention_layernorm.weight', 'transformer.h.9.self_attention.dense.bias', 'transformer.h.9.self_attention.dense.weight', 'transformer.h.9.self_attention.query_key_value.bias', 'transformer.h.9.self_attention.query_key_value.weight', 'transformer.ln_f.bias', 'transformer.ln_f.weight', 'transformer.word_embeddings.weight', 'transformer.word_embeddings_layernorm.bias', 'transformer.word_embeddings_layernorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
            "The class this function is called from is 'BloomTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cu0zZjuMZdXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Example if the base model is OPT:\n",
        "from transformers import OPTForCausalLM, AutoTokenizer\n",
        "\n",
        "model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")  # Replace with the correct OPT checkpoint"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UoCoSWxnZutb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Load the weights into the correct model\n",
        "model.load_state_dict(model_weights, strict=False)  # strict=False to ignore mismatched keys"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sdmxf7zZvAl",
        "outputId": "c73092f5-ada4-476e-93b6-e967ef4117e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.project_out.weight', 'model.decoder.project_in.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.12.self_attn.k_proj.weight', 'model.decoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.v_proj.weight', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.12.self_attn.q_proj.weight', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.12.self_attn.out_proj.weight', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.12.self_attn_layer_norm.weight', 'model.decoder.layers.12.self_attn_layer_norm.bias', 'model.decoder.layers.12.fc1.weight', 'model.decoder.layers.12.fc1.bias', 'model.decoder.layers.12.fc2.weight', 'model.decoder.layers.12.fc2.bias', 'model.decoder.layers.12.final_layer_norm.weight', 'model.decoder.layers.12.final_layer_norm.bias', 'model.decoder.layers.13.self_attn.k_proj.weight', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.weight', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.weight', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.out_proj.weight', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn_layer_norm.weight', 'model.decoder.layers.13.self_attn_layer_norm.bias', 'model.decoder.layers.13.fc1.weight', 'model.decoder.layers.13.fc1.bias', 'model.decoder.layers.13.fc2.weight', 'model.decoder.layers.13.fc2.bias', 'model.decoder.layers.13.final_layer_norm.weight', 'model.decoder.layers.13.final_layer_norm.bias', 'model.decoder.layers.14.self_attn.k_proj.weight', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.weight', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.14.self_attn.q_proj.weight', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.weight', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.14.self_attn_layer_norm.weight', 'model.decoder.layers.14.self_attn_layer_norm.bias', 'model.decoder.layers.14.fc1.weight', 'model.decoder.layers.14.fc1.bias', 'model.decoder.layers.14.fc2.weight', 'model.decoder.layers.14.fc2.bias', 'model.decoder.layers.14.final_layer_norm.weight', 'model.decoder.layers.14.final_layer_norm.bias', 'model.decoder.layers.15.self_attn.k_proj.weight', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.15.self_attn.v_proj.weight', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.q_proj.weight', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.weight', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.decoder.layers.15.self_attn_layer_norm.weight', 'model.decoder.layers.15.self_attn_layer_norm.bias', 'model.decoder.layers.15.fc1.weight', 'model.decoder.layers.15.fc1.bias', 'model.decoder.layers.15.fc2.weight', 'model.decoder.layers.15.fc2.bias', 'model.decoder.layers.15.final_layer_norm.weight', 'model.decoder.layers.15.final_layer_norm.bias', 'model.decoder.layers.16.self_attn.k_proj.weight', 'model.decoder.layers.16.self_attn.k_proj.bias', 'model.decoder.layers.16.self_attn.v_proj.weight', 'model.decoder.layers.16.self_attn.v_proj.bias', 'model.decoder.layers.16.self_attn.q_proj.weight', 'model.decoder.layers.16.self_attn.q_proj.bias', 'model.decoder.layers.16.self_attn.out_proj.weight', 'model.decoder.layers.16.self_attn.out_proj.bias', 'model.decoder.layers.16.self_attn_layer_norm.weight', 'model.decoder.layers.16.self_attn_layer_norm.bias', 'model.decoder.layers.16.fc1.weight', 'model.decoder.layers.16.fc1.bias', 'model.decoder.layers.16.fc2.weight', 'model.decoder.layers.16.fc2.bias', 'model.decoder.layers.16.final_layer_norm.weight', 'model.decoder.layers.16.final_layer_norm.bias', 'model.decoder.layers.17.self_attn.k_proj.weight', 'model.decoder.layers.17.self_attn.k_proj.bias', 'model.decoder.layers.17.self_attn.v_proj.weight', 'model.decoder.layers.17.self_attn.v_proj.bias', 'model.decoder.layers.17.self_attn.q_proj.weight', 'model.decoder.layers.17.self_attn.q_proj.bias', 'model.decoder.layers.17.self_attn.out_proj.weight', 'model.decoder.layers.17.self_attn.out_proj.bias', 'model.decoder.layers.17.self_attn_layer_norm.weight', 'model.decoder.layers.17.self_attn_layer_norm.bias', 'model.decoder.layers.17.fc1.weight', 'model.decoder.layers.17.fc1.bias', 'model.decoder.layers.17.fc2.weight', 'model.decoder.layers.17.fc2.bias', 'model.decoder.layers.17.final_layer_norm.weight', 'model.decoder.layers.17.final_layer_norm.bias', 'model.decoder.layers.18.self_attn.k_proj.weight', 'model.decoder.layers.18.self_attn.k_proj.bias', 'model.decoder.layers.18.self_attn.v_proj.weight', 'model.decoder.layers.18.self_attn.v_proj.bias', 'model.decoder.layers.18.self_attn.q_proj.weight', 'model.decoder.layers.18.self_attn.q_proj.bias', 'model.decoder.layers.18.self_attn.out_proj.weight', 'model.decoder.layers.18.self_attn.out_proj.bias', 'model.decoder.layers.18.self_attn_layer_norm.weight', 'model.decoder.layers.18.self_attn_layer_norm.bias', 'model.decoder.layers.18.fc1.weight', 'model.decoder.layers.18.fc1.bias', 'model.decoder.layers.18.fc2.weight', 'model.decoder.layers.18.fc2.bias', 'model.decoder.layers.18.final_layer_norm.weight', 'model.decoder.layers.18.final_layer_norm.bias', 'model.decoder.layers.19.self_attn.k_proj.weight', 'model.decoder.layers.19.self_attn.k_proj.bias', 'model.decoder.layers.19.self_attn.v_proj.weight', 'model.decoder.layers.19.self_attn.v_proj.bias', 'model.decoder.layers.19.self_attn.q_proj.weight', 'model.decoder.layers.19.self_attn.q_proj.bias', 'model.decoder.layers.19.self_attn.out_proj.weight', 'model.decoder.layers.19.self_attn.out_proj.bias', 'model.decoder.layers.19.self_attn_layer_norm.weight', 'model.decoder.layers.19.self_attn_layer_norm.bias', 'model.decoder.layers.19.fc1.weight', 'model.decoder.layers.19.fc1.bias', 'model.decoder.layers.19.fc2.weight', 'model.decoder.layers.19.fc2.bias', 'model.decoder.layers.19.final_layer_norm.weight', 'model.decoder.layers.19.final_layer_norm.bias', 'model.decoder.layers.20.self_attn.k_proj.weight', 'model.decoder.layers.20.self_attn.k_proj.bias', 'model.decoder.layers.20.self_attn.v_proj.weight', 'model.decoder.layers.20.self_attn.v_proj.bias', 'model.decoder.layers.20.self_attn.q_proj.weight', 'model.decoder.layers.20.self_attn.q_proj.bias', 'model.decoder.layers.20.self_attn.out_proj.weight', 'model.decoder.layers.20.self_attn.out_proj.bias', 'model.decoder.layers.20.self_attn_layer_norm.weight', 'model.decoder.layers.20.self_attn_layer_norm.bias', 'model.decoder.layers.20.fc1.weight', 'model.decoder.layers.20.fc1.bias', 'model.decoder.layers.20.fc2.weight', 'model.decoder.layers.20.fc2.bias', 'model.decoder.layers.20.final_layer_norm.weight', 'model.decoder.layers.20.final_layer_norm.bias', 'model.decoder.layers.21.self_attn.k_proj.weight', 'model.decoder.layers.21.self_attn.k_proj.bias', 'model.decoder.layers.21.self_attn.v_proj.weight', 'model.decoder.layers.21.self_attn.v_proj.bias', 'model.decoder.layers.21.self_attn.q_proj.weight', 'model.decoder.layers.21.self_attn.q_proj.bias', 'model.decoder.layers.21.self_attn.out_proj.weight', 'model.decoder.layers.21.self_attn.out_proj.bias', 'model.decoder.layers.21.self_attn_layer_norm.weight', 'model.decoder.layers.21.self_attn_layer_norm.bias', 'model.decoder.layers.21.fc1.weight', 'model.decoder.layers.21.fc1.bias', 'model.decoder.layers.21.fc2.weight', 'model.decoder.layers.21.fc2.bias', 'model.decoder.layers.21.final_layer_norm.weight', 'model.decoder.layers.21.final_layer_norm.bias', 'model.decoder.layers.22.self_attn.k_proj.weight', 'model.decoder.layers.22.self_attn.k_proj.bias', 'model.decoder.layers.22.self_attn.v_proj.weight', 'model.decoder.layers.22.self_attn.v_proj.bias', 'model.decoder.layers.22.self_attn.q_proj.weight', 'model.decoder.layers.22.self_attn.q_proj.bias', 'model.decoder.layers.22.self_attn.out_proj.weight', 'model.decoder.layers.22.self_attn.out_proj.bias', 'model.decoder.layers.22.self_attn_layer_norm.weight', 'model.decoder.layers.22.self_attn_layer_norm.bias', 'model.decoder.layers.22.fc1.weight', 'model.decoder.layers.22.fc1.bias', 'model.decoder.layers.22.fc2.weight', 'model.decoder.layers.22.fc2.bias', 'model.decoder.layers.22.final_layer_norm.weight', 'model.decoder.layers.22.final_layer_norm.bias', 'model.decoder.layers.23.self_attn.k_proj.weight', 'model.decoder.layers.23.self_attn.k_proj.bias', 'model.decoder.layers.23.self_attn.v_proj.weight', 'model.decoder.layers.23.self_attn.v_proj.bias', 'model.decoder.layers.23.self_attn.q_proj.weight', 'model.decoder.layers.23.self_attn.q_proj.bias', 'model.decoder.layers.23.self_attn.out_proj.weight', 'model.decoder.layers.23.self_attn.out_proj.bias', 'model.decoder.layers.23.self_attn_layer_norm.weight', 'model.decoder.layers.23.self_attn_layer_norm.bias', 'model.decoder.layers.23.fc1.weight', 'model.decoder.layers.23.fc1.bias', 'model.decoder.layers.23.fc2.weight', 'model.decoder.layers.23.fc2.bias', 'model.decoder.layers.23.final_layer_norm.weight', 'model.decoder.layers.23.final_layer_norm.bias', 'lm_head.weight'], unexpected_keys=['encoder_hash_tok_embedding.0.weight', 'encoder_hash_tok_embedding.1.weight', 'encoder_hash_tok_embedding.2.weight', 'encoder_hash_tok_embedding.3.weight', 'encoder_hash_tok_embedding.4.weight', 'encoder_hash_tok_embedding.5.weight', 'global_transformer.layers.0.attention.wk.weight', 'global_transformer.layers.0.attention.wo.weight', 'global_transformer.layers.0.attention.wq.weight', 'global_transformer.layers.0.attention.wv.weight', 'global_transformer.layers.0.attention_norm.weight', 'global_transformer.layers.0.feed_forward.w1.weight', 'global_transformer.layers.0.feed_forward.w2.weight', 'global_transformer.layers.0.feed_forward.w3.weight', 'global_transformer.layers.0.ffn_norm.weight', 'global_transformer.layers.1.attention.wk.weight', 'global_transformer.layers.1.attention.wo.weight', 'global_transformer.layers.1.attention.wq.weight', 'global_transformer.layers.1.attention.wv.weight', 'global_transformer.layers.1.attention_norm.weight', 'global_transformer.layers.1.feed_forward.w1.weight', 'global_transformer.layers.1.feed_forward.w2.weight', 'global_transformer.layers.1.feed_forward.w3.weight', 'global_transformer.layers.1.ffn_norm.weight', 'global_transformer.layers.10.attention.wk.weight', 'global_transformer.layers.10.attention.wo.weight', 'global_transformer.layers.10.attention.wq.weight', 'global_transformer.layers.10.attention.wv.weight', 'global_transformer.layers.10.attention_norm.weight', 'global_transformer.layers.10.feed_forward.w1.weight', 'global_transformer.layers.10.feed_forward.w2.weight', 'global_transformer.layers.10.feed_forward.w3.weight', 'global_transformer.layers.10.ffn_norm.weight', 'global_transformer.layers.11.attention.wk.weight', 'global_transformer.layers.11.attention.wo.weight', 'global_transformer.layers.11.attention.wq.weight', 'global_transformer.layers.11.attention.wv.weight', 'global_transformer.layers.11.attention_norm.weight', 'global_transformer.layers.11.feed_forward.w1.weight', 'global_transformer.layers.11.feed_forward.w2.weight', 'global_transformer.layers.11.feed_forward.w3.weight', 'global_transformer.layers.11.ffn_norm.weight', 'global_transformer.layers.12.attention.wk.weight', 'global_transformer.layers.12.attention.wo.weight', 'global_transformer.layers.12.attention.wq.weight', 'global_transformer.layers.12.attention.wv.weight', 'global_transformer.layers.12.attention_norm.weight', 'global_transformer.layers.12.feed_forward.w1.weight', 'global_transformer.layers.12.feed_forward.w2.weight', 'global_transformer.layers.12.feed_forward.w3.weight', 'global_transformer.layers.12.ffn_norm.weight', 'global_transformer.layers.13.attention.wk.weight', 'global_transformer.layers.13.attention.wo.weight', 'global_transformer.layers.13.attention.wq.weight', 'global_transformer.layers.13.attention.wv.weight', 'global_transformer.layers.13.attention_norm.weight', 'global_transformer.layers.13.feed_forward.w1.weight', 'global_transformer.layers.13.feed_forward.w2.weight', 'global_transformer.layers.13.feed_forward.w3.weight', 'global_transformer.layers.13.ffn_norm.weight', 'global_transformer.layers.14.attention.wk.weight', 'global_transformer.layers.14.attention.wo.weight', 'global_transformer.layers.14.attention.wq.weight', 'global_transformer.layers.14.attention.wv.weight', 'global_transformer.layers.14.attention_norm.weight', 'global_transformer.layers.14.feed_forward.w1.weight', 'global_transformer.layers.14.feed_forward.w2.weight', 'global_transformer.layers.14.feed_forward.w3.weight', 'global_transformer.layers.14.ffn_norm.weight', 'global_transformer.layers.15.attention.wk.weight', 'global_transformer.layers.15.attention.wo.weight', 'global_transformer.layers.15.attention.wq.weight', 'global_transformer.layers.15.attention.wv.weight', 'global_transformer.layers.15.attention_norm.weight', 'global_transformer.layers.15.feed_forward.w1.weight', 'global_transformer.layers.15.feed_forward.w2.weight', 'global_transformer.layers.15.feed_forward.w3.weight', 'global_transformer.layers.15.ffn_norm.weight', 'global_transformer.layers.16.attention.wk.weight', 'global_transformer.layers.16.attention.wo.weight', 'global_transformer.layers.16.attention.wq.weight', 'global_transformer.layers.16.attention.wv.weight', 'global_transformer.layers.16.attention_norm.weight', 'global_transformer.layers.16.feed_forward.w1.weight', 'global_transformer.layers.16.feed_forward.w2.weight', 'global_transformer.layers.16.feed_forward.w3.weight', 'global_transformer.layers.16.ffn_norm.weight', 'global_transformer.layers.17.attention.wk.weight', 'global_transformer.layers.17.attention.wo.weight', 'global_transformer.layers.17.attention.wq.weight', 'global_transformer.layers.17.attention.wv.weight', 'global_transformer.layers.17.attention_norm.weight', 'global_transformer.layers.17.feed_forward.w1.weight', 'global_transformer.layers.17.feed_forward.w2.weight', 'global_transformer.layers.17.feed_forward.w3.weight', 'global_transformer.layers.17.ffn_norm.weight', 'global_transformer.layers.18.attention.wk.weight', 'global_transformer.layers.18.attention.wo.weight', 'global_transformer.layers.18.attention.wq.weight', 'global_transformer.layers.18.attention.wv.weight', 'global_transformer.layers.18.attention_norm.weight', 'global_transformer.layers.18.feed_forward.w1.weight', 'global_transformer.layers.18.feed_forward.w2.weight', 'global_transformer.layers.18.feed_forward.w3.weight', 'global_transformer.layers.18.ffn_norm.weight', 'global_transformer.layers.19.attention.wk.weight', 'global_transformer.layers.19.attention.wo.weight', 'global_transformer.layers.19.attention.wq.weight', 'global_transformer.layers.19.attention.wv.weight', 'global_transformer.layers.19.attention_norm.weight', 'global_transformer.layers.19.feed_forward.w1.weight', 'global_transformer.layers.19.feed_forward.w2.weight', 'global_transformer.layers.19.feed_forward.w3.weight', 'global_transformer.layers.19.ffn_norm.weight', 'global_transformer.layers.2.attention.wk.weight', 'global_transformer.layers.2.attention.wo.weight', 'global_transformer.layers.2.attention.wq.weight', 'global_transformer.layers.2.attention.wv.weight', 'global_transformer.layers.2.attention_norm.weight', 'global_transformer.layers.2.feed_forward.w1.weight', 'global_transformer.layers.2.feed_forward.w2.weight', 'global_transformer.layers.2.feed_forward.w3.weight', 'global_transformer.layers.2.ffn_norm.weight', 'global_transformer.layers.20.attention.wk.weight', 'global_transformer.layers.20.attention.wo.weight', 'global_transformer.layers.20.attention.wq.weight', 'global_transformer.layers.20.attention.wv.weight', 'global_transformer.layers.20.attention_norm.weight', 'global_transformer.layers.20.feed_forward.w1.weight', 'global_transformer.layers.20.feed_forward.w2.weight', 'global_transformer.layers.20.feed_forward.w3.weight', 'global_transformer.layers.20.ffn_norm.weight', 'global_transformer.layers.21.attention.wk.weight', 'global_transformer.layers.21.attention.wo.weight', 'global_transformer.layers.21.attention.wq.weight', 'global_transformer.layers.21.attention.wv.weight', 'global_transformer.layers.21.attention_norm.weight', 'global_transformer.layers.21.feed_forward.w1.weight', 'global_transformer.layers.21.feed_forward.w2.weight', 'global_transformer.layers.21.feed_forward.w3.weight', 'global_transformer.layers.21.ffn_norm.weight', 'global_transformer.layers.22.attention.wk.weight', 'global_transformer.layers.22.attention.wo.weight', 'global_transformer.layers.22.attention.wq.weight', 'global_transformer.layers.22.attention.wv.weight', 'global_transformer.layers.22.attention_norm.weight', 'global_transformer.layers.22.feed_forward.w1.weight', 'global_transformer.layers.22.feed_forward.w2.weight', 'global_transformer.layers.22.feed_forward.w3.weight', 'global_transformer.layers.22.ffn_norm.weight', 'global_transformer.layers.23.attention.wk.weight', 'global_transformer.layers.23.attention.wo.weight', 'global_transformer.layers.23.attention.wq.weight', 'global_transformer.layers.23.attention.wv.weight', 'global_transformer.layers.23.attention_norm.weight', 'global_transformer.layers.23.feed_forward.w1.weight', 'global_transformer.layers.23.feed_forward.w2.weight', 'global_transformer.layers.23.feed_forward.w3.weight', 'global_transformer.layers.23.ffn_norm.weight', 'global_transformer.layers.24.attention.wk.weight', 'global_transformer.layers.24.attention.wo.weight', 'global_transformer.layers.24.attention.wq.weight', 'global_transformer.layers.24.attention.wv.weight', 'global_transformer.layers.24.attention_norm.weight', 'global_transformer.layers.24.feed_forward.w1.weight', 'global_transformer.layers.24.feed_forward.w2.weight', 'global_transformer.layers.24.feed_forward.w3.weight', 'global_transformer.layers.24.ffn_norm.weight', 'global_transformer.layers.3.attention.wk.weight', 'global_transformer.layers.3.attention.wo.weight', 'global_transformer.layers.3.attention.wq.weight', 'global_transformer.layers.3.attention.wv.weight', 'global_transformer.layers.3.attention_norm.weight', 'global_transformer.layers.3.feed_forward.w1.weight', 'global_transformer.layers.3.feed_forward.w2.weight', 'global_transformer.layers.3.feed_forward.w3.weight', 'global_transformer.layers.3.ffn_norm.weight', 'global_transformer.layers.4.attention.wk.weight', 'global_transformer.layers.4.attention.wo.weight', 'global_transformer.layers.4.attention.wq.weight', 'global_transformer.layers.4.attention.wv.weight', 'global_transformer.layers.4.attention_norm.weight', 'global_transformer.layers.4.feed_forward.w1.weight', 'global_transformer.layers.4.feed_forward.w2.weight', 'global_transformer.layers.4.feed_forward.w3.weight', 'global_transformer.layers.4.ffn_norm.weight', 'global_transformer.layers.5.attention.wk.weight', 'global_transformer.layers.5.attention.wo.weight', 'global_transformer.layers.5.attention.wq.weight', 'global_transformer.layers.5.attention.wv.weight', 'global_transformer.layers.5.attention_norm.weight', 'global_transformer.layers.5.feed_forward.w1.weight', 'global_transformer.layers.5.feed_forward.w2.weight', 'global_transformer.layers.5.feed_forward.w3.weight', 'global_transformer.layers.5.ffn_norm.weight', 'global_transformer.layers.6.attention.wk.weight', 'global_transformer.layers.6.attention.wo.weight', 'global_transformer.layers.6.attention.wq.weight', 'global_transformer.layers.6.attention.wv.weight', 'global_transformer.layers.6.attention_norm.weight', 'global_transformer.layers.6.feed_forward.w1.weight', 'global_transformer.layers.6.feed_forward.w2.weight', 'global_transformer.layers.6.feed_forward.w3.weight', 'global_transformer.layers.6.ffn_norm.weight', 'global_transformer.layers.7.attention.wk.weight', 'global_transformer.layers.7.attention.wo.weight', 'global_transformer.layers.7.attention.wq.weight', 'global_transformer.layers.7.attention.wv.weight', 'global_transformer.layers.7.attention_norm.weight', 'global_transformer.layers.7.feed_forward.w1.weight', 'global_transformer.layers.7.feed_forward.w2.weight', 'global_transformer.layers.7.feed_forward.w3.weight', 'global_transformer.layers.7.ffn_norm.weight', 'global_transformer.layers.8.attention.wk.weight', 'global_transformer.layers.8.attention.wo.weight', 'global_transformer.layers.8.attention.wq.weight', 'global_transformer.layers.8.attention.wv.weight', 'global_transformer.layers.8.attention_norm.weight', 'global_transformer.layers.8.feed_forward.w1.weight', 'global_transformer.layers.8.feed_forward.w2.weight', 'global_transformer.layers.8.feed_forward.w3.weight', 'global_transformer.layers.8.ffn_norm.weight', 'global_transformer.layers.9.attention.wk.weight', 'global_transformer.layers.9.attention.wo.weight', 'global_transformer.layers.9.attention.wq.weight', 'global_transformer.layers.9.attention.wv.weight', 'global_transformer.layers.9.attention_norm.weight', 'global_transformer.layers.9.feed_forward.w1.weight', 'global_transformer.layers.9.feed_forward.w2.weight', 'global_transformer.layers.9.feed_forward.w3.weight', 'global_transformer.layers.9.ffn_norm.weight', 'local_decoder.cross_attn_layers.0.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.0.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.0.wk.weight', 'local_decoder.cross_attn_layers.0.wo.weight', 'local_decoder.cross_attn_layers.0.wq.weight', 'local_decoder.cross_attn_layers.0.wv.weight', 'local_decoder.cross_attn_layers.1.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.1.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.1.wk.weight', 'local_decoder.cross_attn_layers.1.wo.weight', 'local_decoder.cross_attn_layers.1.wq.weight', 'local_decoder.cross_attn_layers.1.wv.weight', 'local_decoder.cross_attn_layers.2.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.2.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.2.wk.weight', 'local_decoder.cross_attn_layers.2.wo.weight', 'local_decoder.cross_attn_layers.2.wq.weight', 'local_decoder.cross_attn_layers.2.wv.weight', 'local_decoder.cross_attn_layers.3.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.3.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.3.wk.weight', 'local_decoder.cross_attn_layers.3.wo.weight', 'local_decoder.cross_attn_layers.3.wq.weight', 'local_decoder.cross_attn_layers.3.wv.weight', 'local_decoder.cross_attn_layers.4.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.4.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.4.wk.weight', 'local_decoder.cross_attn_layers.4.wo.weight', 'local_decoder.cross_attn_layers.4.wq.weight', 'local_decoder.cross_attn_layers.4.wv.weight', 'local_decoder.cross_attn_layers.5.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.5.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.5.wk.weight', 'local_decoder.cross_attn_layers.5.wo.weight', 'local_decoder.cross_attn_layers.5.wq.weight', 'local_decoder.cross_attn_layers.5.wv.weight', 'local_decoder.cross_attn_layers.6.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.6.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.6.wk.weight', 'local_decoder.cross_attn_layers.6.wo.weight', 'local_decoder.cross_attn_layers.6.wq.weight', 'local_decoder.cross_attn_layers.6.wv.weight', 'local_decoder.cross_attn_layers.7.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.7.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.7.wk.weight', 'local_decoder.cross_attn_layers.7.wo.weight', 'local_decoder.cross_attn_layers.7.wq.weight', 'local_decoder.cross_attn_layers.7.wv.weight', 'local_decoder.cross_attn_layers.8.cross_attn_norm_kv.weight', 'local_decoder.cross_attn_layers.8.cross_attn_norm_q.weight', 'local_decoder.cross_attn_layers.8.wk.weight', 'local_decoder.cross_attn_layers.8.wo.weight', 'local_decoder.cross_attn_layers.8.wq.weight', 'local_decoder.cross_attn_layers.8.wv.weight', 'local_decoder.layers.0.attention.wk.weight', 'local_decoder.layers.0.attention.wo.weight', 'local_decoder.layers.0.attention.wq.weight', 'local_decoder.layers.0.attention.wv.weight', 'local_decoder.layers.0.attention_norm.weight', 'local_decoder.layers.0.feed_forward.w1.weight', 'local_decoder.layers.0.feed_forward.w2.weight', 'local_decoder.layers.0.feed_forward.w3.weight', 'local_decoder.layers.0.ffn_norm.weight', 'local_decoder.layers.1.attention.wk.weight', 'local_decoder.layers.1.attention.wo.weight', 'local_decoder.layers.1.attention.wq.weight', 'local_decoder.layers.1.attention.wv.weight', 'local_decoder.layers.1.attention_norm.weight', 'local_decoder.layers.1.feed_forward.w1.weight', 'local_decoder.layers.1.feed_forward.w2.weight', 'local_decoder.layers.1.feed_forward.w3.weight', 'local_decoder.layers.1.ffn_norm.weight', 'local_decoder.layers.2.attention.wk.weight', 'local_decoder.layers.2.attention.wo.weight', 'local_decoder.layers.2.attention.wq.weight', 'local_decoder.layers.2.attention.wv.weight', 'local_decoder.layers.2.attention_norm.weight', 'local_decoder.layers.2.feed_forward.w1.weight', 'local_decoder.layers.2.feed_forward.w2.weight', 'local_decoder.layers.2.feed_forward.w3.weight', 'local_decoder.layers.2.ffn_norm.weight', 'local_decoder.layers.3.attention.wk.weight', 'local_decoder.layers.3.attention.wo.weight', 'local_decoder.layers.3.attention.wq.weight', 'local_decoder.layers.3.attention.wv.weight', 'local_decoder.layers.3.attention_norm.weight', 'local_decoder.layers.3.feed_forward.w1.weight', 'local_decoder.layers.3.feed_forward.w2.weight', 'local_decoder.layers.3.feed_forward.w3.weight', 'local_decoder.layers.3.ffn_norm.weight', 'local_decoder.layers.4.attention.wk.weight', 'local_decoder.layers.4.attention.wo.weight', 'local_decoder.layers.4.attention.wq.weight', 'local_decoder.layers.4.attention.wv.weight', 'local_decoder.layers.4.attention_norm.weight', 'local_decoder.layers.4.feed_forward.w1.weight', 'local_decoder.layers.4.feed_forward.w2.weight', 'local_decoder.layers.4.feed_forward.w3.weight', 'local_decoder.layers.4.ffn_norm.weight', 'local_decoder.layers.5.attention.wk.weight', 'local_decoder.layers.5.attention.wo.weight', 'local_decoder.layers.5.attention.wq.weight', 'local_decoder.layers.5.attention.wv.weight', 'local_decoder.layers.5.attention_norm.weight', 'local_decoder.layers.5.feed_forward.w1.weight', 'local_decoder.layers.5.feed_forward.w2.weight', 'local_decoder.layers.5.feed_forward.w3.weight', 'local_decoder.layers.5.ffn_norm.weight', 'local_decoder.layers.6.attention.wk.weight', 'local_decoder.layers.6.attention.wo.weight', 'local_decoder.layers.6.attention.wq.weight', 'local_decoder.layers.6.attention.wv.weight', 'local_decoder.layers.6.attention_norm.weight', 'local_decoder.layers.6.feed_forward.w1.weight', 'local_decoder.layers.6.feed_forward.w2.weight', 'local_decoder.layers.6.feed_forward.w3.weight', 'local_decoder.layers.6.ffn_norm.weight', 'local_decoder.layers.7.attention.wk.weight', 'local_decoder.layers.7.attention.wo.weight', 'local_decoder.layers.7.attention.wq.weight', 'local_decoder.layers.7.attention.wv.weight', 'local_decoder.layers.7.attention_norm.weight', 'local_decoder.layers.7.feed_forward.w1.weight', 'local_decoder.layers.7.feed_forward.w2.weight', 'local_decoder.layers.7.feed_forward.w3.weight', 'local_decoder.layers.7.ffn_norm.weight', 'local_decoder.layers.8.attention.wk.weight', 'local_decoder.layers.8.attention.wo.weight', 'local_decoder.layers.8.attention.wq.weight', 'local_decoder.layers.8.attention.wv.weight', 'local_decoder.layers.8.attention_norm.weight', 'local_decoder.layers.8.feed_forward.w1.weight', 'local_decoder.layers.8.feed_forward.w2.weight', 'local_decoder.layers.8.feed_forward.w3.weight', 'local_decoder.layers.8.ffn_norm.weight', 'local_decoder.norm.weight', 'local_decoder.output.weight', 'local_decoder.patch_embedding_projection.weight', 'local_encoder.cross_attn_layers.0.cross_attn_norm_kv.weight', 'local_encoder.cross_attn_layers.0.cross_attn_norm_q.weight', 'local_encoder.cross_attn_layers.0.wk.weight', 'local_encoder.cross_attn_layers.0.wo.weight', 'local_encoder.cross_attn_layers.0.wq.weight', 'local_encoder.cross_attn_layers.0.wv.weight', 'local_encoder.layers.0.attention.wk.weight', 'local_encoder.layers.0.attention.wo.weight', 'local_encoder.layers.0.attention.wq.weight', 'local_encoder.layers.0.attention.wv.weight', 'local_encoder.layers.0.attention_norm.weight', 'local_encoder.layers.0.feed_forward.w1.weight', 'local_encoder.layers.0.feed_forward.w2.weight', 'local_encoder.layers.0.feed_forward.w3.weight', 'local_encoder.layers.0.ffn_norm.weight', 'local_encoder.patch_embedding_projection.weight', 'local_encoder.tok_embeddings.weight'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():  # Disable gradient calculation during inference\n",
        "    output = model.generate(input_ids, max_new_tokens=50)  # Adjust max_new_tokens as needed\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmtRohzCaMMy",
        "outputId": "a35953f1-87be-4505-a560-72b7bc4954dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi�����vent\u0002ás\u0003#\u0012\u0007 promleds\u0001#que\u0015\u0012\u0007 pro B���ract\u0003\u0005#\u0012\u0007 promleds\u000b\u0002ás�������ás\u0001#\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2D9dFnyBa2EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXSBW-E_bAtN",
        "outputId": "5bf46a80-c6f9-472b-abad-1cd88ae6baaf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UogO7jHGbYV2",
        "outputId": "b2af369d-4463-45a1-8d35-b54c802b1d41"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():  # Disable gradient calculation during inference\n",
        "    output = model.generate(input_ids, max_new_tokens=50)  # Adjust max_new_tokens as needed\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "faV2JFY8bApy",
        "outputId": "e048ab2c-ddc5-4d34-a0dc-658c1755c542"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f5ce00324026>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Generate output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Disable gradient calculation during inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust max_new_tokens as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Decode the output to get the generated text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, position_ids, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m         outputs = self.model.decoder(\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, position_ids, cache_position)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mreturn_legacy_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "source": [
        "\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "import re\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and the LLaMA tokenizer\n",
        "# Using Bloom as a base model because BLT is based on Bloom architecture\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi\"  # Replace with your desired input text\n",
        "\n",
        "# Clean the input text (optional, remove if not needed)\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "\n",
        "# Tokenize with truncation\n",
        "input_ids = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output using the LLaMA tokenizer\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "WMGi3pmybaE8",
        "outputId": "709ccdef-8edc-467e-f077-b613ea1eaee2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1765c09becd8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Generate output with adjusted parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     output = model.generate(\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, position_ids, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m         outputs = self.model.decoder(\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, position_ids, cache_position)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mreturn_legacy_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "source": [
        "\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "import re\n",
        "import torch\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "model.eval() # Put the model in evaluation mode\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi\"  # Replace with your desired input text\n",
        "\n",
        "# Clean the input text (optional, remove if not needed)\n",
        "cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", input_text)\n",
        "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "\n",
        "# Tokenize the input text using the Bloom tokenizer\n",
        "input_ids = tokenizer(cleaned_text, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Move to appropriate device\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ1rIWReb0Nj",
        "outputId": "8bf7dfab-27a5-4285-8b11-3c4a111989fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi, and the\n",
            "following day he was sent to a place where there were no roads,\n",
            "and in which his father had been killed. He went on with him till they came up by an old road that ran along one side of it.\n",
            "He\n"
          ]
        }
      ]
    },
    {
      "source": [
        "\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "import re\n",
        "import torch\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "model.eval() # Put the model in evaluation mode\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"who is python?\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text using the Bloom tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Move to appropriate device\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.2,\n",
        "        top_k=30,\n",
        "        top_p=0.50,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efh4S3A7cDyD",
        "outputId": "21497a8b-c8b4-431c-997c-4a4ec7e4272a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `30` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "who is python?\", self.__class__').__name__\n",
            "        if not isinstance(self[\"user\"], (list)):\n",
            "            raise TypeError(\n",
            "                \"Expected a list of user names to be returned from the function call ({0}) but got {1}!\\n\"\n",
            "                \"Available types are {2} and {3}\",\n",
            "                type = lambda self: str(), nameType=self['username']\n",
            "            )\n",
            "        return UserDict(\n",
            "            username=str((unicode)(username), 'utf-8'),\n",
            "            email=[u'%s@%s' %\n",
            "                          (u'' + u'<' * len(\n",
            "                        unicode(\n",
            "                            '%s:%d'\n",
            "                            for i in range\n",
            "                                (\n",
            "                    0,\n",
            "                            (len() - 1)\n",
            "                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "سؤال الكود يستخدم الاوزان من المسار ولا يستخدم بلوم؟\n",
        "\n",
        "سؤال جيد!\n",
        "\n",
        "أنت محق، الكود يستخدم أساسًا الأوزان من المسار الذي حددته (/content/safetensors/blt_1b/consolidated.safetensors) ولا يستخدم بلوم بشكل كامل.\n",
        "\n",
        "دعني أوضح أكثر:\n",
        "\n",
        "Bloom كأساس: يتم استدعاء BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\") لإنشاء هيكل نموذج بلوم. هذا يحدد بنية الشبكة العصبية التي سيتم استخدامها.\n",
        "الأوزان الخاصة: يتم تحميل أوزان BLT-1B من ملف safetensors الذي حددته. تُستخدم هذه الأوزان لضبط معلمات النموذج وتحسين أدائه.\n",
        "المُولِّد: عند استدعاء model.generate()، يقوم النموذج باستخدام بنية Bloom والأوزان الخاصة التي تم تحميلها لتوليد النص.\n",
        "لذا، يمكننا القول إن الكود يستخدم Bloom كقالب أو هيكل، لكنه لا يستخدم الأوزان الافتراضية لـ Bloom. بل يتم استبدالها بأوزان BLT-1B الخاصة التي تم تحميلها.\n",
        "\n",
        "باختصار: الكود يستخدم بنية بلوم لكنه يطبق عليها أوزان BLT-1B لتوجيه النموذج لتوليد النص بطريقة تتماشى مع تلك الأوزان.\n",
        "\n",
        "أتمنى أن يكون هذا قد وضح الأمر!"
      ],
      "metadata": {
        "id": "6AcWeqDOdfZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "import re\n",
        "import torch\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "model.eval() # Put the model in evaluation mode\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"who is ai?\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text using the Bloom tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Move to appropriate device\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.2,\n",
        "        top_k=30,\n",
        "        top_p=0.50,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcDNR1hQbN0Q",
        "outputId": "859b3ce8-b2fe-4a56-fa86-57fb0340ae35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "who is ai?\"\n",
            "\n",
            "\"I am the one who has been sent to kill you. I have come here for\n",
            "your sake, and if it be not your will that we should meet again,\n",
            "you must go away now; but let us wait till our time comes when my son may die.\n",
            "I shall tell him all about this affair in a few days' time; then he can do his best.\n",
            "\n",
            "\"You are right! You cannot leave me alone with such an innocent child as mine!\n",
            "You know how much love there was between them both before they were born?\n",
            "And yet what could she possibly want from her father?\n",
            "\n",
            "\"But why did Mr Huxley send\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
      ],
      "metadata": {
        "id": "ntffoPpnc2J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install sentencepiece\n",
        "!pip install safetensors\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "import re\n",
        "import torch\n",
        "import accelerate\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer with device_map\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\", device_map=\"auto\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "model.eval() # Put the model in evaluation mode\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi, and the following day he was sent to a place where there were no roads, and in which his father had been killed. He went on with him till they came up by an old road that ran along one side of it. He\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text using the Bloom tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Move input_ids to the same device as the model\n",
        "input_ids = input_ids.to(model.device)\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dCcqvbt5dyfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "import re\n",
        "import torch\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"auto\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "model.eval() # Put the model in evaluation mode\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"who is ai?\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text using the Bloom tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Move to appropriate device\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=22,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.2,\n",
        "        top_k=30,\n",
        "        top_p=0.50,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "f78bc8f3bf8e434688efe84e056639ed",
            "6258b67436544967883bea91fe612356",
            "0282a725238f4a67bdca9ae1eb7cd9d3",
            "561561e64d4b4a0cb956af94a70ff7bb",
            "d4ad8a775b7f44cebe110014e62b5265",
            "a4bdb2a78c894e3380ef8400c95d7f5f",
            "b191c88940174991a99556d0475b1cdf",
            "ec3eb88506bd4171b16373da875106d4",
            "dfa8fb9432da4a138054edcc86f98549",
            "5c26485818d54873b588b2e88455cfb1",
            "9d46469d91154cc6ab6aa39ea81d21b4"
          ]
        },
        "id": "RD4jQyZldJiA",
        "outputId": "8c9dce78-9073-4a43-b0ce-d07cb2ec0b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f78bc8f3bf8e434688efe84e056639ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/blt/bytelatent/tokenizers/blt_tokenizer.py"
      ],
      "metadata": {
        "id": "m_ZpQzN6hvX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "!python /content/blt/setup/download_tokenizer.py llama3 /content/a --api_key hf_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iErklh3Kiji1",
        "outputId": "ca982409-40e9-4b7f-91ea-276310b857bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:933: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n",
            "tokenizer.model: 100% 2.18M/2.18M [00:00<00:00, 14.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "import re\n",
        "import torch\n",
        "import accelerate\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer with device_map\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\", device_map=\"auto\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "model.eval() # Put the model in evaluation mode\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"hi, and the following day he was sent to a place where there were no roads, and in which his father had been killed. He went on with him till they came up by an old road that ran along one side of it. He\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text using the Bloom tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Move input_ids to the same device as the model\n",
        "input_ids = input_ids.to(model.device)\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "19a4ea748ba4480e9d96101142f120bf",
            "e2d13a5571454df8954e41483d792fe1",
            "941f0f710bb74552a3876399dcdb881a",
            "14d2583ecf274b8e9f937ad009643319",
            "ffeb5febca824bd68e88b2980a5a9b6b",
            "55d119c388f64054bffdc6a47c5cae72",
            "b03b382414844b5e996478d68e4b33df",
            "cd7d6075252343a2ba03c84838c6ac60",
            "020e99660c1047a9a1cc3f6a50a59856",
            "a0db9e6b181649cc902b0d0bc05b98ea",
            "32cd9f6230274baca19ee790450d06d1",
            "8a9f789421b447b8b514fc3605bdd4e5",
            "a49bf73d1866489cabb9e0f4e69633d5",
            "759fb6b6ba64467ea80a80ffdd8a9fd7",
            "fe10ba710263432b9957ca9c26d13245",
            "ef075ad9d9c14c0fba53400e554fed03",
            "3984b4b8a01045c29015a63c2eb923dc",
            "0d020d846e5b4a28897a372e8e05a70f",
            "ccbe4fa5b787491faca0f12a602b285e",
            "fbc70a2f85fb479ca6c8ddf0852f9a2f",
            "bf1df91d88c04e1ba9f220df57ac8799",
            "36688d015aad49d18182097af6a2522e",
            "4d39b61c62014969b5c2515432085634",
            "a99f6298fcea4bfb8200824cd93ea467",
            "7cfda4aa70a047faabc69189cbccf30a",
            "23689ba215c1418e8a979f853e2a642a",
            "ee97b247323045ce8f2920fbcf57d0fc",
            "43f4f74467b241e689419e9493e75677",
            "1787535f26da4bb5a01b58663ab56c49",
            "cc6e165f8ea747d58b72ca496e8786c5",
            "8752b93754994b75bb1c3e10c8310242",
            "5339318ac4ae45c1b6c83151f97901ee",
            "9571577850164f8f93b15b38a5a458e2"
          ]
        },
        "id": "Q48mHRcYim_L",
        "outputId": "66d85a74-a66a-495c-cde9-469e110ebcce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19a4ea748ba4480e9d96101142f120bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a9f789421b447b8b514fc3605bdd4e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d39b61c62014969b5c2515432085634"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
            "The class this function is called from is 'BloomTokenizerFast'.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi, and the following day he was sent to a place where there were no roads, and in which his father had been killed. He went on with him till they came up by an old road that ran along one side of it. Heur path--ren cont Wser Aess.. whser Aess.. whser Aess.. whser Aess.. whser Aess.. whser Aess.. whser Aess.. whser Aess.. whser Aess..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/.cache"
      ],
      "metadata": {
        "id": "hPK6Zlg4lAHe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjXdbz01lil3",
        "outputId": "b0a767a1-d65e-40d6-e7d6-e88c83bcd674"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from safetensors.torch import load_file\n",
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "import re\n",
        "import torch\n",
        "import accelerate\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer with device_map\n",
        "model = BloomForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", device_map=\"auto\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "model.eval() # Put the model in evaluation mode\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"what is python?\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text using the Bloom tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Move input_ids to the same device as the model\n",
        "input_ids = input_ids.to(model.device)\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403,
          "referenced_widgets": [
            "70d39c5cff9a4f7ab92cf03846327a8a",
            "510ef7c1afc145c98bbcab559f5611ec",
            "c6e83bf013aa432bb022c071902230d2",
            "555d527dcbfd4e62870840e87b7a1c8a",
            "c980e0032ab94db1b965c85643a6e899",
            "4485b0b33c9a40a3b97dcbc6554b1f77",
            "cab6a8c99d744a82aef195bc67710135",
            "826d9a76dc594576b1414ecdf228c1d4",
            "830f908d816a4abc9521741cbf74c0ab",
            "6ff14329f3c742d8ada737a3614d1733",
            "2f25c11565c24ef78a9dcdf55abf85a8",
            "9260de2b6825483481b685111af9ff0a",
            "2d91bee53faf4ca080db25cd8744de6a",
            "17de6bd7cc674986ab151f366393b5f3",
            "0a5990bfdeb348e6b267e68873d166c0",
            "f3fc47c0672342588cc07af08eb246c1",
            "9f267c06863e42b99ba3e0a17fe3e069",
            "acc389daa4244fb9b19ce4d3a79d3b52",
            "f81c24150a0d48e4924a5fb8ba4a57d0",
            "2ffa1d8731644f35881877d89d9b8b35",
            "088c7947448d4971890834785d8795d2",
            "65a36b580544433989037bf93b378735",
            "bd5b32ff45fe4b369231429312e5a615",
            "450ae8ba85bc480bbabb84f45fe95c3c",
            "004b3c33f4ca4fd68d90adb0bee52094",
            "1c82addfd11b4de3adbabd73c2c04aa8",
            "9887397bdb7f407081a57f39f82acf45",
            "225ec7ffb01c479da4404e66d96c29bd",
            "567454cc2eb0415a95bbbb08c572a39d",
            "071eb0739d4640409b47c5fe30b435fa",
            "757b695ce5b341d1896529412bb27a58",
            "89636cdabb8b458780bec5acab35c9ca",
            "94f76743b59348848b522742b14e118e",
            "7bc8cf61167a45bdb99e65fa4279e9ed",
            "b63b28b0dcdc46d1909eb565e828f03f",
            "bf7240a336994fa4b2f6cb76a50ef3e5",
            "b19d3f09063e4c7f9c2291bf9a87d29e",
            "fbffa5a074184d0a83f1c45ed680b305",
            "db206453f2be475ba78f25e116c446ef",
            "6d542517b9b94ccd983446a63a7c4f4f",
            "8b98bb429cd3418f8174d15c831cb9ef",
            "3f521189747649e4b7ce82fccd719038",
            "1e9e56d68ade4a81a4263035372a10c5",
            "777f9f79f15a4c789f8054252e89a143",
            "33da43ea81bf4d349d4382cc0980d3e6",
            "53f7d3bea6f94ef2a3cda1c709d2b330",
            "932120a80e5a4b14818b19c7537bacf9",
            "f39b9fd9b03c415c9f465a90f2049e95",
            "49aa81e38f8c4b3f9aacab75313bd706",
            "9b26cf1a43f74a7d893166cb07568aba",
            "9d6e332ff1fe462b97d9c65ba0da478b",
            "797bcb2a1c4e45079ab0227eb7af538f",
            "0e885702aa0842c985c5ee4ae3abb745",
            "650fed1371fd4e389179dad939f0bc1d",
            "d9048d46cf9347f19431f8606fecea6d",
            "80adcdc172ad46b3912b447cd6b8b7dd",
            "ba942683e3a245a5b99ed2bed5b24d64",
            "a25e06b054474ca9bfe20ea5e9d1c793",
            "826607a977b94c1aa7ee71e9b6ceadfc",
            "904c5d47507e45bcb77a641c1097f2cd",
            "b3f6635f80be4427b25eda3b6e440702",
            "eef8cea8b679400c83156e0bfe8795f3",
            "42b1147402804bcaba2949997390f5f7",
            "9ce893531ea141e4ae21d9e3cbb90925",
            "b4f62b6af92b4c80a09e92f0129a50ce",
            "138c420982714c73a43c44e061d4b815"
          ]
        },
        "id": "d6c5mXzTjsHh",
        "outputId": "74350ebe-de27-493e-fe08-e02ed7de7fe4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d39c5cff9a4f7ab92cf03846327a8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type llama to instantiate a model of type bloom. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9260de2b6825483481b685111af9ff0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BloomForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['lm_head.weight', 'transformer.h.0.input_layernorm.bias', 'transformer.h.0.input_layernorm.weight', 'transformer.h.0.mlp.dense_4h_to_h.bias', 'transformer.h.0.mlp.dense_4h_to_h.weight', 'transformer.h.0.mlp.dense_h_to_4h.bias', 'transformer.h.0.mlp.dense_h_to_4h.weight', 'transformer.h.0.post_attention_layernorm.bias', 'transformer.h.0.post_attention_layernorm.weight', 'transformer.h.0.self_attention.dense.bias', 'transformer.h.0.self_attention.dense.weight', 'transformer.h.0.self_attention.query_key_value.bias', 'transformer.h.0.self_attention.query_key_value.weight', 'transformer.h.1.input_layernorm.bias', 'transformer.h.1.input_layernorm.weight', 'transformer.h.1.mlp.dense_4h_to_h.bias', 'transformer.h.1.mlp.dense_4h_to_h.weight', 'transformer.h.1.mlp.dense_h_to_4h.bias', 'transformer.h.1.mlp.dense_h_to_4h.weight', 'transformer.h.1.post_attention_layernorm.bias', 'transformer.h.1.post_attention_layernorm.weight', 'transformer.h.1.self_attention.dense.bias', 'transformer.h.1.self_attention.dense.weight', 'transformer.h.1.self_attention.query_key_value.bias', 'transformer.h.1.self_attention.query_key_value.weight', 'transformer.h.10.input_layernorm.bias', 'transformer.h.10.input_layernorm.weight', 'transformer.h.10.mlp.dense_4h_to_h.bias', 'transformer.h.10.mlp.dense_4h_to_h.weight', 'transformer.h.10.mlp.dense_h_to_4h.bias', 'transformer.h.10.mlp.dense_h_to_4h.weight', 'transformer.h.10.post_attention_layernorm.bias', 'transformer.h.10.post_attention_layernorm.weight', 'transformer.h.10.self_attention.dense.bias', 'transformer.h.10.self_attention.dense.weight', 'transformer.h.10.self_attention.query_key_value.bias', 'transformer.h.10.self_attention.query_key_value.weight', 'transformer.h.11.input_layernorm.bias', 'transformer.h.11.input_layernorm.weight', 'transformer.h.11.mlp.dense_4h_to_h.bias', 'transformer.h.11.mlp.dense_4h_to_h.weight', 'transformer.h.11.mlp.dense_h_to_4h.bias', 'transformer.h.11.mlp.dense_h_to_4h.weight', 'transformer.h.11.post_attention_layernorm.bias', 'transformer.h.11.post_attention_layernorm.weight', 'transformer.h.11.self_attention.dense.bias', 'transformer.h.11.self_attention.dense.weight', 'transformer.h.11.self_attention.query_key_value.bias', 'transformer.h.11.self_attention.query_key_value.weight', 'transformer.h.12.input_layernorm.bias', 'transformer.h.12.input_layernorm.weight', 'transformer.h.12.mlp.dense_4h_to_h.bias', 'transformer.h.12.mlp.dense_4h_to_h.weight', 'transformer.h.12.mlp.dense_h_to_4h.bias', 'transformer.h.12.mlp.dense_h_to_4h.weight', 'transformer.h.12.post_attention_layernorm.bias', 'transformer.h.12.post_attention_layernorm.weight', 'transformer.h.12.self_attention.dense.bias', 'transformer.h.12.self_attention.dense.weight', 'transformer.h.12.self_attention.query_key_value.bias', 'transformer.h.12.self_attention.query_key_value.weight', 'transformer.h.13.input_layernorm.bias', 'transformer.h.13.input_layernorm.weight', 'transformer.h.13.mlp.dense_4h_to_h.bias', 'transformer.h.13.mlp.dense_4h_to_h.weight', 'transformer.h.13.mlp.dense_h_to_4h.bias', 'transformer.h.13.mlp.dense_h_to_4h.weight', 'transformer.h.13.post_attention_layernorm.bias', 'transformer.h.13.post_attention_layernorm.weight', 'transformer.h.13.self_attention.dense.bias', 'transformer.h.13.self_attention.dense.weight', 'transformer.h.13.self_attention.query_key_value.bias', 'transformer.h.13.self_attention.query_key_value.weight', 'transformer.h.14.input_layernorm.bias', 'transformer.h.14.input_layernorm.weight', 'transformer.h.14.mlp.dense_4h_to_h.bias', 'transformer.h.14.mlp.dense_4h_to_h.weight', 'transformer.h.14.mlp.dense_h_to_4h.bias', 'transformer.h.14.mlp.dense_h_to_4h.weight', 'transformer.h.14.post_attention_layernorm.bias', 'transformer.h.14.post_attention_layernorm.weight', 'transformer.h.14.self_attention.dense.bias', 'transformer.h.14.self_attention.dense.weight', 'transformer.h.14.self_attention.query_key_value.bias', 'transformer.h.14.self_attention.query_key_value.weight', 'transformer.h.15.input_layernorm.bias', 'transformer.h.15.input_layernorm.weight', 'transformer.h.15.mlp.dense_4h_to_h.bias', 'transformer.h.15.mlp.dense_4h_to_h.weight', 'transformer.h.15.mlp.dense_h_to_4h.bias', 'transformer.h.15.mlp.dense_h_to_4h.weight', 'transformer.h.15.post_attention_layernorm.bias', 'transformer.h.15.post_attention_layernorm.weight', 'transformer.h.15.self_attention.dense.bias', 'transformer.h.15.self_attention.dense.weight', 'transformer.h.15.self_attention.query_key_value.bias', 'transformer.h.15.self_attention.query_key_value.weight', 'transformer.h.2.input_layernorm.bias', 'transformer.h.2.input_layernorm.weight', 'transformer.h.2.mlp.dense_4h_to_h.bias', 'transformer.h.2.mlp.dense_4h_to_h.weight', 'transformer.h.2.mlp.dense_h_to_4h.bias', 'transformer.h.2.mlp.dense_h_to_4h.weight', 'transformer.h.2.post_attention_layernorm.bias', 'transformer.h.2.post_attention_layernorm.weight', 'transformer.h.2.self_attention.dense.bias', 'transformer.h.2.self_attention.dense.weight', 'transformer.h.2.self_attention.query_key_value.bias', 'transformer.h.2.self_attention.query_key_value.weight', 'transformer.h.3.input_layernorm.bias', 'transformer.h.3.input_layernorm.weight', 'transformer.h.3.mlp.dense_4h_to_h.bias', 'transformer.h.3.mlp.dense_4h_to_h.weight', 'transformer.h.3.mlp.dense_h_to_4h.bias', 'transformer.h.3.mlp.dense_h_to_4h.weight', 'transformer.h.3.post_attention_layernorm.bias', 'transformer.h.3.post_attention_layernorm.weight', 'transformer.h.3.self_attention.dense.bias', 'transformer.h.3.self_attention.dense.weight', 'transformer.h.3.self_attention.query_key_value.bias', 'transformer.h.3.self_attention.query_key_value.weight', 'transformer.h.4.input_layernorm.bias', 'transformer.h.4.input_layernorm.weight', 'transformer.h.4.mlp.dense_4h_to_h.bias', 'transformer.h.4.mlp.dense_4h_to_h.weight', 'transformer.h.4.mlp.dense_h_to_4h.bias', 'transformer.h.4.mlp.dense_h_to_4h.weight', 'transformer.h.4.post_attention_layernorm.bias', 'transformer.h.4.post_attention_layernorm.weight', 'transformer.h.4.self_attention.dense.bias', 'transformer.h.4.self_attention.dense.weight', 'transformer.h.4.self_attention.query_key_value.bias', 'transformer.h.4.self_attention.query_key_value.weight', 'transformer.h.5.input_layernorm.bias', 'transformer.h.5.input_layernorm.weight', 'transformer.h.5.mlp.dense_4h_to_h.bias', 'transformer.h.5.mlp.dense_4h_to_h.weight', 'transformer.h.5.mlp.dense_h_to_4h.bias', 'transformer.h.5.mlp.dense_h_to_4h.weight', 'transformer.h.5.post_attention_layernorm.bias', 'transformer.h.5.post_attention_layernorm.weight', 'transformer.h.5.self_attention.dense.bias', 'transformer.h.5.self_attention.dense.weight', 'transformer.h.5.self_attention.query_key_value.bias', 'transformer.h.5.self_attention.query_key_value.weight', 'transformer.h.6.input_layernorm.bias', 'transformer.h.6.input_layernorm.weight', 'transformer.h.6.mlp.dense_4h_to_h.bias', 'transformer.h.6.mlp.dense_4h_to_h.weight', 'transformer.h.6.mlp.dense_h_to_4h.bias', 'transformer.h.6.mlp.dense_h_to_4h.weight', 'transformer.h.6.post_attention_layernorm.bias', 'transformer.h.6.post_attention_layernorm.weight', 'transformer.h.6.self_attention.dense.bias', 'transformer.h.6.self_attention.dense.weight', 'transformer.h.6.self_attention.query_key_value.bias', 'transformer.h.6.self_attention.query_key_value.weight', 'transformer.h.7.input_layernorm.bias', 'transformer.h.7.input_layernorm.weight', 'transformer.h.7.mlp.dense_4h_to_h.bias', 'transformer.h.7.mlp.dense_4h_to_h.weight', 'transformer.h.7.mlp.dense_h_to_4h.bias', 'transformer.h.7.mlp.dense_h_to_4h.weight', 'transformer.h.7.post_attention_layernorm.bias', 'transformer.h.7.post_attention_layernorm.weight', 'transformer.h.7.self_attention.dense.bias', 'transformer.h.7.self_attention.dense.weight', 'transformer.h.7.self_attention.query_key_value.bias', 'transformer.h.7.self_attention.query_key_value.weight', 'transformer.h.8.input_layernorm.bias', 'transformer.h.8.input_layernorm.weight', 'transformer.h.8.mlp.dense_4h_to_h.bias', 'transformer.h.8.mlp.dense_4h_to_h.weight', 'transformer.h.8.mlp.dense_h_to_4h.bias', 'transformer.h.8.mlp.dense_h_to_4h.weight', 'transformer.h.8.post_attention_layernorm.bias', 'transformer.h.8.post_attention_layernorm.weight', 'transformer.h.8.self_attention.dense.bias', 'transformer.h.8.self_attention.dense.weight', 'transformer.h.8.self_attention.query_key_value.bias', 'transformer.h.8.self_attention.query_key_value.weight', 'transformer.h.9.input_layernorm.bias', 'transformer.h.9.input_layernorm.weight', 'transformer.h.9.mlp.dense_4h_to_h.bias', 'transformer.h.9.mlp.dense_4h_to_h.weight', 'transformer.h.9.mlp.dense_h_to_4h.bias', 'transformer.h.9.mlp.dense_h_to_4h.weight', 'transformer.h.9.post_attention_layernorm.bias', 'transformer.h.9.post_attention_layernorm.weight', 'transformer.h.9.self_attention.dense.bias', 'transformer.h.9.self_attention.dense.weight', 'transformer.h.9.self_attention.query_key_value.bias', 'transformer.h.9.self_attention.query_key_value.weight', 'transformer.ln_f.bias', 'transformer.ln_f.weight', 'transformer.word_embeddings.weight', 'transformer.word_embeddings_layernorm.bias', 'transformer.word_embeddings_layernorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd5b32ff45fe4b369231429312e5a615"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bc8cf61167a45bdb99e65fa4279e9ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33da43ea81bf4d349d4382cc0980d3e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80adcdc172ad46b3912b447cd6b8b7dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
            "The class this function is called from is 'BloomTokenizerFast'.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is python????apellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellidoapellido\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", device_map=\"auto\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
      ],
      "metadata": {
        "id": "LrRfe_eEl_E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VgJXBrQvmHdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import re\n",
        "import torch\n",
        "import accelerate\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer with device_map\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", device_map=\"auto\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "model.eval() # Put the model in evaluation mode\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"what is python?\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text using the Bloom tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Move input_ids to the same device as the model\n",
        "input_ids = input_ids.to(model.device)\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck8pIwSBl2s9",
        "outputId": "f8cc5451-0408-4802-a3c5-c597a4d4aa17"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is python? How to use it?\n",
            "python 3.6, what are the latest versions of them\n",
            "I am just starting Python and I have been given a few tasks by my supervisor.\n",
            "This week we were supposed to make our own web application in Java,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import re\n",
        "import torch\n",
        "import accelerate\n",
        "\n",
        "# Load BLT-1B model weights\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors')\n",
        "\n",
        "# Load entropy model weights (if needed)\n",
        "# entropy_weights = load_file('/content/safetensors/entropy_model/consolidated.safetensors')\n",
        "\n",
        "# Initialize the Bloom model and tokenizer with device_map\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", device_map=\"auto\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "\n",
        "# Load the BLT-1B weights into the model (with strict=False)\n",
        "model.load_state_dict(model_weights, strict=False)\n",
        "model.eval() # Put the model in evaluation mode\n",
        "\n",
        "# Input text for inference\n",
        "input_text = \"Who invented the electric lamp?\"  # Replace with your desired input text\n",
        "\n",
        "# Tokenize the input text using the Bloom tokenizer\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Move input_ids to the same device as the model\n",
        "input_ids = input_ids.to(model.device)\n",
        "\n",
        "# Generate output with adjusted parameters\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.2,\n",
        "        top_k=20,\n",
        "        top_p=0.40,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "# Decode the output to get the generated text\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-7C514Ol6DS",
        "outputId": "a587b5bf-8748-4db4-c2cc-af0d76129331"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who invented the electric lamp? The first person to invent an electrical light was Thomas Edison. He is also known as “the father of electricity”. In 1879, he created a new type of incandescent bulb that used carbon filament instead of gas.\n",
            "What are some famous\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "هل يستخدم الكود الأوزان الخاصة بـ content/safetensors/blt_1b/consolidated.safetensors في الاستدلال؟\n",
        "نعم، بالتأكيد.\n",
        "الخطوات التي تؤكد ذلك في الكود هي:\n",
        "model_weights = load_file('/content/safetensors/blt_1b/consolidated.safetensors'): هنا يتم تحميل الأوزان من ملف consolidated.safetensors إلى المتغير model_weights.\n",
        "model.load_state_dict(model_weights, strict=False): هذه هي الخطوة الحاسمة. هنا يتم تحميل الأوزان الموجودة في model_weights (والتي جاءت من ملف BLT-1B) إلى داخل النموذج model. الخيار strict=False يسمح بتحميل الأوزان حتى لو كان هناك بعض الاختلافات الطفيفة بين أسماء الطبقات في الملف وفي بنية النموذج (سيتم تحميل الأوزان للطبقات المتطابقة).\n",
        "output = model.generate(...): عند استدعاء هذه الدالة، يتم استخدام النموذج model بحالته الحالية، والتي تتضمن الآن الأوزان المحملة من ملف BLT-1B.\n",
        "هل meta-llama/Llama-3.2-1B مجرد هيكل يضع فيه الأوزان؟\n",
        "هو أكثر من مجرد هيكل، ولكنه يُستخدم كهيكل أساسي في هذا الكود.\n",
        "عند استدعاء AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\"):\n",
        "يتم أولاً بناء بنية (هيكل) النموذج المتوافقة مع Llama-3.2-1B.\n",
        "يتم أيضاً تحميل الأوزان الأصلية المدربة مسبقاً لنموذج Llama-3.2-1B.\n",
        "لكن، في الخطوة التالية (model.load_state_dict(...))، يتم استبدال هذه الأوزان الأصلية بالأوزان المخصصة (BLT-1B) التي قمت بتحميلها.\n",
        "لذا، نعم، في سياق هذا الكود تحديداً، يتم استخدام meta-llama/Llama-3.2-1B بشكل أساسي لتوفير البنية (الهيكل) المتوافقة التي يمكن تحميل أوزان BLT-1B عليها. يجب أن تكون بنية BLT-1B متوافقة بشكل كبير مع بنية Llama-3.2-1B لكي تعمل عملية load_state_dict بشكل صحيح (وهو ما يضمنه استخدام strict=False بدرجة معينة).\n",
        "هل الاستدلال يكون بالأوزان (BLT-1B) أم بنموذج meta-llama/Llama-3.2-1B؟\n",
        "الاستدلال يتم باستخدام الأوزان المخصصة (BLT-1B) التي تم تحميلها على بنية (هيكل) نموذج meta-llama/Llama-3.2-1B.\n",
        "بمعنى آخر:\n",
        "الهيكل/البنية: من meta-llama/Llama-3.2-1B.\n",
        "الأوزان (المعرفة والسلوك): من content/safetensors/blt_1b/consolidated.safetensors (بعد تحميلها بواسطة load_state_dict).\n",
        "فالنموذج model الذي تستخدمه في model.generate() هو نموذج ببنية Llama-"
      ],
      "metadata": {
        "id": "ev08UHF-pIAH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2r2TwMYnpJS5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}