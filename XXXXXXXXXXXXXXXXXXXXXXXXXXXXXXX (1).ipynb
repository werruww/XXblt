{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuJ5prxTo-YL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F8E6s9szo_El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K5BOd2FBo_Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git clone https://github.com/facebookresearch/blt\n",
        "cd blt\n",
        "conda create -n blt python=3.12\n",
        "conda activate blt\n",
        "pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121\n",
        "pip install ninja\n",
        "pip install -v -U git+https://github.com/facebookresearch/xformers.git@de742ec3d64bd83b1184cc043e541f15d270c148\n",
        "pip install -r requirements.txt\n",
        "conda activate blt"
      ],
      "metadata": {
        "id": "6HynIMuFo_KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/blt\n",
        "%cd blt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La5D8M2XpDMC",
        "outputId": "2f040641-2ebf-405f-d5ac-de81d1c3bed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'blt'...\n",
            "remote: Enumerating objects: 1168, done.\u001b[K\n",
            "remote: Counting objects: 100% (340/340), done.\u001b[K\n",
            "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
            "remote: Total 1168 (delta 259), reused 229 (delta 220), pack-reused 828 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1168/1168), 703.57 KiB | 2.10 MiB/s, done.\n",
            "Resolving deltas: 100% (751/751), done.\n",
            "/content/blt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121\n",
        "!pip install ninja\n",
        "!pip install -v -U git+https://github.com/facebookresearch/xformers.git@de742ec3d64bd83b1184cc043e541f15d270c148\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E00aJsrMpFEV",
        "outputId": "67d9dfe8-150b-4bcc-f53a-6f2ce02978df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torch-2.6.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (768.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.0/768.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-triton==3.1.0+cf34004b8a (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.1.0%2Bcf34004b8a-cp311-cp311-linux_x86_64.whl (239.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.7/239.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: pytorch-triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.6.0.dev20241112+cu121 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.6.0.dev20241112+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 pytorch-triton-3.1.0+cf34004b8a torch-2.6.0.dev20241112+cu121\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.4\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Collecting git+https://github.com/facebookresearch/xformers.git@de742ec3d64bd83b1184cc043e541f15d270c148\n",
            "  Cloning https://github.com/facebookresearch/xformers.git (to revision de742ec3d64bd83b1184cc043e541f15d270c148) to /tmp/pip-req-build-mzf203xk\n",
            "  Running command git version\n",
            "  git version 2.34.1\n",
            "  Running command git clone --filter=blob:none https://github.com/facebookresearch/xformers.git /tmp/pip-req-build-mzf203xk\n",
            "  Cloning into '/tmp/pip-req-build-mzf203xk'...\n",
            "  Updating files:   0% (2/1090)\n",
            "  Updating files:   1% (11/1090)\n",
            "  Updating files:   2% (22/1090)\n",
            "  Updating files:   3% (33/1090)\n",
            "  Updating files:   4% (44/1090)\n",
            "  Updating files:   5% (55/1090)\n",
            "  Updating files:   6% (66/1090)\n",
            "  Updating files:   7% (77/1090)\n",
            "  Updating files:   8% (88/1090)\n",
            "  Updating files:   9% (99/1090)\n",
            "  Updating files:  10% (109/1090)\n",
            "  Updating files:  11% (120/1090)\n",
            "  Updating files:  12% (131/1090)\n",
            "  Updating files:  13% (142/1090)\n",
            "  Updating files:  14% (153/1090)\n",
            "  Updating files:  15% (164/1090)\n",
            "  Updating files:  16% (175/1090)\n",
            "  Updating files:  17% (186/1090)\n",
            "  Updating files:  18% (197/1090)\n",
            "  Updating files:  19% (208/1090)\n",
            "  Updating files:  20% (218/1090)\n",
            "  Updating files:  21% (229/1090)\n",
            "  Updating files:  22% (240/1090)\n",
            "  Updating files:  23% (251/1090)\n",
            "  Updating files:  24% (262/1090)\n",
            "  Updating files:  25% (273/1090)\n",
            "  Updating files:  26% (284/1090)\n",
            "  Updating files:  27% (295/1090)\n",
            "  Updating files:  28% (306/1090)\n",
            "  Updating files:  29% (317/1090)\n",
            "  Updating files:  30% (327/1090)\n",
            "  Updating files:  31% (338/1090)\n",
            "  Updating files:  32% (349/1090)\n",
            "  Updating files:  33% (360/1090)\n",
            "  Updating files:  34% (371/1090)\n",
            "  Updating files:  35% (382/1090)\n",
            "  Updating files:  36% (393/1090)\n",
            "  Updating files:  37% (404/1090)\n",
            "  Updating files:  38% (415/1090)\n",
            "  Updating files:  39% (426/1090)\n",
            "  Updating files:  40% (436/1090)\n",
            "  Updating files:  41% (447/1090)\n",
            "  Updating files:  42% (458/1090)\n",
            "  Updating files:  43% (469/1090)\n",
            "  Updating files:  44% (480/1090)\n",
            "  Updating files:  45% (491/1090)\n",
            "  Updating files:  46% (502/1090)\n",
            "  Updating files:  47% (513/1090)\n",
            "  Updating files:  48% (524/1090)\n",
            "  Updating files:  49% (535/1090)\n",
            "  Updating files:  50% (545/1090)\n",
            "  Updating files:  51% (556/1090)\n",
            "  Updating files:  52% (567/1090)\n",
            "  Updating files:  53% (578/1090)\n",
            "  Updating files:  54% (589/1090)\n",
            "  Updating files:  55% (600/1090)\n",
            "  Updating files:  56% (611/1090)\n",
            "  Updating files:  57% (622/1090)\n",
            "  Updating files:  58% (633/1090)\n",
            "  Updating files:  59% (644/1090)\n",
            "  Updating files:  60% (654/1090)\n",
            "  Updating files:  61% (665/1090)\n",
            "  Updating files:  62% (676/1090)\n",
            "  Updating files:  63% (687/1090)\n",
            "  Updating files:  64% (698/1090)\n",
            "  Updating files:  65% (709/1090)\n",
            "  Updating files:  66% (720/1090)\n",
            "  Updating files:  67% (731/1090)\n",
            "  Updating files:  68% (742/1090)\n",
            "  Updating files:  69% (753/1090)\n",
            "  Updating files:  70% (763/1090)\n",
            "  Updating files:  71% (774/1090)\n",
            "  Updating files:  72% (785/1090)\n",
            "  Updating files:  73% (796/1090)\n",
            "  Updating files:  74% (807/1090)\n",
            "  Updating files:  75% (818/1090)\n",
            "  Updating files:  76% (829/1090)\n",
            "  Updating files:  77% (840/1090)\n",
            "  Updating files:  78% (851/1090)\n",
            "  Updating files:  79% (862/1090)\n",
            "  Updating files:  80% (872/1090)\n",
            "  Updating files:  81% (883/1090)\n",
            "  Updating files:  82% (894/1090)\n",
            "  Updating files:  83% (905/1090)\n",
            "  Updating files:  84% (916/1090)\n",
            "  Updating files:  85% (927/1090)\n",
            "  Updating files:  86% (938/1090)\n",
            "  Updating files:  87% (949/1090)\n",
            "  Updating files:  88% (960/1090)\n",
            "  Updating files:  89% (971/1090)\n",
            "  Updating files:  90% (981/1090)\n",
            "  Updating files:  91% (992/1090)\n",
            "  Updating files:  92% (1003/1090)\n",
            "  Updating files:  93% (1014/1090)\n",
            "  Updating files:  94% (1025/1090)\n",
            "  Updating files:  95% (1036/1090)\n",
            "  Updating files:  96% (1047/1090)\n",
            "  Updating files:  97% (1058/1090)\n",
            "  Updating files:  98% (1069/1090)\n",
            "  Updating files:  99% (1080/1090)\n",
            "  Updating files: 100% (1090/1090)\n",
            "  Updating files: 100% (1090/1090), done.\n",
            "  Running command git show-ref de742ec3d64bd83b1184cc043e541f15d270c148\n",
            "  Running command git rev-parse -q --verify 'sha^de742ec3d64bd83b1184cc043e541f15d270c148'\n",
            "  Running command git fetch -q https://github.com/facebookresearch/xformers.git de742ec3d64bd83b1184cc043e541f15d270c148\n",
            "  Running command git rev-parse FETCH_HEAD\n",
            "  de742ec3d64bd83b1184cc043e541f15d270c148\n",
            "  Running command git rev-parse HEAD\n",
            "  8fc8ec5a4d6498ff81c0c418b89bbaf133ae3a44\n",
            "  Running command git checkout -q de742ec3d64bd83b1184cc043e541f15d270c148\n",
            "  Resolved https://github.com/facebookresearch/xformers.git to commit de742ec3d64bd83b1184cc043e541f15d270c148\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Running command git rev-parse HEAD\n",
            "  de742ec3d64bd83b1184cc043e541f15d270c148\n",
            "  Running command python setup.py egg_info\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-eiaihj8y/xformers.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-eiaihj8y/xformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-eiaihj8y/xformers.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-eiaihj8y/xformers.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-eiaihj8y/xformers.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-eiaihj8y/xformers.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-eiaihj8y/xformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no files found matching 'third_party/flash-attention/version.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-eiaihj8y/xformers.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.4 in /usr/local/lib/python3.11/dist-packages (from xformers==0.0.29+de742ec.d20250508) (2.6.0.dev20241112+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers==0.0.29+de742ec.d20250508) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (12.1.105)\n",
            "Requirement already satisfied: pytorch-triton==3.1.0+cf34004b8a in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (3.1.0+cf34004b8a)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4->xformers==0.0.29+de742ec.d20250508) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.4->xformers==0.0.29+de742ec.d20250508) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4->xformers==0.0.29+de742ec.d20250508) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4->xformers==0.0.29+de742ec.d20250508) (3.0.2)\n",
            "Building wheels for collected packages: xformers\n",
            "  Running command git rev-parse HEAD\n",
            "  de742ec3d64bd83b1184cc043e541f15d270c148\n",
            "  Running command git show-ref de742ec3d64bd83b1184cc043e541f15d270c148\n",
            "  Running command python setup.py bdist_wheel\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers\n",
            "  copying xformers/_deprecation_warning.py -> build/lib.linux-x86_64-cpython-311/xformers\n",
            "  copying xformers/test.py -> build/lib.linux-x86_64-cpython-311/xformers\n",
            "  copying xformers/checkpoint.py -> build/lib.linux-x86_64-cpython-311/xformers\n",
            "  copying xformers/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers\n",
            "  copying xformers/utils.py -> build/lib.linux-x86_64-cpython-311/xformers\n",
            "  copying xformers/attn_bias_utils.py -> build/lib.linux-x86_64-cpython-311/xformers\n",
            "  copying xformers/info.py -> build/lib.linux-x86_64-cpython-311/xformers\n",
            "  copying xformers/_cpp_lib.py -> build/lib.linux-x86_64-cpython-311/xformers\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/components\n",
            "  copying xformers/components/input_projection.py -> build/lib.linux-x86_64-cpython-311/xformers/components\n",
            "  copying xformers/components/simplicial_embedding.py -> build/lib.linux-x86_64-cpython-311/xformers/components\n",
            "  copying xformers/components/reversible.py -> build/lib.linux-x86_64-cpython-311/xformers/components\n",
            "  copying xformers/components/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/components\n",
            "  copying xformers/components/residual.py -> build/lib.linux-x86_64-cpython-311/xformers/components\n",
            "  copying xformers/components/patch_embedding.py -> build/lib.linux-x86_64-cpython-311/xformers/components\n",
            "  copying xformers/components/activations.py -> build/lib.linux-x86_64-cpython-311/xformers/components\n",
            "  copying xformers/components/multi_head_dispatch.py -> build/lib.linux-x86_64-cpython-311/xformers/components\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/profiler\n",
            "  copying xformers/profiler/device_limits.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler\n",
            "  copying xformers/profiler/profiler.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler\n",
            "  copying xformers/profiler/profiler_dcgm_impl.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler\n",
            "  copying xformers/profiler/find_slowest.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler\n",
            "  copying xformers/profiler/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler\n",
            "  copying xformers/profiler/api.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler\n",
            "  copying xformers/profiler/profiler_dcgm.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler\n",
            "  copying xformers/profiler/profile_analyzer.py -> build/lib.linux-x86_64-cpython-311/xformers/profiler\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/triton\n",
            "  copying xformers/triton/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/triton\n",
            "  copying xformers/triton/vararg_kernel.py -> build/lib.linux-x86_64-cpython-311/xformers/triton\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn\n",
            "  copying xformers/_flash_attn/bert_padding.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn\n",
            "  copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn\n",
            "  copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn\n",
            "  copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn\n",
            "  copying xformers/_flash_attn/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn\n",
            "  copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn\n",
            "  copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn\n",
            "  copying xformers/_flash_attn/fused_softmax.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/helpers\n",
            "  copying xformers/helpers/test_utils.py -> build/lib.linux-x86_64-cpython-311/xformers/helpers\n",
            "  copying xformers/helpers/hierarchical_configs.py -> build/lib.linux-x86_64-cpython-311/xformers/helpers\n",
            "  copying xformers/helpers/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/helpers\n",
            "  copying xformers/helpers/timm_sparse_attention.py -> build/lib.linux-x86_64-cpython-311/xformers/helpers\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/factory\n",
            "  copying xformers/factory/weight_init.py -> build/lib.linux-x86_64-cpython-311/xformers/factory\n",
            "  copying xformers/factory/block_configs.py -> build/lib.linux-x86_64-cpython-311/xformers/factory\n",
            "  copying xformers/factory/model_factory.py -> build/lib.linux-x86_64-cpython-311/xformers/factory\n",
            "  copying xformers/factory/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/factory\n",
            "  copying xformers/factory/block_factory.py -> build/lib.linux-x86_64-cpython-311/xformers/factory\n",
            "  copying xformers/factory/hydra_helper.py -> build/lib.linux-x86_64-cpython-311/xformers/factory\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_revnet.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_core.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_merge_attentions.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_attn_decoding.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_indexing.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_tiled_matmul.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/utils.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_multi_head_dispatch.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_sp24.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  copying xformers/benchmarks/benchmark_sequence_parallel_fused.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/seqpar.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/rmsnorm.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/sp24.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/common.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/swiglu_op.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/modpar_layers.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/indexing.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/sequence_parallel_fused_ops.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/unbind.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/tiled_matmul.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/rope_padded.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/differentiable_collectives.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  copying xformers/ops/ipc.py -> build/lib.linux-x86_64-cpython-311/xformers/ops\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/sparse\n",
            "  copying xformers/sparse/blocksparse_tensor.py -> build/lib.linux-x86_64-cpython-311/xformers/sparse\n",
            "  copying xformers/sparse/_csr_ops.py -> build/lib.linux-x86_64-cpython-311/xformers/sparse\n",
            "  copying xformers/sparse/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/sparse\n",
            "  copying xformers/sparse/utils.py -> build/lib.linux-x86_64-cpython-311/xformers/sparse\n",
            "  copying xformers/sparse/csr_tensor.py -> build/lib.linux-x86_64-cpython-311/xformers/sparse\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/linformer.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/global_tokens.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/attention_patterns.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/scaled_dot_product.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/sparsity_config.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/fourier_mix.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/visual.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/lambda_layer.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/utils.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/favor.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/random.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/ortho.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/local.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/base.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/compositional.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/pooling.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/core.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/nystrom.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/attention_mask.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  copying xformers/components/attention/_sputnik_sparse.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/components/feedforward\n",
            "  copying xformers/components/feedforward/mlp.py -> build/lib.linux-x86_64-cpython-311/xformers/components/feedforward\n",
            "  copying xformers/components/feedforward/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/components/feedforward\n",
            "  copying xformers/components/feedforward/conv_mlp.py -> build/lib.linux-x86_64-cpython-311/xformers/components/feedforward\n",
            "  copying xformers/components/feedforward/mixture_of_experts.py -> build/lib.linux-x86_64-cpython-311/xformers/components/feedforward\n",
            "  copying xformers/components/feedforward/base.py -> build/lib.linux-x86_64-cpython-311/xformers/components/feedforward\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding\n",
            "  copying xformers/components/positional_embedding/vocab.py -> build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding\n",
            "  copying xformers/components/positional_embedding/param.py -> build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding\n",
            "  copying xformers/components/positional_embedding/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding\n",
            "  copying xformers/components/positional_embedding/sine.py -> build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding\n",
            "  copying xformers/components/positional_embedding/rotary.py -> build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding\n",
            "  copying xformers/components/positional_embedding/base.py -> build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/components/attention/feature_maps\n",
            "  copying xformers/components/attention/feature_maps/softmax.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention/feature_maps\n",
            "  copying xformers/components/attention/feature_maps/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention/feature_maps\n",
            "  copying xformers/components/attention/feature_maps/base.py -> build/lib.linux-x86_64-cpython-311/xformers/components/attention/feature_maps\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils\n",
            "  copying xformers/_flash_attn/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils\n",
            "  copying xformers/_flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils\n",
            "  copying xformers/_flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils\n",
            "  copying xformers/_flash_attn/utils/generation.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils\n",
            "  copying xformers/_flash_attn/utils/distributed.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules\n",
            "  copying xformers/_flash_attn/modules/embedding.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules\n",
            "  copying xformers/_flash_attn/modules/mlp.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules\n",
            "  copying xformers/_flash_attn/modules/mha.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules\n",
            "  copying xformers/_flash_attn/modules/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules\n",
            "  copying xformers/_flash_attn/modules/block.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers\n",
            "  copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers\n",
            "  copying xformers/_flash_attn/layers/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers\n",
            "  copying xformers/_flash_attn/layers/rotary.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/btlm.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/falcon.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/bert.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/gptj.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/gpt.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/vit.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/baichuan.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/llama.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/bigcode.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  copying xformers/_flash_attn/models/opt.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/losses\n",
            "  copying xformers/_flash_attn/losses/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/losses\n",
            "  copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/losses\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops\n",
            "  copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops\n",
            "  copying xformers/_flash_attn/ops/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops\n",
            "  copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops\n",
            "  copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops\n",
            "  copying xformers/_flash_attn/ops/activations.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton\n",
            "  copying xformers/_flash_attn/ops/triton/layer_norm.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton\n",
            "  copying xformers/_flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton\n",
            "  copying xformers/_flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton\n",
            "  copying xformers/_flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton\n",
            "  copying xformers/_flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton\n",
            "  copying xformers/_flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton\n",
            "  copying xformers/_flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA\n",
            "  copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA\n",
            "  copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA\n",
            "  copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA\n",
            "  copying xformers/benchmarks/LRA/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA\n",
            "  copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA\n",
            "  copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code\n",
            "  copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code\n",
            "  copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code\n",
            "  copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/ops/_triton\n",
            "  copying xformers/ops/_triton/k_index_select_cat.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton\n",
            "  copying xformers/ops/_triton/rmsnorm_kernels.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton\n",
            "  copying xformers/ops/_triton/k_scaled_index_add.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton\n",
            "  copying xformers/ops/_triton/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton\n",
            "  copying xformers/ops/_triton/tiled_matmul_kernels.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton\n",
            "  copying xformers/ops/_triton/rope_padded_kernels.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/_triton\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/torch_attention_compat.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/common.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/flash3.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/ck_decoder.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/ck.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/flash.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/dispatch.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/triton_splitk.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/ck_splitk.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/attn_bias.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  copying xformers/ops/fmha/cutlass.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha\n",
            "  creating build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/_triton\n",
            "  copying xformers/ops/fmha/_triton/splitk_kernels.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/_triton\n",
            "  copying xformers/ops/fmha/_triton/__init__.py -> build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/_triton\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  building 'xformers._C' extension\n",
            "  creating /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention\n",
            "  creating /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/autograd\n",
            "  creating /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu\n",
            "  creating /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/fmha\n",
            "  creating /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused\n",
            "  creating /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24\n",
            "  creating /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/cuda\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2008: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\n",
            "  If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "    warnings.warn(\n",
            "  Emitting ninja build file /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
            "  Compiling objects...\n",
            "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "  [1/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/attention.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/attention.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/attention.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [2/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/matmul.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/cpu/matmul.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/matmul.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [3/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/autograd/matmul.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/autograd/matmul.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/autograd/matmul.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [4/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/sddmm.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/cpu/sddmm.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/sddmm.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [5/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/sparse_softmax.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/cpu/sparse_softmax.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/sparse_softmax.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [6/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/spmm.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/cpu/spmm.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/spmm.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [7/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/fmha/attention_cutlass_rand_uniform.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/cuda/fmha/attention_cutlass_rand_uniform.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/fmha/attention_cutlass_rand_uniform.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 218054 bytes gmem, 72 bytes cmem[3], 112 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN71_GLOBAL__N__5954686e_33_attention_cutlass_rand_uniform_cu_f6c39a24_193919rand_uniform_kernelIfEEvlllfN2at15PhiloxCudaStateEPT_l' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN71_GLOBAL__N__5954686e_33_attention_cutlass_rand_uniform_cu_f6c39a24_193919rand_uniform_kernelIfEEvlllfN2at15PhiloxCudaStateEPT_l\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 42 registers, 424 bytes cmem[0]\n",
            "  [8/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/matmul.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/cuda/matmul.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/matmul.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN46_GLOBAL__N__428dcae1_9_matmul_cu_f6c39a24_198430matmul_with_sparse_mask_kernelIN3c104HalfEEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS3_16DefaultPtrTraitsElEENS4_IS5_Lm3ES6_lEES8_NS4_IlLm2ES6_lEE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN46_GLOBAL__N__428dcae1_9_matmul_cu_f6c39a24_198430matmul_with_sparse_mask_kernelIN3c104HalfEEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS3_16DefaultPtrTraitsElEENS4_IS5_Lm3ES6_lEES8_NS4_IlLm2ES6_lEE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 40 registers, 528 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN46_GLOBAL__N__428dcae1_9_matmul_cu_f6c39a24_198430matmul_with_sparse_mask_kernelIfEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm3ES4_lEES6_NS2_IlLm2ES4_lEE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN46_GLOBAL__N__428dcae1_9_matmul_cu_f6c39a24_198430matmul_with_sparse_mask_kernelIfEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm3ES4_lEES6_NS2_IlLm2ES4_lEE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 528 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN46_GLOBAL__N__428dcae1_9_matmul_cu_f6c39a24_198430matmul_with_sparse_mask_kernelIdEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm3ES4_lEES6_NS2_IlLm2ES4_lEE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN46_GLOBAL__N__428dcae1_9_matmul_cu_f6c39a24_198430matmul_with_sparse_mask_kernelIdEEvN2at27GenericPackedTensorAccessorIT_Lm1ENS1_16DefaultPtrTraitsElEENS2_IS3_Lm3ES4_lEES6_NS2_IlLm2ES4_lEE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 528 bytes cmem[0]\n",
            "  [9/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/sddmm.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/cuda/sddmm.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/sddmm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik40_GLOBAL__N__5f575198_8_sddmm_cu_76d5abc216CudaSddmmKernel2IfLi1ELi32ELi32ELi32ELi1EEEviiiPKiS3_S3_PKfS5_Pfi' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik40_GLOBAL__N__5f575198_8_sddmm_cu_76d5abc216CudaSddmmKernel2IfLi1ELi32ELi32ELi32ELi1EEEviiiPKiS3_S3_PKfS5_Pfi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 91 registers, 128 bytes smem, 420 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik40_GLOBAL__N__5f575198_8_sddmm_cu_76d5abc216CudaSddmmKernel2I6float2Li2ELi32ELi32ELi16ELi1EEEviiiPKiS4_S4_PKfS6_Pfi' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik40_GLOBAL__N__5f575198_8_sddmm_cu_76d5abc216CudaSddmmKernel2I6float2Li2ELi32ELi32ELi16ELi1EEEviiiPKiS4_S4_PKfS6_Pfi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 114 registers, 256 bytes smem, 420 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik40_GLOBAL__N__5f575198_8_sddmm_cu_76d5abc216CudaSddmmKernel2I6float4Li4ELi32ELi32ELi8ELi1EEEviiiPKiS4_S4_PKfS6_Pfi' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik40_GLOBAL__N__5f575198_8_sddmm_cu_76d5abc216CudaSddmmKernel2I6float4Li4ELi32ELi32ELi8ELi1EEEviiiPKiS4_S4_PKfS6_Pfi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 132 registers, 512 bytes smem, 420 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik40_GLOBAL__N__5f575198_8_sddmm_cu_76d5abc216CudaSddmmKernel2I6float4Li4ELi32ELi32ELi8ELi0EEEviiiPKiS4_S4_PKfS6_Pfi' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik40_GLOBAL__N__5f575198_8_sddmm_cu_76d5abc216CudaSddmmKernel2I6float4Li4ELi32ELi32ELi8ELi0EEEviiiPKiS4_S4_PKfS6_Pfi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 512 bytes smem, 420 bytes cmem[0]\n",
            "  [10/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/sparse_softmax.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/cuda/sparse_softmax.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/sparse_softmax.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik50_GLOBAL__N__6b0d318d_17_sparse_softmax_cu_4ff6710c19SparseSoftmaxKernelEiiPKfPKiS4_S4_Pfi' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik50_GLOBAL__N__6b0d318d_17_sparse_softmax_cu_4ff6710c19SparseSoftmaxKernelEiiPKfPKiS4_S4_Pfi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 16 registers, 404 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z27SparseSoftmaxBackwardKerneliiPKfS0_PKiS2_S2_Pfi' for 'sm_75'\n",
            "  ptxas info    : Function properties for _Z27SparseSoftmaxBackwardKerneliiPKfS0_PKiS2_S2_Pfi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 14 registers, 412 bytes cmem[0]\n",
            "  [11/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/sddmm2_cuda.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/cuda/sddmm2_cuda.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/sddmm2_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN7ge_spmm14sddmmCSR1ScaleIfEEviiimPiS1_PT_S3_S3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7ge_spmm14sddmmCSR1ScaleIfEEviiimPiS1_PT_S3_S3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 61 registers, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7ge_spmm14sddmmCSR2ScaleIfEEviiimPiS1_PT_S3_S3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7ge_spmm14sddmmCSR2ScaleIfEEviiimPiS1_PT_S3_S3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7ge_spmm14sddmmCOO1ScaleIfEEviiimPiS1_PT_S3_S3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7ge_spmm14sddmmCOO1ScaleIfEEviiimPiS1_PT_S3_S3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 61 registers, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7ge_spmm14sddmmCOO2ScaleIfEEviiimPiS1_PT_S3_S3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7ge_spmm14sddmmCOO2ScaleIfEEviiimPiS1_PT_S3_S3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7ge_spmm14sddmmCOO4ScaleIfEEviiimPiS1_PT_S3_S3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7ge_spmm14sddmmCOO4ScaleIfEEviiimPiS1_PT_S3_S3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 74 registers, 416 bytes cmem[0]\n",
            "  [12/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/spmm.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/cuda/spmm.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/spmm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIfffLi1ELi32ELi32ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES5_PKNS6_11ScalarIndexES9_PKfPS7_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIfffLi1ELi32ELi32ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES5_PKNS6_11ScalarIndexES9_PKfPS7_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 82 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIfffLi1ELi32ELi32ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES5_PKNS6_11ScalarIndexES9_PKfPS7_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIfffLi1ELi32ELi32ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES5_PKNS6_11ScalarIndexES9_PKfPS7_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 72 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float2S3_Li2ELi32ELi32ELi16ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float2S3_Li2ELi32ELi32ELi16ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 114 registers, 512 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float2S3_Li2ELi32ELi32ELi16ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float2S3_Li2ELi32ELi32ELi16ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 114 registers, 512 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 168 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 176 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi64ELi8ELi4ELi0ELb1ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi64ELi8ELi4ELi0ELb1ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi64ELi8ELi4ELi0ELb1ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi64ELi8ELi4ELi0ELb1ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4S3_Li4ELi32ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 164 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 114 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 114 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 104 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float46float2Li4ELi32ELi16ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 104 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 84 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 72 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 74 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float4fLi4ELi32ELi8ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 72 registers, 1024 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float26float4Li4ELi16ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIf6float26float4Li4ELi16ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 102 registers, 512 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float26float4Li4ELi16ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIf6float26float4Li4ELi16ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES7_PKNS8_11ScalarIndexESB_PKfPS9_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 102 registers, 512 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIff6float4Li4ELi8ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIff6float4Li4ELi8ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 66 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIff6float4Li4ELi8ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIff6float4Li4ELi8ELi32ELi8ELi4ELi0ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIff6float4Li1ELi32ELi128ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c57Kernel2INS_10SpmmConfigIff6float4Li1ELi32ELi128ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 166 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIff6float4Li1ELi32ELi128ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7sputnik39_GLOBAL__N__0b3b7516_7_spmm_cu_4ffa47c517KernelWithBounds2INS_10SpmmConfigIff6float4Li1ELi32ELi128ELi32ELi4ELi1ELb0ELi8EEEEEviiiPKiPKNT_11ScalarValueES6_PKNS7_11ScalarIndexESA_PKfPS8_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 168 registers, 256 bytes smem, 428 bytes cmem[0]\n",
            "  [13/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/sddmm.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/sddmm.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/sddmm.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [14/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/matmul.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/matmul.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/matmul.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [15/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/sparse_softmax.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/sparse_softmax.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/sparse_softmax.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [16/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/spmm.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/attention/spmm.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/spmm.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [17/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/memset_32b.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sequence_parallel_fused/memset_32b.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/memset_32b.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [18/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/nvcc_info.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/nvcc_info.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/nvcc_info.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  [19/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/synchronization.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sequence_parallel_fused/synchronization.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/synchronization.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [20/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/memset_32b_kernels.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sequence_parallel_fused/memset_32b_kernels.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/memset_32b_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  [21/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/meta_utils.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sparse24/meta_utils.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/meta_utils.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 8 bytes gmem, 64 bytes cmem[4]\n",
            "  [22/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/gemm.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sparse24/gemm.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/gemm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 8 bytes gmem, 64 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_10bfloat16_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi256ELi32EEELi256ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_SF_Li0ENSG_INSH_ILi64ELi128EEELi256ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi64EEELi1ESY_Li16EEELSV_1EfNSE_8RowMajorENSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSF_Li0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S11_fS13_NS1F_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_S13_SD_SF_fS13_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1S_S1S_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1R_Li1ENS1W_22PredicatedTileIteratorINS1W_26OutputTileOptimalThreadMapINS1W_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS20_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1V_4warp24FragmentIteratorTensorOpIS1H_S1K_fNSL_IfLi4ELb1EEES13_EENS25_20TileIteratorTensorOpIS1H_S1K_fS13_EENS1W_18SharedLoadIteratorINS23_18CompactedThreadMapEfLi32EEENS1V_6thread17LinearCombinationISD_Li8EffLNS2E_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_10bfloat16_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi256ELi32EEELi256ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_SF_Li0ENSG_INSH_ILi64ELi128EEELi256ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi64EEELi1ESY_Li16EEELSV_1EfNSE_8RowMajorENSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSF_Li0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S11_fS13_NS1F_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_S13_SD_SF_fS13_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1S_S1S_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1R_Li1ENS1W_22PredicatedTileIteratorINS1W_26OutputTileOptimalThreadMapINS1W_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS20_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1V_4warp24FragmentIteratorTensorOpIS1H_S1K_fNSL_IfLi4ELb1EEES13_EENS25_20TileIteratorTensorOpIS1H_S1K_fS13_EENS1W_18SharedLoadIteratorINS23_18CompactedThreadMapEfLi32EEENS1V_6thread17LinearCombinationISD_Li8EffLNS2E_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 744 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_10bfloat16_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi256ELi32EEELi256ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_NSE_8RowMajorELi0ENSG_INSH_ILi128ELi64EEELi256ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESZ_Li16EEELSV_1EfSX_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSF_Li0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S12_fSX_NS1F_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SX_SD_SF_fSX_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1S_S1S_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1R_Li1ENS1W_22PredicatedTileIteratorINS1W_26OutputTileOptimalThreadMapINS1W_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS20_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1V_4warp24FragmentIteratorTensorOpIS1H_S1K_fNSL_IfLi4ELb1EEESX_EENS25_20TileIteratorTensorOpIS1H_S1K_fSX_EENS1W_18SharedLoadIteratorINS23_18CompactedThreadMapEfLi32EEENS1V_6thread17LinearCombinationISD_Li8EffLNS2E_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_10bfloat16_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi256ELi32EEELi256ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_NSE_8RowMajorELi0ENSG_INSH_ILi128ELi64EEELi256ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESZ_Li16EEELSV_1EfSX_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSF_Li0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S12_fSX_NS1F_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SX_SD_SF_fSX_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1S_S1S_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1R_Li1ENS1W_22PredicatedTileIteratorINS1W_26OutputTileOptimalThreadMapINS1W_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS20_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1V_4warp24FragmentIteratorTensorOpIS1H_S1K_fNSL_IfLi4ELb1EEESX_EENS25_20TileIteratorTensorOpIS1H_S1K_fSX_EENS1W_18SharedLoadIteratorINS23_18CompactedThreadMapEfLi32EEENS1V_6thread17LinearCombinationISD_Li8EffLNS2E_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 62 registers, 744 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_10bfloat16_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi256EEELi256ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi64ELi128EEELi256ENSH_ILi8ELi4EEELi8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi64EEELi1ES10_Li16EEELSV_1EfSF_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSX_Li0ES1A_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S13_fSF_NS1G_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SF_SD_SX_fSF_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1T_S1T_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1S_Li1ENS1X_22PredicatedTileIteratorINS1X_26OutputTileOptimalThreadMapINS1X_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS21_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1W_4warp24FragmentIteratorTensorOpIS1I_S1L_fNSL_IfLi4ELb1EEESF_EENS26_20TileIteratorTensorOpIS1I_S1L_fSF_EENS1X_18SharedLoadIteratorINS24_18CompactedThreadMapEfLi32EEENS1W_6thread17LinearCombinationISD_Li8EffLNS2F_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_10bfloat16_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi256EEELi256ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi64ELi128EEELi256ENSH_ILi8ELi4EEELi8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi64EEELi1ES10_Li16EEELSV_1EfSF_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSX_Li0ES1A_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S13_fSF_NS1G_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SF_SD_SX_fSF_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1T_S1T_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1S_Li1ENS1X_22PredicatedTileIteratorINS1X_26OutputTileOptimalThreadMapINS1X_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS21_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1W_4warp24FragmentIteratorTensorOpIS1I_S1L_fNSL_IfLi4ELb1EEESF_EENS26_20TileIteratorTensorOpIS1I_S1L_fSF_EENS1X_18SharedLoadIteratorINS24_18CompactedThreadMapEfLi32EEENS1W_6thread17LinearCombinationISD_Li8EffLNS2F_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 744 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_10bfloat16_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi256EEELi256ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_SF_Li0ENSG_INSH_ILi128ELi64EEELi256ENSH_ILi8ELi4EEELi8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESZ_Li16EEELSV_1EfSF_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtNSE_11ColumnMajorELi0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S12_fSF_NS1G_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SF_SD_S1D_fSF_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1T_S1T_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1S_Li1ENS1X_22PredicatedTileIteratorINS1X_26OutputTileOptimalThreadMapINS1X_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS21_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1W_4warp24FragmentIteratorTensorOpIS1I_S1L_fNSL_IfLi4ELb1EEESF_EENS26_20TileIteratorTensorOpIS1I_S1L_fSF_EENS1X_18SharedLoadIteratorINS24_18CompactedThreadMapEfLi32EEENS1W_6thread17LinearCombinationISD_Li8EffLNS2F_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_10bfloat16_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi256EEELi256ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_SF_Li0ENSG_INSH_ILi128ELi64EEELi256ENSH_ILi8ELi4EEELi8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESZ_Li16EEELSV_1EfSF_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtNSE_11ColumnMajorELi0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S12_fSF_NS1G_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SF_SD_S1D_fSF_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1T_S1T_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1S_Li1ENS1X_22PredicatedTileIteratorINS1X_26OutputTileOptimalThreadMapINS1X_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS21_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1W_4warp24FragmentIteratorTensorOpIS1I_S1L_fNSL_IfLi4ELb1EEESF_EENS26_20TileIteratorTensorOpIS1I_S1L_fSF_EENS1X_18SharedLoadIteratorINS24_18CompactedThreadMapEfLi32EEENS1W_6thread17LinearCombinationISD_Li8EffLNS2F_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 744 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_6half_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi256ELi32EEELi256ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_SF_Li0ENSG_INSH_ILi64ELi128EEELi256ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi64EEELi1ESY_Li16EEELSV_1EfNSE_8RowMajorENSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSF_Li0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S11_fS13_NS1F_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_S13_SD_SF_fS13_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1S_S1S_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1R_Li1ENS1W_22PredicatedTileIteratorINS1W_26OutputTileOptimalThreadMapINS1W_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS20_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1V_4warp24FragmentIteratorTensorOpIS1H_S1K_fNSL_IfLi4ELb1EEES13_EENS25_25TileIteratorTensorOpMixedIS1H_S1K_fLi32ELi16ELi8ELi8ELb0EEENS1W_23SharedLoadIteratorMixedINS23_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1V_6thread17LinearCombinationISD_Li8EffLNS2E_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_6half_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi256ELi32EEELi256ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_SF_Li0ENSG_INSH_ILi64ELi128EEELi256ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi64EEELi1ESY_Li16EEELSV_1EfNSE_8RowMajorENSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSF_Li0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S11_fS13_NS1F_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_S13_SD_SF_fS13_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1S_S1S_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1R_Li1ENS1W_22PredicatedTileIteratorINS1W_26OutputTileOptimalThreadMapINS1W_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS20_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1V_4warp24FragmentIteratorTensorOpIS1H_S1K_fNSL_IfLi4ELb1EEES13_EENS25_25TileIteratorTensorOpMixedIS1H_S1K_fLi32ELi16ELi8ELi8ELb0EEENS1W_23SharedLoadIteratorMixedINS23_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1V_6thread17LinearCombinationISD_Li8EffLNS2E_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 744 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_6half_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi256ELi32EEELi256ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_NSE_8RowMajorELi0ENSG_INSH_ILi128ELi64EEELi256ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESZ_Li16EEELSV_1EfSX_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSF_Li0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S12_fSX_NS1F_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SX_SD_SF_fSX_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1S_S1S_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1R_Li1ENS1W_22PredicatedTileIteratorINS1W_26OutputTileOptimalThreadMapINS1W_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS20_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1V_4warp24FragmentIteratorTensorOpIS1H_S1K_fNSL_IfLi4ELb1EEESX_EENS25_25TileIteratorTensorOpMixedIS1H_S1K_fLi32ELi16ELi8ELi8ELb0EEENS1W_23SharedLoadIteratorMixedINS23_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1V_6thread17LinearCombinationISD_Li8EffLNS2E_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_6half_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi256ELi32EEELi256ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_NSE_8RowMajorELi0ENSG_INSH_ILi128ELi64EEELi256ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESZ_Li16EEELSV_1EfSX_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSF_Li0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S12_fSX_NS1F_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SX_SD_SF_fSX_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1S_S1S_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1R_Li1ENS1W_22PredicatedTileIteratorINS1W_26OutputTileOptimalThreadMapINS1W_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS20_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1V_4warp24FragmentIteratorTensorOpIS1H_S1K_fNSL_IfLi4ELb1EEESX_EENS25_25TileIteratorTensorOpMixedIS1H_S1K_fLi32ELi16ELi8ELi8ELb0EEENS1W_23SharedLoadIteratorMixedINS23_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1V_6thread17LinearCombinationISD_Li8EffLNS2E_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 62 registers, 744 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_6half_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi256EEELi256ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi64ELi128EEELi256ENSH_ILi8ELi4EEELi8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi64EEELi1ES10_Li16EEELSV_1EfSF_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSX_Li0ES1A_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S13_fSF_NS1G_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SF_SD_SX_fSF_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1T_S1T_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1S_Li1ENS1X_22PredicatedTileIteratorINS1X_26OutputTileOptimalThreadMapINS1X_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS21_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1W_4warp24FragmentIteratorTensorOpIS1I_S1L_fNSL_IfLi4ELb1EEESF_EENS26_25TileIteratorTensorOpMixedIS1I_S1L_fLi32ELi16ELi8ELi8ELb0EEENS1X_23SharedLoadIteratorMixedINS24_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1W_6thread17LinearCombinationISD_Li8EffLNS2F_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_6half_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi256EEELi256ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi64ELi128EEELi256ENSH_ILi8ELi4EEELi8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi64EEELi1ES10_Li16EEELSV_1EfSF_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtSX_Li0ES1A_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S13_fSF_NS1G_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SF_SD_SX_fSF_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1T_S1T_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1S_Li1ENS1X_22PredicatedTileIteratorINS1X_26OutputTileOptimalThreadMapINS1X_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS21_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1W_4warp24FragmentIteratorTensorOpIS1I_S1L_fNSL_IfLi4ELb1EEESF_EENS26_25TileIteratorTensorOpMixedIS1I_S1L_fLi32ELi16ELi8ELi8ELb0EEENS1X_23SharedLoadIteratorMixedINS24_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1W_6thread17LinearCombinationISD_Li8EffLNS2F_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 62 registers, 744 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_6half_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi256EEELi256ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_SF_Li0ENSG_INSH_ILi128ELi64EEELi256ENSH_ILi8ELi4EEELi8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESZ_Li16EEELSV_1EfSF_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtNSE_11ColumnMajorELi0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S12_fSF_NS1G_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SF_SD_S1D_fSF_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1T_S1T_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1S_Li1ENS1X_22PredicatedTileIteratorINS1X_26OutputTileOptimalThreadMapINS1X_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS21_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1W_4warp24FragmentIteratorTensorOpIS1I_S1L_fNSL_IfLi4ELb1EEESF_EENS26_25TileIteratorTensorOpMixedIS1I_S1L_fLi32ELi16ELi8ELi8ELb0EEENS1X_23SharedLoadIteratorMixedINS24_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1W_6thread17LinearCombinationISD_Li8EffLNS2F_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel10SparseGemmINS1_11threadblock19SparseMmaMultistageINS1_9GemmShapeILi256ELi128ELi64EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi256ELi32EEENS_6half_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi256EEELi256ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi64ELi128EEESD_SF_Li0ENSG_INSH_ILi128ELi64EEELi256ENSH_ILi8ELi4EEELi8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESZ_Li16EEELSV_1EfSF_NSA_INSB_ILi256ELi4EEEtNSE_22ColumnMajorInterleavedILi2EEELi1ENS8_30PitchLinearStripminedThreadMapINSH_ILi512ELi2EEELi128ELi8EEENSL_ItLi8ELb0EEELb0ESN_EENSP_INSB_ILi512ELi2EEEtNSE_11ColumnMajorELi0ES19_Li16EEELSV_1ENS4_15SparseMmaPolicyINS1_4warp17SparseMmaTensorOpINS6_ILi64ELi64ELi64EEESD_SR_SD_S12_fSF_NS1G_17MmaTensorOpPolicyINST_9SparseMmaINS6_ILi16ELi8ELi32EEELi32ESD_SF_SD_S1D_fSF_NST_13OpMultiplyAddELNST_12SPFormatType4KindE0EEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1T_S1T_Li1EEELi4EbEENS_8epilogue11threadblock8EpilogueIS7_S1S_Li1ENS1X_22PredicatedTileIteratorINS1X_26OutputTileOptimalThreadMapINS1X_15OutputTileShapeILi128ELi8ELi4ELi1ELi1EEENS21_ILi1ELi8ELi1ELi1ELi8EEELi256ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1W_4warp24FragmentIteratorTensorOpIS1I_S1L_fNSL_IfLi4ELb1EEESF_EENS26_25TileIteratorTensorOpMixedIS1I_S1L_fLi32ELi16ELi8ELi8ELb0EEENS1X_23SharedLoadIteratorMixedINS24_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1W_6thread17LinearCombinationISD_Li8EffLNS2F_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEENS4_30GemmIdentityThreadblockSwizzleILi3EEELb0EEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 62 registers, 744 bytes cmem[0], 8 bytes cmem[2]\n",
            "  [23/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sparse24/sparse24.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [24/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/synchronization_kernels.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sequence_parallel_fused/synchronization_kernels.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/synchronization_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 317 bytes gmem, 80 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN64_GLOBAL__N__e2ac1b78_26_synchronization_kernels_cu_f6c39a24_337318wait_values_kernelESt5arrayIPiLm8EEmim' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN64_GLOBAL__N__e2ac1b78_26_synchronization_kernels_cu_f6c39a24_337318wait_values_kernelESt5arrayIPiLm8EEmim\n",
            "      64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 27 registers, 64 bytes cumulative stack size, 440 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN64_GLOBAL__N__e2ac1b78_26_synchronization_kernels_cu_f6c39a24_337319write_values_kernelESt5arrayIPiLm8EEmi' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN64_GLOBAL__N__e2ac1b78_26_synchronization_kernels_cu_f6c39a24_337319write_values_kernelESt5arrayIPiLm8EEmi\n",
            "      64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 20 registers, 64 bytes cumulative stack size, 428 bytes cmem[0]\n",
            "  [25/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_apply.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sparse24/sparse24_apply.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_apply.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN55_GLOBAL__N__881af683_17_sparse24_apply_cu_f6c39a24_395321sparse24_apply_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN55_GLOBAL__N__881af683_17_sparse24_apply_cu_f6c39a24_395321sparse24_apply_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 107 registers, 424 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN55_GLOBAL__N__881af683_17_sparse24_apply_cu_f6c39a24_395321sparse24_apply_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN55_GLOBAL__N__881af683_17_sparse24_apply_cu_f6c39a24_395321sparse24_apply_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 107 registers, 424 bytes cmem[0]\n",
            "  [26/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_apply_dense_output.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sparse24/sparse24_apply_dense_output.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_apply_dense_output.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN68_GLOBAL__N__bf63448f_30_sparse24_apply_dense_output_cu_440feaf2_407929sparse24_apply_dense_output_kIN7cutlass10bfloat16_tELb0ELb1EEEvNS_6ParamsIT_EE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN68_GLOBAL__N__bf63448f_30_sparse24_apply_dense_output_cu_440feaf2_407929sparse24_apply_dense_output_kIN7cutlass10bfloat16_tELb0ELb1EEEvNS_6ParamsIT_EE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 58 registers, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN68_GLOBAL__N__bf63448f_30_sparse24_apply_dense_output_cu_440feaf2_407929sparse24_apply_dense_output_kIN7cutlass10bfloat16_tELb1ELb1EEEvNS_6ParamsIT_EE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN68_GLOBAL__N__bf63448f_30_sparse24_apply_dense_output_cu_440feaf2_407929sparse24_apply_dense_output_kIN7cutlass10bfloat16_tELb1ELb1EEEvNS_6ParamsIT_EE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 50 registers, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN68_GLOBAL__N__bf63448f_30_sparse24_apply_dense_output_cu_440feaf2_407929sparse24_apply_dense_output_kIN7cutlass6half_tELb0ELb1EEEvNS_6ParamsIT_EE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN68_GLOBAL__N__bf63448f_30_sparse24_apply_dense_output_cu_440feaf2_407929sparse24_apply_dense_output_kIN7cutlass6half_tELb0ELb1EEEvNS_6ParamsIT_EE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 82 registers, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN68_GLOBAL__N__bf63448f_30_sparse24_apply_dense_output_cu_440feaf2_407929sparse24_apply_dense_output_kIN7cutlass6half_tELb1ELb1EEEvNS_6ParamsIT_EE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN68_GLOBAL__N__bf63448f_30_sparse24_apply_dense_output_cu_440feaf2_407929sparse24_apply_dense_output_kIN7cutlass6half_tELb1ELb1EEEvNS_6ParamsIT_EE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 82 registers, 416 bytes cmem[0]\n",
            "  [27/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_largest_mask_2d.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sparse24/sparse24_largest_mask_2d.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_largest_mask_2d.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_Z27sparse24_largest_mask_2d_cuI14Sp24MaskKernelIN7cutlass10bfloat16_tELb1EEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _Z27sparse24_largest_mask_2d_cuI14Sp24MaskKernelIN7cutlass10bfloat16_tELb1EEEvNT_6ParamsE\n",
            "      8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 70 registers, 8 bytes cumulative stack size, 400 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z27sparse24_largest_mask_2d_cuI14Sp24MaskKernelIN7cutlass6half_tELb1EEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _Z27sparse24_largest_mask_2d_cuI14Sp24MaskKernelIN7cutlass6half_tELb1EEEvNT_6ParamsE\n",
            "      8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 8 bytes cumulative stack size, 400 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z27sparse24_largest_mask_2d_cuI14Sp24MaskKernelIN7cutlass10bfloat16_tELb0EEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _Z27sparse24_largest_mask_2d_cuI14Sp24MaskKernelIN7cutlass10bfloat16_tELb0EEEvNT_6ParamsE\n",
            "      8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 63 registers, 8 bytes cumulative stack size, 400 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z27sparse24_largest_mask_2d_cuI14Sp24MaskKernelIN7cutlass6half_tELb0EEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _Z27sparse24_largest_mask_2d_cuI14Sp24MaskKernelIN7cutlass6half_tELb0EEEvNT_6ParamsE\n",
            "      8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 8 bytes cumulative stack size, 400 bytes cmem[0]\n",
            "  [28/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_pack_test.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sparse24/sparse24_pack_test.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_pack_test.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN59_GLOBAL__N__3506ffbb_21_sparse24_pack_test_cu_440feaf2_451224meta_shuffle_test_kernelEN2at27GenericPackedTensorAccessorIlLm3ENS0_16DefaultPtrTraitsElEES3_b' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN59_GLOBAL__N__3506ffbb_21_sparse24_pack_test_cu_440feaf2_451224meta_shuffle_test_kernelEN2at27GenericPackedTensorAccessorIlLm3ENS0_16DefaultPtrTraitsElEES3_b\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 20 registers, 465 bytes cmem[0]\n",
            "  [29/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/cuda/dual_gemm_silu_identity_mul.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/swiglu/cuda/dual_gemm_silu_identity_mul.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/cuda/dual_gemm_silu_identity_mul.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 872 bytes gmem, 96 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel8DualGemmINS1_11threadblock17DualMmaMultistageINS1_9GemmShapeILi128ELi64ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_10bfloat16_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi128EEELi128ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi64EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi32ELi64EEELi128ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi1ESZ_Li16EEELSV_1ES10_S13_fSF_NS4_9MmaPolicyINS1_4warp11MmaTensorOpINS6_ILi64ELi32ELi32EEESD_SR_SD_S12_fSF_NS15_17MmaTensorOpPolicyINST_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SF_SD_SX_fSF_NST_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1G_Li1EEES1H_Li3ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1F_Li1ENS1L_22PredicatedTileIteratorINS1L_26OutputTileOptimalThreadMapINS1L_15OutputTileShapeILi64ELi8ELi2ELi1ELi1EEENS1P_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1K_4warp24FragmentIteratorTensorOpIS17_S1A_fNSL_IfLi4ELb1EEESF_EENS1U_20TileIteratorTensorOpIS17_S1A_fSF_EENS1L_18SharedLoadIteratorINS1S_18CompactedThreadMapEfLi32EEENS1K_6thread17LinearCombinationISD_Li8EffLNS23_9ScaleType4KindE1ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEES2A_NS23_14LeftSiLUAndMulISD_Li8ESD_fLS27_2EEENS4_30GemmIdentityThreadblockSwizzleILi2EEELb0ELb1ELb1EEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel8DualGemmINS1_11threadblock17DualMmaMultistageINS1_9GemmShapeILi128ELi64ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_10bfloat16_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi128EEELi128ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi64EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi32ELi64EEELi128ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi1ESZ_Li16EEELSV_1ES10_S13_fSF_NS4_9MmaPolicyINS1_4warp11MmaTensorOpINS6_ILi64ELi32ELi32EEESD_SR_SD_S12_fSF_NS15_17MmaTensorOpPolicyINST_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SF_SD_SX_fSF_NST_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1G_Li1EEES1H_Li3ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1F_Li1ENS1L_22PredicatedTileIteratorINS1L_26OutputTileOptimalThreadMapINS1L_15OutputTileShapeILi64ELi8ELi2ELi1ELi1EEENS1P_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1K_4warp24FragmentIteratorTensorOpIS17_S1A_fNSL_IfLi4ELb1EEESF_EENS1U_20TileIteratorTensorOpIS17_S1A_fSF_EENS1L_18SharedLoadIteratorINS1S_18CompactedThreadMapEfLi32EEENS1K_6thread17LinearCombinationISD_Li8EffLNS23_9ScaleType4KindE1ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEES2A_NS23_14LeftSiLUAndMulISD_Li8ESD_fLS27_2EEENS4_30GemmIdentityThreadblockSwizzleILi2EEELb0ELb1ELb1EEEEEvNT_6ParamsE\n",
            "      8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 80 registers, 8 bytes cumulative stack size, 1072 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass6KernelINS_4gemm6kernel8DualGemmINS1_11threadblock17DualMmaMultistageINS1_9GemmShapeILi128ELi64ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_6half_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi128EEELi128ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi64EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi32ELi64EEELi128ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi1ESZ_Li16EEELSV_1ES10_S13_fSF_NS4_9MmaPolicyINS1_4warp11MmaTensorOpINS6_ILi64ELi32ELi32EEESD_SR_SD_S12_fSF_NS15_17MmaTensorOpPolicyINST_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SF_SD_SX_fSF_NST_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1G_Li1EEES1H_Li3ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1F_Li1ENS1L_22PredicatedTileIteratorINS1L_26OutputTileOptimalThreadMapINS1L_15OutputTileShapeILi64ELi8ELi2ELi1ELi1EEENS1P_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1K_4warp24FragmentIteratorTensorOpIS17_S1A_fNSL_IfLi4ELb1EEESF_EENS1U_25TileIteratorTensorOpMixedIS17_S1A_fLi32ELi16ELi8ELi8ELb0EEENS1L_23SharedLoadIteratorMixedINS1S_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1K_6thread17LinearCombinationISD_Li8EffLNS23_9ScaleType4KindE1ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEES2A_NS23_14LeftSiLUAndMulISD_Li8ESD_fLS27_2EEENS4_30GemmIdentityThreadblockSwizzleILi2EEELb0ELb1ELb1EEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass6KernelINS_4gemm6kernel8DualGemmINS1_11threadblock17DualMmaMultistageINS1_9GemmShapeILi128ELi64ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_6half_tENS_6layout8RowMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi32ELi128EEELi128ENSH_ILi4ELi8EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_37RowMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi0ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi64EEESD_NSE_11ColumnMajorELi0ENSG_INSH_ILi32ELi64EEELi128ESJ_Li8EEESM_Lb0ESN_EENSP_ISW_SD_NSE_40ColumnMajorTensorOpMultiplicandCrosswiseILi16ELi32EEELi1ESZ_Li16EEELSV_1ES10_S13_fSF_NS4_9MmaPolicyINS1_4warp11MmaTensorOpINS6_ILi64ELi32ELi32EEESD_SR_SD_S12_fSF_NS15_17MmaTensorOpPolicyINST_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SF_SD_SX_fSF_NST_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELi1ELb0EbEENSB_ILi0ELi0EEES1G_Li1EEES1H_Li3ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1F_Li1ENS1L_22PredicatedTileIteratorINS1L_26OutputTileOptimalThreadMapINS1L_15OutputTileShapeILi64ELi8ELi2ELi1ELi1EEENS1P_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1K_4warp24FragmentIteratorTensorOpIS17_S1A_fNSL_IfLi4ELb1EEESF_EENS1U_25TileIteratorTensorOpMixedIS17_S1A_fLi32ELi16ELi8ELi8ELb0EEENS1L_23SharedLoadIteratorMixedINS1S_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1K_6thread17LinearCombinationISD_Li8EffLNS23_9ScaleType4KindE1ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEES2A_NS23_14LeftSiLUAndMulISD_Li8ESD_fLS27_2EEENS4_30GemmIdentityThreadblockSwizzleILi2EEELb0ELb1ELb1EEEEEvNT_6ParamsE\n",
            "      8 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 80 registers, 8 bytes cumulative stack size, 1072 bytes cmem[0]\n",
            "  [30/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_pack.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/sparse24/sparse24_pack.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_pack.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_15MetadataCutlassENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_18MetadataCuSparseLtENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_15MetadataCutlassENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_18MetadataCuSparseLtENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas warning : Value of minnctapersm for entry _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_ is out of range. minnctapersm will be ignored\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 147 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_15MetadataCutlassENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_15MetadataCutlassENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 157 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 144 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 150 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_18MetadataCuSparseLtENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_18MetadataCuSparseLtENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 150 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass10bfloat16_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 145 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 147 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_15MetadataCutlassENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_15MetadataCutlassENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 152 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_15MetadataCutlassENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 146 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_5AbsOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 150 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_18MetadataCuSparseLtENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_18MetadataCuSparseLtENS2_10Causal1122INS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 149 registers, 457 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN54_GLOBAL__N__76823a08_16_sparse24_pack_cu_f6c39a24_440134sparse24_sparsify_both_ways_kernelIN8xformers4sp2411KernelTypesIN7cutlass6half_tEEENS2_18MetadataCuSparseLtENS2_19LargestValuesGreedyINS2_10IdentityOpEEEEEvNT_6ParamsET0_T1_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 149 registers, 457 bytes cmem[0]\n",
            "  [31/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/swiglu_op.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/swiglu/swiglu_op.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/swiglu_op.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  [32/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/cuda/gemm_fused_operand_sum.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/swiglu/cuda/gemm_fused_operand_sum.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/cuda/gemm_fused_operand_sum.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 8 bytes gmem, 64 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass7Kernel2INS_4gemm6kernel18GemmWithKReductionINS1_11threadblock26MmaWithReductionMultistageINS1_9GemmShapeILi128ELi128ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_10bfloat16_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi128ELi32EEELi128ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi128EEESD_NSE_8RowMajorELi0ESK_SM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESK_Li16EEELSV_1EfSX_NS4_9MmaPolicyINS1_4warp24MmaWithReductionTensorOpINS6_ILi64ELi64ELi32EEESD_SR_SD_S10_fSX_NS13_17MmaTensorOpPolicyINST_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SX_SD_SF_fSX_NST_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELb1ELi1ELb0EbEENSB_ILi0ELi0EEES1E_Li1EEELi4ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1D_Li1ENS1J_22PredicatedTileIteratorINS1J_26OutputTileOptimalThreadMapINS1J_15OutputTileShapeILi128ELi8ELi2ELi1ELi1EEENS1N_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1I_4warp24FragmentIteratorTensorOpIS15_S18_fNSL_IfLi4ELb1EEESX_EENS1S_20TileIteratorTensorOpIS15_S18_fSX_EENS1J_18SharedLoadIteratorINS1Q_18CompactedThreadMapEfLi32EEENS1I_6thread17LinearCombinationISD_Li8EffLNS21_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEENS1J_22EpilogueGemmKReductionIfSD_S7_S1D_Lb1EEENS4_30GemmIdentityThreadblockSwizzleILi8EEEEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass7Kernel2INS_4gemm6kernel18GemmWithKReductionINS1_11threadblock26MmaWithReductionMultistageINS1_9GemmShapeILi128ELi128ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_10bfloat16_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi128ELi32EEELi128ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi128EEESD_NSE_8RowMajorELi0ESK_SM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESK_Li16EEELSV_1EfSX_NS4_9MmaPolicyINS1_4warp24MmaWithReductionTensorOpINS6_ILi64ELi64ELi32EEESD_SR_SD_S10_fSX_NS13_17MmaTensorOpPolicyINST_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SX_SD_SF_fSX_NST_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELb1ELi1ELb0EbEENSB_ILi0ELi0EEES1E_Li1EEELi4ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1D_Li1ENS1J_22PredicatedTileIteratorINS1J_26OutputTileOptimalThreadMapINS1J_15OutputTileShapeILi128ELi8ELi2ELi1ELi1EEENS1N_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1I_4warp24FragmentIteratorTensorOpIS15_S18_fNSL_IfLi4ELb1EEESX_EENS1S_20TileIteratorTensorOpIS15_S18_fSX_EENS1J_18SharedLoadIteratorINS1Q_18CompactedThreadMapEfLi32EEENS1I_6thread17LinearCombinationISD_Li8EffLNS21_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi1ELi1EEENS1J_22EpilogueGemmKReductionIfSD_S7_S1D_Lb1EEENS4_30GemmIdentityThreadblockSwizzleILi8EEEEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 56 registers, 712 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN7cutlass7Kernel2INS_4gemm6kernel18GemmWithKReductionINS1_11threadblock26MmaWithReductionMultistageINS1_9GemmShapeILi128ELi128ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_6half_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi128ELi32EEELi128ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi128EEESD_NSE_8RowMajorELi0ESK_SM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESK_Li16EEELSV_1EfSX_NS4_9MmaPolicyINS1_4warp24MmaWithReductionTensorOpINS6_ILi64ELi64ELi32EEESD_SR_SD_S10_fSX_NS13_17MmaTensorOpPolicyINST_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SX_SD_SF_fSX_NST_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELb1ELi1ELb0EbEENSB_ILi0ELi0EEES1E_Li1EEELi4ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1D_Li1ENS1J_22PredicatedTileIteratorINS1J_26OutputTileOptimalThreadMapINS1J_15OutputTileShapeILi128ELi8ELi2ELi1ELi1EEENS1N_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1I_4warp24FragmentIteratorTensorOpIS15_S18_fNSL_IfLi4ELb1EEESX_EENS1S_25TileIteratorTensorOpMixedIS15_S18_fLi32ELi16ELi8ELi8ELb0EEENS1J_23SharedLoadIteratorMixedINS1Q_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1I_6thread17LinearCombinationISD_Li8EffLNS21_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEENS1J_22EpilogueGemmKReductionIfSD_S7_S1D_Lb1EEENS4_30GemmIdentityThreadblockSwizzleILi8EEEEEEEvNT_6ParamsE' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN7cutlass7Kernel2INS_4gemm6kernel18GemmWithKReductionINS1_11threadblock26MmaWithReductionMultistageINS1_9GemmShapeILi128ELi128ELi32EEENS_9transform11threadblock28PredicatedTileAccessIteratorINS_11MatrixShapeILi128ELi32EEENS_6half_tENS_6layout11ColumnMajorELi1ENS8_29PitchLinearWarpRakedThreadMapINS_16PitchLinearShapeILi128ELi32EEELi128ENSH_ILi8ELi4EEELi8EEENS_5ArrayISD_Li8ELb0EEELb0ENSE_9NoPermuteEEENS9_25RegularTileAccessIteratorISC_SD_NSE_40ColumnMajorTensorOpMultiplicandCongruousILi16ELi64EEELi1ESK_Li16EEELNS_4arch14CacheOperation4KindE1ENSA_INSB_ILi32ELi128EEESD_NSE_8RowMajorELi0ESK_SM_Lb0ESN_EENSP_ISW_SD_NSE_37RowMajorTensorOpMultiplicandCongruousILi16ELi64EEELi0ESK_Li16EEELSV_1EfSX_NS4_9MmaPolicyINS1_4warp24MmaWithReductionTensorOpINS6_ILi64ELi64ELi32EEESD_SR_SD_S10_fSX_NS13_17MmaTensorOpPolicyINST_3MmaINS6_ILi16ELi8ELi16EEELi32ESD_SX_SD_SF_fSX_NST_13OpMultiplyAddEEENSB_ILi1ELi1EEEEELb1ELi1ELb0EbEENSB_ILi0ELi0EEES1E_Li1EEELi4ELNS1_23SharedMemoryClearOptionE0EbEENS_8epilogue11threadblock8EpilogueIS7_S1D_Li1ENS1J_22PredicatedTileIteratorINS1J_26OutputTileOptimalThreadMapINS1J_15OutputTileShapeILi128ELi8ELi2ELi1ELi1EEENS1N_ILi1ELi8ELi1ELi1ELi8EEELi128ELi8ELi16EEESD_Lb0ESN_Lb0EEENS1I_4warp24FragmentIteratorTensorOpIS15_S18_fNSL_IfLi4ELb1EEESX_EENS1S_25TileIteratorTensorOpMixedIS15_S18_fLi32ELi16ELi8ELi8ELb0EEENS1J_23SharedLoadIteratorMixedINS1Q_18CompactedThreadMapEfLi32ELi16ELi8ELi8ELb0EEENS1I_6thread17LinearCombinationISD_Li8EffLNS21_9ScaleType4KindE0ELNS_15FloatRoundStyleE2ESD_EENSB_ILi0ELi8EEELi2ELi1EEENS1J_22EpilogueGemmKReductionIfSD_S7_S1D_Lb1EEENS4_30GemmIdentityThreadblockSwizzleILi8EEEEEEEvNT_6ParamsE\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 53 registers, 712 bytes cmem[0]\n",
            "  [33/34] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/cuda/silu_bw_fused.o.d -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/swiglu/cuda/silu_bw_fused.cu -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/cuda/silu_bw_fused.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DHAS_PYTORCH --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --extended-lambda -D_ENABLE_EXTENDED_ALIGNED_STORAGE -std=c++17 --generate-line-info -DNDEBUG --threads 4 --ptxas-options=-v --ptxas-options=-O2 --ptxas-options=-allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE2_clEvEUlN3c108BFloat16ESE_SE_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESL_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE2_clEvEUlN3c108BFloat16ESE_SE_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESL_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 30 registers, 1624 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE2_clEvEUlN3c108BFloat16ESE_SE_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESL_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE2_clEvEUlN3c108BFloat16ESE_SE_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESL_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 418 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE1_clEvEUlN3c104HalfESE_SE_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESL_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE1_clEvEUlN3c104HalfESE_SE_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESL_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 28 registers, 1624 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE1_clEvEUlN3c104HalfESE_SE_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESL_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE1_clEvEUlN3c104HalfESE_SE_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESL_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 418 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE0_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESJ_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE0_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESJ_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 28 registers, 1624 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE0_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESJ_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE0_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESJ_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 34 registers, 418 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESJ_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESJ_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 44 registers, 1624 bytes cmem[0], 88 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESJ_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb1EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESJ_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 44 registers, 418 bytes cmem[0], 88 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE2_clEvEUlN3c108BFloat16ESE_SE_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESL_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE2_clEvEUlN3c108BFloat16ESE_SE_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESL_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 30 registers, 1624 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE2_clEvEUlN3c108BFloat16ESE_SE_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESL_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE2_clEvEUlN3c108BFloat16ESE_SE_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESL_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 418 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE1_clEvEUlN3c104HalfESE_SE_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESL_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE1_clEvEUlN3c104HalfESE_SE_E_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESL_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 28 registers, 1624 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE1_clEvEUlN3c104HalfESE_SE_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESL_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE1_clEvEUlN3c104HalfESE_SE_E_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESL_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 418 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE0_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESJ_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE0_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESJ_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 28 registers, 1624 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE0_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESJ_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE0_clEvEUlfffE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESJ_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 34 registers, 418 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESJ_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi3EjLb0EESJ_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 44 registers, 1624 bytes cmem[0], 88 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESJ_EEviT0_T1_T2_T3_' for 'sm_75'\n",
            "  ptxas info    : Function properties for _ZN2at6native54_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZN52_INTERNAL_e9a3ca45_16_silu_bw_fused_cu_440feaf2_505054_GLOBAL__N__e9a3ca45_16_silu_bw_fused_cu_440feaf2_505013silu_bw_fusedILb0EEESt5tupleIJNS_6TensorES7_EERKS7_SA_SA_ENKUlvE_clEvENKUlvE_clEvEUldddE_NS_6detail5ArrayIPcLi6EEE23TrivialOffsetCalculatorILi3EjESJ_EEviT0_T1_T2_T3_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 44 registers, 418 bytes cmem[0], 88 bytes cmem[2]\n",
            "  [34/34] c++ -MMD -MF /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/swiglu_packedw.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-req-build-mzf203xk/xformers/csrc -I/tmp/pip-req-build-mzf203xk/third_party/sputnik -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/tools/util/include -I/tmp/pip-req-build-mzf203xk/third_party/cutlass/examples -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /tmp/pip-req-build-mzf203xk/xformers/csrc/swiglu/swiglu_packedw.cpp -o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/swiglu_packedw.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/attention.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/autograd/matmul.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/matmul.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/sddmm.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/sparse_softmax.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cpu/spmm.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/fmha/attention_cutlass_rand_uniform.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/matmul.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/sddmm.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/sddmm2_cuda.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/sparse_softmax.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/cuda/spmm.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/matmul.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/sddmm.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/sparse_softmax.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/attention/spmm.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/nvcc_info.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/memset_32b.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/memset_32b_kernels.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/synchronization.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sequence_parallel_fused/synchronization_kernels.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/gemm.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/meta_utils.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_apply.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_apply_dense_output.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_largest_mask_2d.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_pack.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/sparse24/sparse24_pack_test.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/cuda/dual_gemm_silu_identity_mul.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/cuda/gemm_fused_operand_sum.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/cuda/silu_bw_fused.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/swiglu_op.o /tmp/pip-req-build-mzf203xk/build/temp.linux-x86_64-cpython-311/xformers/csrc/swiglu/swiglu_packedw.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/xformers/_C.so\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "  !!\n",
            "\n",
            "          ********************************************************************************\n",
            "          Please avoid running ``setup.py`` directly.\n",
            "          Instead, use pypa/build, pypa/installer or other\n",
            "          standards-based tools.\n",
            "\n",
            "          See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "          ********************************************************************************\n",
            "\n",
            "  !!\n",
            "    self.initialize_options()\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_deprecation_warning.py -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/version.py -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/test.py -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/checkpoint.py -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/components\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/input_projection.py -> build/bdist.linux-x86_64/wheel/./xformers/components\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/linformer.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/components/attention/feature_maps\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/feature_maps/softmax.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention/feature_maps\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/feature_maps/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention/feature_maps\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/feature_maps/base.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention/feature_maps\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/global_tokens.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/attention_patterns.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/scaled_dot_product.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/sparsity_config.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/fourier_mix.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/visual.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/lambda_layer.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/utils.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/favor.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/random.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/ortho.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/local.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/base.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/compositional.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/pooling.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/core.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/nystrom.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/attention_mask.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/attention/_sputnik_sparse.py -> build/bdist.linux-x86_64/wheel/./xformers/components/attention\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/components/feedforward\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/feedforward/mlp.py -> build/bdist.linux-x86_64/wheel/./xformers/components/feedforward\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/feedforward/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/components/feedforward\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/feedforward/conv_mlp.py -> build/bdist.linux-x86_64/wheel/./xformers/components/feedforward\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/feedforward/mixture_of_experts.py -> build/bdist.linux-x86_64/wheel/./xformers/components/feedforward\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/feedforward/base.py -> build/bdist.linux-x86_64/wheel/./xformers/components/feedforward\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/simplicial_embedding.py -> build/bdist.linux-x86_64/wheel/./xformers/components\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/reversible.py -> build/bdist.linux-x86_64/wheel/./xformers/components\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/components\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/components/positional_embedding\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding/vocab.py -> build/bdist.linux-x86_64/wheel/./xformers/components/positional_embedding\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding/param.py -> build/bdist.linux-x86_64/wheel/./xformers/components/positional_embedding\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/components/positional_embedding\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding/sine.py -> build/bdist.linux-x86_64/wheel/./xformers/components/positional_embedding\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding/rotary.py -> build/bdist.linux-x86_64/wheel/./xformers/components/positional_embedding\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/positional_embedding/base.py -> build/bdist.linux-x86_64/wheel/./xformers/components/positional_embedding\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/residual.py -> build/bdist.linux-x86_64/wheel/./xformers/components\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/patch_embedding.py -> build/bdist.linux-x86_64/wheel/./xformers/components\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/activations.py -> build/bdist.linux-x86_64/wheel/./xformers/components\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/components/multi_head_dispatch.py -> build/bdist.linux-x86_64/wheel/./xformers/components\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_C.so -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/profiler\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/profiler/device_limits.py -> build/bdist.linux-x86_64/wheel/./xformers/profiler\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/profiler/profiler.py -> build/bdist.linux-x86_64/wheel/./xformers/profiler\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/profiler/profiler_dcgm_impl.py -> build/bdist.linux-x86_64/wheel/./xformers/profiler\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/profiler/find_slowest.py -> build/bdist.linux-x86_64/wheel/./xformers/profiler\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/profiler/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/profiler\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/profiler/api.py -> build/bdist.linux-x86_64/wheel/./xformers/profiler\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/profiler/profiler_dcgm.py -> build/bdist.linux-x86_64/wheel/./xformers/profiler\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/profiler/profile_analyzer.py -> build/bdist.linux-x86_64/wheel/./xformers/profiler\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/triton/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/triton/vararg_kernel.py -> build/bdist.linux-x86_64/wheel/./xformers/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/utils.py -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/attn_bias_utils.py -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/info.py -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/_flash_attn\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/bert_padding.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_triton_og.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_attn_interface.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/_flash_attn/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils/pretrained.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils/benchmark.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils/generation.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/utils/distributed.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/_flash_attn/modules\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules/embedding.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/modules\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules/mlp.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/modules\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules/mha.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/modules\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/modules\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/modules/block.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/modules\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/_flash_attn/layers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers/patch_embed.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/layers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/layers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/layers/rotary.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/btlm.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/falcon.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/bert.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/gptj.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/gpt_neox.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/gpt.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/vit.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/baichuan.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/llama.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/bigcode.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/models/opt.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/_flash_attn/losses\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/losses/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/losses\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/losses/cross_entropy.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/losses\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/flash_blocksparse_attention.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/_flash_attn/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/layer_norm.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/rms_norm.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/_flash_attn/ops/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton/layer_norm.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton/mlp.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton/linear.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton/rotary.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton/k_activations.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/triton/cross_entropy.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/fused_dense.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/ops/activations.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_flash_attn/fused_softmax.py -> build/bdist.linux-x86_64/wheel/./xformers/_flash_attn\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/cpp_lib.json -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/helpers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/helpers/test_utils.py -> build/bdist.linux-x86_64/wheel/./xformers/helpers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/helpers/hierarchical_configs.py -> build/bdist.linux-x86_64/wheel/./xformers/helpers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/helpers/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/helpers\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/helpers/timm_sparse_attention.py -> build/bdist.linux-x86_64/wheel/./xformers/helpers\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/factory\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/factory/weight_init.py -> build/bdist.linux-x86_64/wheel/./xformers/factory\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/factory/block_configs.py -> build/bdist.linux-x86_64/wheel/./xformers/factory\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/factory/model_factory.py -> build/bdist.linux-x86_64/wheel/./xformers/factory\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/factory/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/factory\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/factory/block_factory.py -> build/bdist.linux-x86_64/wheel/./xformers/factory\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/factory/hydra_helper.py -> build/bdist.linux-x86_64/wheel/./xformers/factory\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_revnet.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_core.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_sddmm.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_swiglu.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_merge_attentions.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_attn_decoding.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_indexing.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_tiled_matmul.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_nystrom_utils.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/utils.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_multi_head_dispatch.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_mem_eff_attention.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/benchmarks/LRA\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/benchmarks/LRA/code\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code/model_wrapper.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks/LRA/code\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code/dataset.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks/LRA/code\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/code/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks/LRA/code\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/batch_submit.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks/LRA\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/run_with_submitit.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks/LRA\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/run_tasks.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks/LRA\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks/LRA\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/batch_fetch_results.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks/LRA\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/LRA/run_grid_search.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks/LRA\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_sp24.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/benchmarks/benchmark_sequence_parallel_fused.py -> build/bdist.linux-x86_64/wheel/./xformers/benchmarks\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/seqpar.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/rmsnorm.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/sp24.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/common.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/swiglu_op.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/modpar_layers.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/indexing.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/sequence_parallel_fused_ops.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/unbind.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/tiled_matmul.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/rope_padded.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/differentiable_collectives.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/ops/_triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/_triton/k_index_select_cat.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/_triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/_triton/rmsnorm_kernels.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/_triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/_triton/k_scaled_index_add.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/_triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/_triton/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/_triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/_triton/tiled_matmul_kernels.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/_triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/_triton/rope_padded_kernels.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/_triton\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/torch_attention_compat.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/common.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/flash3.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/ck_decoder.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/ck.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/flash.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/dispatch.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/triton_splitk.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/ck_splitk.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/attn_bias.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/ops/fmha/_triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/_triton/splitk_kernels.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha/_triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/_triton/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha/_triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/fmha/cutlass.py -> build/bdist.linux-x86_64/wheel/./xformers/ops/fmha\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/ops/ipc.py -> build/bdist.linux-x86_64/wheel/./xformers/ops\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers/sparse\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/sparse/blocksparse_tensor.py -> build/bdist.linux-x86_64/wheel/./xformers/sparse\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/sparse/_csr_ops.py -> build/bdist.linux-x86_64/wheel/./xformers/sparse\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/sparse/__init__.py -> build/bdist.linux-x86_64/wheel/./xformers/sparse\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/sparse/utils.py -> build/bdist.linux-x86_64/wheel/./xformers/sparse\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/sparse/csr_tensor.py -> build/bdist.linux-x86_64/wheel/./xformers/sparse\n",
            "  copying build/lib.linux-x86_64-cpython-311/xformers/_cpp_lib.py -> build/bdist.linux-x86_64/wheel/./xformers\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating xformers.egg-info\n",
            "  writing xformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to xformers.egg-info/dependency_links.txt\n",
            "  writing requirements to xformers.egg-info/requires.txt\n",
            "  writing top-level names to xformers.egg-info/top_level.txt\n",
            "  writing manifest file 'xformers.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'xformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no files found matching 'third_party/flash-attention/version.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'xformers.egg-info/SOURCES.txt'\n",
            "  Copying xformers.egg-info to build/bdist.linux-x86_64/wheel/./xformers-0.0.29+de742ec.d20250508-py3.11.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/xformers-0.0.29+de742ec.d20250508.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-xh6arjy3/xformers-0.0.29+de742ec.d20250508-cp311-cp311-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'xformers/_C.so'\n",
            "  adding 'xformers/__init__.py'\n",
            "  adding 'xformers/_cpp_lib.py'\n",
            "  adding 'xformers/_deprecation_warning.py'\n",
            "  adding 'xformers/attn_bias_utils.py'\n",
            "  adding 'xformers/checkpoint.py'\n",
            "  adding 'xformers/cpp_lib.json'\n",
            "  adding 'xformers/info.py'\n",
            "  adding 'xformers/test.py'\n",
            "  adding 'xformers/utils.py'\n",
            "  adding 'xformers/version.py'\n",
            "  adding 'xformers/_flash_attn/__init__.py'\n",
            "  adding 'xformers/_flash_attn/bert_padding.py'\n",
            "  adding 'xformers/_flash_attn/flash_attn_interface.py'\n",
            "  adding 'xformers/_flash_attn/flash_attn_triton.py'\n",
            "  adding 'xformers/_flash_attn/flash_attn_triton_og.py'\n",
            "  adding 'xformers/_flash_attn/flash_blocksparse_attention.py'\n",
            "  adding 'xformers/_flash_attn/flash_blocksparse_attn_interface.py'\n",
            "  adding 'xformers/_flash_attn/fused_softmax.py'\n",
            "  adding 'xformers/_flash_attn/layers/__init__.py'\n",
            "  adding 'xformers/_flash_attn/layers/patch_embed.py'\n",
            "  adding 'xformers/_flash_attn/layers/rotary.py'\n",
            "  adding 'xformers/_flash_attn/losses/__init__.py'\n",
            "  adding 'xformers/_flash_attn/losses/cross_entropy.py'\n",
            "  adding 'xformers/_flash_attn/models/__init__.py'\n",
            "  adding 'xformers/_flash_attn/models/baichuan.py'\n",
            "  adding 'xformers/_flash_attn/models/bert.py'\n",
            "  adding 'xformers/_flash_attn/models/bigcode.py'\n",
            "  adding 'xformers/_flash_attn/models/btlm.py'\n",
            "  adding 'xformers/_flash_attn/models/falcon.py'\n",
            "  adding 'xformers/_flash_attn/models/gpt.py'\n",
            "  adding 'xformers/_flash_attn/models/gpt_neox.py'\n",
            "  adding 'xformers/_flash_attn/models/gptj.py'\n",
            "  adding 'xformers/_flash_attn/models/llama.py'\n",
            "  adding 'xformers/_flash_attn/models/opt.py'\n",
            "  adding 'xformers/_flash_attn/models/vit.py'\n",
            "  adding 'xformers/_flash_attn/modules/__init__.py'\n",
            "  adding 'xformers/_flash_attn/modules/block.py'\n",
            "  adding 'xformers/_flash_attn/modules/embedding.py'\n",
            "  adding 'xformers/_flash_attn/modules/mha.py'\n",
            "  adding 'xformers/_flash_attn/modules/mlp.py'\n",
            "  adding 'xformers/_flash_attn/ops/__init__.py'\n",
            "  adding 'xformers/_flash_attn/ops/activations.py'\n",
            "  adding 'xformers/_flash_attn/ops/fused_dense.py'\n",
            "  adding 'xformers/_flash_attn/ops/layer_norm.py'\n",
            "  adding 'xformers/_flash_attn/ops/rms_norm.py'\n",
            "  adding 'xformers/_flash_attn/ops/triton/__init__.py'\n",
            "  adding 'xformers/_flash_attn/ops/triton/cross_entropy.py'\n",
            "  adding 'xformers/_flash_attn/ops/triton/k_activations.py'\n",
            "  adding 'xformers/_flash_attn/ops/triton/layer_norm.py'\n",
            "  adding 'xformers/_flash_attn/ops/triton/linear.py'\n",
            "  adding 'xformers/_flash_attn/ops/triton/mlp.py'\n",
            "  adding 'xformers/_flash_attn/ops/triton/rotary.py'\n",
            "  adding 'xformers/_flash_attn/utils/__init__.py'\n",
            "  adding 'xformers/_flash_attn/utils/benchmark.py'\n",
            "  adding 'xformers/_flash_attn/utils/distributed.py'\n",
            "  adding 'xformers/_flash_attn/utils/generation.py'\n",
            "  adding 'xformers/_flash_attn/utils/pretrained.py'\n",
            "  adding 'xformers/benchmarks/__init__.py'\n",
            "  adding 'xformers/benchmarks/benchmark_attn_decoding.py'\n",
            "  adding 'xformers/benchmarks/benchmark_core.py'\n",
            "  adding 'xformers/benchmarks/benchmark_indexing.py'\n",
            "  adding 'xformers/benchmarks/benchmark_mem_eff_attention.py'\n",
            "  adding 'xformers/benchmarks/benchmark_merge_attentions.py'\n",
            "  adding 'xformers/benchmarks/benchmark_multi_head_dispatch.py'\n",
            "  adding 'xformers/benchmarks/benchmark_nystrom_utils.py'\n",
            "  adding 'xformers/benchmarks/benchmark_revnet.py'\n",
            "  adding 'xformers/benchmarks/benchmark_sddmm.py'\n",
            "  adding 'xformers/benchmarks/benchmark_sequence_parallel_fused.py'\n",
            "  adding 'xformers/benchmarks/benchmark_sp24.py'\n",
            "  adding 'xformers/benchmarks/benchmark_swiglu.py'\n",
            "  adding 'xformers/benchmarks/benchmark_tiled_matmul.py'\n",
            "  adding 'xformers/benchmarks/utils.py'\n",
            "  adding 'xformers/benchmarks/LRA/__init__.py'\n",
            "  adding 'xformers/benchmarks/LRA/batch_fetch_results.py'\n",
            "  adding 'xformers/benchmarks/LRA/batch_submit.py'\n",
            "  adding 'xformers/benchmarks/LRA/run_grid_search.py'\n",
            "  adding 'xformers/benchmarks/LRA/run_tasks.py'\n",
            "  adding 'xformers/benchmarks/LRA/run_with_submitit.py'\n",
            "  adding 'xformers/benchmarks/LRA/code/__init__.py'\n",
            "  adding 'xformers/benchmarks/LRA/code/dataset.py'\n",
            "  adding 'xformers/benchmarks/LRA/code/model_wrapper.py'\n",
            "  adding 'xformers/components/__init__.py'\n",
            "  adding 'xformers/components/activations.py'\n",
            "  adding 'xformers/components/input_projection.py'\n",
            "  adding 'xformers/components/multi_head_dispatch.py'\n",
            "  adding 'xformers/components/patch_embedding.py'\n",
            "  adding 'xformers/components/residual.py'\n",
            "  adding 'xformers/components/reversible.py'\n",
            "  adding 'xformers/components/simplicial_embedding.py'\n",
            "  adding 'xformers/components/attention/__init__.py'\n",
            "  adding 'xformers/components/attention/_sputnik_sparse.py'\n",
            "  adding 'xformers/components/attention/attention_mask.py'\n",
            "  adding 'xformers/components/attention/attention_patterns.py'\n",
            "  adding 'xformers/components/attention/base.py'\n",
            "  adding 'xformers/components/attention/compositional.py'\n",
            "  adding 'xformers/components/attention/core.py'\n",
            "  adding 'xformers/components/attention/favor.py'\n",
            "  adding 'xformers/components/attention/fourier_mix.py'\n",
            "  adding 'xformers/components/attention/global_tokens.py'\n",
            "  adding 'xformers/components/attention/lambda_layer.py'\n",
            "  adding 'xformers/components/attention/linformer.py'\n",
            "  adding 'xformers/components/attention/local.py'\n",
            "  adding 'xformers/components/attention/nystrom.py'\n",
            "  adding 'xformers/components/attention/ortho.py'\n",
            "  adding 'xformers/components/attention/pooling.py'\n",
            "  adding 'xformers/components/attention/random.py'\n",
            "  adding 'xformers/components/attention/scaled_dot_product.py'\n",
            "  adding 'xformers/components/attention/sparsity_config.py'\n",
            "  adding 'xformers/components/attention/utils.py'\n",
            "  adding 'xformers/components/attention/visual.py'\n",
            "  adding 'xformers/components/attention/feature_maps/__init__.py'\n",
            "  adding 'xformers/components/attention/feature_maps/base.py'\n",
            "  adding 'xformers/components/attention/feature_maps/softmax.py'\n",
            "  adding 'xformers/components/feedforward/__init__.py'\n",
            "  adding 'xformers/components/feedforward/base.py'\n",
            "  adding 'xformers/components/feedforward/conv_mlp.py'\n",
            "  adding 'xformers/components/feedforward/mixture_of_experts.py'\n",
            "  adding 'xformers/components/feedforward/mlp.py'\n",
            "  adding 'xformers/components/positional_embedding/__init__.py'\n",
            "  adding 'xformers/components/positional_embedding/base.py'\n",
            "  adding 'xformers/components/positional_embedding/param.py'\n",
            "  adding 'xformers/components/positional_embedding/rotary.py'\n",
            "  adding 'xformers/components/positional_embedding/sine.py'\n",
            "  adding 'xformers/components/positional_embedding/vocab.py'\n",
            "  adding 'xformers/factory/__init__.py'\n",
            "  adding 'xformers/factory/block_configs.py'\n",
            "  adding 'xformers/factory/block_factory.py'\n",
            "  adding 'xformers/factory/hydra_helper.py'\n",
            "  adding 'xformers/factory/model_factory.py'\n",
            "  adding 'xformers/factory/weight_init.py'\n",
            "  adding 'xformers/helpers/__init__.py'\n",
            "  adding 'xformers/helpers/hierarchical_configs.py'\n",
            "  adding 'xformers/helpers/test_utils.py'\n",
            "  adding 'xformers/helpers/timm_sparse_attention.py'\n",
            "  adding 'xformers/ops/__init__.py'\n",
            "  adding 'xformers/ops/common.py'\n",
            "  adding 'xformers/ops/differentiable_collectives.py'\n",
            "  adding 'xformers/ops/indexing.py'\n",
            "  adding 'xformers/ops/ipc.py'\n",
            "  adding 'xformers/ops/modpar_layers.py'\n",
            "  adding 'xformers/ops/rmsnorm.py'\n",
            "  adding 'xformers/ops/rope_padded.py'\n",
            "  adding 'xformers/ops/seqpar.py'\n",
            "  adding 'xformers/ops/sequence_parallel_fused_ops.py'\n",
            "  adding 'xformers/ops/sp24.py'\n",
            "  adding 'xformers/ops/swiglu_op.py'\n",
            "  adding 'xformers/ops/tiled_matmul.py'\n",
            "  adding 'xformers/ops/unbind.py'\n",
            "  adding 'xformers/ops/_triton/__init__.py'\n",
            "  adding 'xformers/ops/_triton/k_index_select_cat.py'\n",
            "  adding 'xformers/ops/_triton/k_scaled_index_add.py'\n",
            "  adding 'xformers/ops/_triton/rmsnorm_kernels.py'\n",
            "  adding 'xformers/ops/_triton/rope_padded_kernels.py'\n",
            "  adding 'xformers/ops/_triton/tiled_matmul_kernels.py'\n",
            "  adding 'xformers/ops/fmha/__init__.py'\n",
            "  adding 'xformers/ops/fmha/attn_bias.py'\n",
            "  adding 'xformers/ops/fmha/ck.py'\n",
            "  adding 'xformers/ops/fmha/ck_decoder.py'\n",
            "  adding 'xformers/ops/fmha/ck_splitk.py'\n",
            "  adding 'xformers/ops/fmha/common.py'\n",
            "  adding 'xformers/ops/fmha/cutlass.py'\n",
            "  adding 'xformers/ops/fmha/dispatch.py'\n",
            "  adding 'xformers/ops/fmha/flash.py'\n",
            "  adding 'xformers/ops/fmha/flash3.py'\n",
            "  adding 'xformers/ops/fmha/torch_attention_compat.py'\n",
            "  adding 'xformers/ops/fmha/triton_splitk.py'\n",
            "  adding 'xformers/ops/fmha/_triton/__init__.py'\n",
            "  adding 'xformers/ops/fmha/_triton/splitk_kernels.py'\n",
            "  adding 'xformers/profiler/__init__.py'\n",
            "  adding 'xformers/profiler/api.py'\n",
            "  adding 'xformers/profiler/device_limits.py'\n",
            "  adding 'xformers/profiler/find_slowest.py'\n",
            "  adding 'xformers/profiler/profile_analyzer.py'\n",
            "  adding 'xformers/profiler/profiler.py'\n",
            "  adding 'xformers/profiler/profiler_dcgm.py'\n",
            "  adding 'xformers/profiler/profiler_dcgm_impl.py'\n",
            "  adding 'xformers/sparse/__init__.py'\n",
            "  adding 'xformers/sparse/_csr_ops.py'\n",
            "  adding 'xformers/sparse/blocksparse_tensor.py'\n",
            "  adding 'xformers/sparse/csr_tensor.py'\n",
            "  adding 'xformers/sparse/utils.py'\n",
            "  adding 'xformers/triton/__init__.py'\n",
            "  adding 'xformers/triton/vararg_kernel.py'\n",
            "  adding 'xformers-0.0.29+de742ec.d20250508.dist-info/LICENSE'\n",
            "  adding 'xformers-0.0.29+de742ec.d20250508.dist-info/METADATA'\n",
            "  adding 'xformers-0.0.29+de742ec.d20250508.dist-info/WHEEL'\n",
            "  adding 'xformers-0.0.29+de742ec.d20250508.dist-info/top_level.txt'\n",
            "  adding 'xformers-0.0.29+de742ec.d20250508.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for xformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xformers: filename=xformers-0.0.29+de742ec.d20250508-cp311-cp311-linux_x86_64.whl size=14878184 sha256=e0ba03c4527ee5ec1b6e4b894de1e924a890bcf83997b86c715882b6cd67fcb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/6b/f4/6a0199acc811817e733a8748cd0b6959acc83836c57d238492\n",
            "Successfully built xformers\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.29+de742ec.d20250508\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
            "Collecting omegaconf (from -r requirements.txt (line 2))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting msgspec (from -r requirements.txt (line 3))\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting rouge-score (from -r requirements.txt (line 4))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu (from -r requirements.txt (line 5))\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.2.0)\n",
            "Collecting tiktoken (from -r requirements.txt (line 7))\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting blobfile (from -r requirements.txt (line 8))\n",
            "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.19.10)\n",
            "Collecting viztracer (from -r requirements.txt (line 10))\n",
            "  Downloading viztracer-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (26 kB)\n",
            "Collecting lm-eval (from -r requirements.txt (line 11))\n",
            "  Downloading lm_eval-0.4.8-py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (1.15.2)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (12.0.0)\n",
            "Collecting datatrove (from -r requirements.txt (line 14))\n",
            "  Downloading datatrove-0.5.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (3.10.18)\n",
            "Collecting luigi (from -r requirements.txt (line 16))\n",
            "  Downloading luigi-3.6.0.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (2.11.4)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (5.5.0)\n",
            "Collecting submitit (from -r requirements.txt (line 19))\n",
            "  Downloading submitit-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (0.15.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (13.9.4)\n",
            "Requirement already satisfied: huggingface-hub==0.30.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (0.30.2)\n",
            "Requirement already satisfied: fsspec[full] in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (2025.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.30.*->-r requirements.txt (line 23)) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.30.*->-r requirements.txt (line 23)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.30.*->-r requirements.txt (line 23)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.30.*->-r requirements.txt (line 23)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.30.*->-r requirements.txt (line 23)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.30.*->-r requirements.txt (line 23)) (4.13.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r requirements.txt (line 2))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score->-r requirements.txt (line 4)) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score->-r requirements.txt (line 4)) (1.17.0)\n",
            "Collecting portalocker (from sacrebleu->-r requirements.txt (line 5))\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu->-r requirements.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu->-r requirements.txt (line 5)) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->-r requirements.txt (line 5))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu->-r requirements.txt (line 5)) (5.4.0)\n",
            "Collecting pycryptodomex>=3.8 (from blobfile->-r requirements.txt (line 8))\n",
            "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements.txt (line 8)) (2.4.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 9)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 9)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 9)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 9)) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 9)) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 9)) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 9)) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 9)) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 9)) (75.2.0)\n",
            "Collecting objprint>=0.3.0 (from viztracer->-r requirements.txt (line 10))\n",
            "  Downloading objprint-0.3.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 11)) (1.6.0)\n",
            "Collecting evaluate (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.16.0 (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting jsonlines (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 11)) (2.10.2)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 11)) (0.15.2)\n",
            "Collecting pybind11>=2.6.2 (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 11)) (1.6.1)\n",
            "Collecting sqlitedict (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 11)) (2.6.0.dev20241112+cu121)\n",
            "Collecting tqdm-multiprocess (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 11)) (4.51.3)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 11)) (0.23.0)\n",
            "Collecting dill (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting word2number (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 11)) (10.7.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->-r requirements.txt (line 13)) (12.570.86)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from datatrove->-r requirements.txt (line 14)) (4.12.3)\n",
            "Collecting loguru>=0.7.0 (from datatrove->-r requirements.txt (line 14))\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting multiprocess (from datatrove->-r requirements.txt (line 14))\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.5 in /usr/local/lib/python3.11/dist-packages (from luigi->-r requirements.txt (line 16)) (2.9.0.post0)\n",
            "Collecting tenacity<9,>=8 (from luigi->-r requirements.txt (line 16))\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tornado<7,>=5.0 in /usr/local/lib/python3.11/dist-packages (from luigi->-r requirements.txt (line 16)) (6.4.2)\n",
            "Collecting python-daemon (from luigi->-r requirements.txt (line 16))\n",
            "  Downloading python_daemon-3.1.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 17)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 17)) (0.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair->-r requirements.txt (line 18)) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair->-r requirements.txt (line 18)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair->-r requirements.txt (line 18)) (1.37.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 19)) (3.1.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->-r requirements.txt (line 20)) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 21)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 21)) (2.19.1)\n",
            "Collecting adlfs (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading adlfs-2024.12.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 22)) (3.11.15)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 22)) (2024.12.1)\n",
            "Requirement already satisfied: distributed in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 22)) (2024.12.1)\n",
            "Collecting dropbox (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading dropbox-12.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting dropboxdrivefs (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading dropboxdrivefs-1.4.1.tar.gz (7.4 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fusepy (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading fusepy-3.0.1.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 22)) (2025.3.2)\n",
            "Collecting libarchive-c (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading libarchive_c-5.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting ocifs (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading ocifs-1.3.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: panel in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 22)) (1.6.3)\n",
            "Collecting paramiko (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pyarrow>=1 in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 22)) (18.1.0)\n",
            "Requirement already satisfied: pygit2 in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 22)) (1.18.0)\n",
            "Collecting s3fs (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading s3fs-2025.3.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting smbprotocol (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading smbprotocol-1.15.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm-eval->-r requirements.txt (line 11)) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 22)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 22)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 22)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 22)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 22)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 22)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 22)) (1.20.0)\n",
            "Collecting dill (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm-eval->-r requirements.txt (line 11)) (2.2.2)\n",
            "Collecting xxhash (from datasets>=2.16.0->lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datatrove->-r requirements.txt (line 14))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting datasets>=2.16.0 (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.16.0->lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting datasets>=2.16.0 (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting dill (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting datasets>=2.16.0 (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting accelerate>=0.26.0 (from lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting fsspec[full] (from -r requirements.txt (line 22))\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (4.0.12)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 18)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 18)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 18)) (0.24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 21)) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.30.*->-r requirements.txt (line 23)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.30.*->-r requirements.txt (line 23)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.30.*->-r requirements.txt (line 23)) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm-eval->-r requirements.txt (line 11)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm-eval->-r requirements.txt (line 11)) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (12.1.105)\n",
            "Requirement already satisfied: pytorch-triton==3.1.0+cf34004b8a in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (3.1.0+cf34004b8a)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 11)) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm-eval->-r requirements.txt (line 11)) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->lm-eval->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1->lm-eval->-r requirements.txt (line 11)) (0.21.1)\n",
            "Collecting azure-core<2.0.0,>=1.28.0 (from adlfs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading azure_core-1.34.0-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-datalake-store<0.1,>=0.0.53 (from adlfs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading azure_datalake_store-0.0.53-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting azure-identity (from adlfs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading azure_identity-1.22.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-storage-blob>=12.17.0 (from adlfs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading azure_storage_blob-12.25.1-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask->fsspec[full]->-r requirements.txt (line 22)) (1.4.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask->fsspec[full]->-r requirements.txt (line 22)) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask->fsspec[full]->-r requirements.txt (line 22)) (8.7.0)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from distributed->fsspec[full]->-r requirements.txt (line 22)) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from distributed->fsspec[full]->-r requirements.txt (line 22)) (1.1.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.11/dist-packages (from distributed->fsspec[full]->-r requirements.txt (line 22)) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from distributed->fsspec[full]->-r requirements.txt (line 22)) (3.1.0)\n",
            "Requirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from distributed->fsspec[full]->-r requirements.txt (line 22)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair->-r requirements.txt (line 18)) (3.0.2)\n",
            "Collecting stone<3.3.3,>=2 (from dropbox->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading stone-3.3.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs->fsspec[full]->-r requirements.txt (line 22)) (4.4.2)\n",
            "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gcsfs (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading gcsfs-2025.3.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2025.3.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs->fsspec[full]->-r requirements.txt (line 22)) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs->fsspec[full]->-r requirements.txt (line 22)) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs->fsspec[full]->-r requirements.txt (line 22)) (2.19.0)\n",
            "Collecting oci>=2.43.1 (from ocifs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading oci-2.151.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 22)) (6.2.0)\n",
            "Requirement already satisfied: bokeh<3.8.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 22)) (3.7.2)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 22)) (2.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 22)) (3.8)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 22)) (0.4.2)\n",
            "Requirement already satisfied: param<3.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 22)) (2.2.0)\n",
            "Requirement already satisfied: pyviz-comms>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 22)) (3.0.4)\n",
            "Collecting bcrypt>=3.2 (from paramiko->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.11/dist-packages (from paramiko->fsspec[full]->-r requirements.txt (line 22)) (43.0.3)\n",
            "Collecting pynacl>=1.5 (from paramiko->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: cffi>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from pygit2->fsspec[full]->-r requirements.txt (line 22)) (1.17.1)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval->-r requirements.txt (line 11))\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting lockfile>=0.10 (from python-daemon->luigi->-r requirements.txt (line 16))\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading aiobotocore-2.22.0-py3-none-any.whl.metadata (24 kB)\n",
            "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3fs (from fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading s3fs-2025.3.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading s3fs-2025.3.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pyspnego (from smbprotocol->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading pyspnego-0.11.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting botocore<1.37.4,>=1.37.2 (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading botocore-1.37.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[full]->-r requirements.txt (line 22)) (1.17.2)\n",
            "Collecting msal<2,>=1.16.0 (from azure-datalake-store<0.1,>=0.0.53->adlfs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading msal-1.32.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob>=12.17.0->adlfs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.8.0,>=3.5.0->panel->fsspec[full]->-r requirements.txt (line 22)) (1.3.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.8.0,>=3.5.0->panel->fsspec[full]->-r requirements.txt (line 22)) (11.2.1)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.8.0,>=3.5.0->panel->fsspec[full]->-r requirements.txt (line 22)) (2025.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.17.0->pygit2->fsspec[full]->-r requirements.txt (line 22)) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (5.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask->fsspec[full]->-r requirements.txt (line 22)) (3.21.0)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval->-r requirements.txt (line 11)) (5.2.0)\n",
            "Requirement already satisfied: pyOpenSSL<25.0.0,>=17.5.0 in /usr/local/lib/python3.11/dist-packages (from oci>=2.43.1->ocifs->fsspec[full]->-r requirements.txt (line 22)) (24.2.1)\n",
            "Requirement already satisfied: pytz>=2016.10 in /usr/local/lib/python3.11/dist-packages (from oci>=2.43.1->ocifs->fsspec[full]->-r requirements.txt (line 22)) (2025.2)\n",
            "Collecting circuitbreaker<3.0.0,>=1.3.1 (from oci>=2.43.1->ocifs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading circuitbreaker-2.1.3-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->lm-eval->-r requirements.txt (line 11)) (2025.2)\n",
            "Requirement already satisfied: ply>=3.4 in /usr/local/lib/python3.11/dist-packages (from stone<3.3.3,>=2->dropbox->fsspec[full]->-r requirements.txt (line 22)) (3.11)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity->adlfs->fsspec[full]->-r requirements.txt (line 22))\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel->fsspec[full]->-r requirements.txt (line 22)) (0.5.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (1.7.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel->fsspec[full]->-r requirements.txt (line 22)) (1.0.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (1.70.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (1.26.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2,>=1.16.0->azure-datalake-store<0.1,>=0.0.53->adlfs->fsspec[full]->-r requirements.txt (line 22)) (2.10.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->fsspec[full]->-r requirements.txt (line 22)) (3.2.2)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading viztracer-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datatrove-0.5.0-py3-none-any.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading submitit-1.5.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading objprint-0.3.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading adlfs-2024.12.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading dropbox-12.0.2-py3-none-any.whl (572 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.1/572.1 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gcsfs-2025.3.0-py2.py3-none-any.whl (36 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading libarchive_c-5.2-py3-none-any.whl (15 kB)\n",
            "Downloading ocifs-1.3.2-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading paramiko-3.5.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_daemon-3.1.2-py3-none-any.whl (30 kB)\n",
            "Downloading s3fs-2025.3.0-py3-none-any.whl (30 kB)\n",
            "Downloading smbprotocol-1.15.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading aiobotocore-2.22.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.34.0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_datalake_store-0.0.53-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_storage_blob-12.25.1-py3-none-any.whl (406 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.0/407.0 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading oci-2.151.0-py3-none-any.whl (30.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.1/30.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stone-3.3.1-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Downloading azure_identity-1.22.0-py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.5/185.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspnego-0.11.2-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading botocore-1.37.3-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading circuitbreaker-2.1.3-py3-none-any.whl (7.7 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading msal-1.32.3-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, rouge-score, luigi, dropboxdrivefs, fusepy, sqlitedict, word2number\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=dc2c0d5669490cf5231e570f1e502459de6a9995017be56195902fa1793a0cd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f212b3d3ce2b7fcb4d833c0d2fe2831b3bc3c0a7809e2502c2e96cdb45c257c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "  Building wheel for luigi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for luigi: filename=luigi-3.6.0-py3-none-any.whl size=1093756 sha256=beab71f809531e09bb44ffbbcad5c8086cc1b9d5a71656b8adda5ee5a32408f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/3c/af6674483adf7af53c451451c24b1e330e00b6fe1cc70bb96a\n",
            "  Building wheel for dropboxdrivefs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dropboxdrivefs: filename=dropboxdrivefs-1.4.1-py3-none-any.whl size=8239 sha256=a936bfa2b2681df9a554d7aa68c06ee5b43a250c708a3f1f6926501b5ff64486\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/fd/ae/739b6fd4bbedc3c90dd265e6c4719ccd861cd7519fc49c1f06\n",
            "  Building wheel for fusepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10486 sha256=403096a5ff1ec88849fadf42aba0cc6c9a23b46a55b548e0cd0b425932c39734\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/4a/86/fdda91f8b8ebb0a70e4181dc2423b1f70c3c2d3bd1158685b5\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=d25fc5b7ded26c9291f4ef40bf26bde2f456a33779fc15c2e849099603bbc419\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=b7831624b3af13979d2f9731332feda2e4e1ceb9620c3c6bc8abf9fd7f944dd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built antlr4-python3-runtime rouge-score luigi dropboxdrivefs fusepy sqlitedict word2number\n",
            "Installing collected packages: word2number, sqlitedict, lockfile, libarchive-c, fusepy, circuitbreaker, antlr4-python3-runtime, xxhash, tenacity, tcolorpy, submitit, stone, python-daemon, pycryptodomex, pybind11, portalocker, pathvalidate, omegaconf, objprint, msgspec, mbstrdecoder, loguru, jsonlines, jmespath, isodate, fsspec, dill, colorama, bcrypt, aioitertools, viztracer, typepy, tqdm-multiprocess, tiktoken, sacrebleu, rouge-score, pynacl, multiprocess, luigi, dropbox, botocore, blobfile, azure-core, pyspnego, paramiko, dropboxdrivefs, datatrove, azure-storage-blob, aiobotocore, smbprotocol, s3fs, oci, msal, datasets, DataProperty, tabledata, ocifs, msal-extensions, evaluate, azure-datalake-store, pytablewriter, gcsfs, azure-identity, lm-eval, adlfs\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.1.2\n",
            "    Uninstalling tenacity-9.1.2:\n",
            "      Successfully uninstalled tenacity-9.1.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: gcsfs\n",
            "    Found existing installation: gcsfs 2025.3.2\n",
            "    Uninstalling gcsfs-2025.3.2:\n",
            "      Successfully uninstalled gcsfs-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.6.0.dev20241112+cu121 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.6.0.dev20241112+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed DataProperty-1.1.0 adlfs-2024.12.0 aiobotocore-2.22.0 aioitertools-0.12.0 antlr4-python3-runtime-4.9.3 azure-core-1.34.0 azure-datalake-store-0.0.53 azure-identity-1.22.0 azure-storage-blob-12.25.1 bcrypt-4.3.0 blobfile-3.0.0 botocore-1.37.3 circuitbreaker-2.1.3 colorama-0.4.6 datasets-3.6.0 datatrove-0.5.0 dill-0.3.8 dropbox-12.0.2 dropboxdrivefs-1.4.1 evaluate-0.4.3 fsspec-2025.3.0 fusepy-3.0.1 gcsfs-2025.3.0 isodate-0.7.2 jmespath-1.0.1 jsonlines-4.0.0 libarchive-c-5.2 lm-eval-0.4.8 lockfile-0.12.2 loguru-0.7.3 luigi-3.6.0 mbstrdecoder-1.1.4 msal-1.32.3 msal-extensions-1.3.1 msgspec-0.19.0 multiprocess-0.70.16 objprint-0.3.0 oci-2.151.0 ocifs-1.3.2 omegaconf-2.3.0 paramiko-3.5.1 pathvalidate-3.2.3 portalocker-3.1.1 pybind11-2.13.6 pycryptodomex-3.22.0 pynacl-1.5.0 pyspnego-0.11.2 pytablewriter-1.2.1 python-daemon-3.1.2 rouge-score-0.1.2 s3fs-2025.3.0 sacrebleu-2.5.1 smbprotocol-1.15.0 sqlitedict-2.1.0 stone-3.3.1 submitit-1.5.2 tabledata-1.3.4 tcolorpy-0.1.7 tenacity-8.5.0 tiktoken-0.9.0 tqdm-multiprocess-0.0.11 typepy-1.3.4 viztracer-1.0.3 word2number-1.1 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "7a5971122b8e4d5a8f1a022e4e8caaac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nBGKU15ZpLbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m bytelatent.hf load-transformers --entropy-repo facebook/blt-entropy --blt-repo facebook/blt-1b hub --prompt \"My test prompt\""
      ],
      "metadata": {
        "id": "TmK_PkcnqFWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "!python -m bytelatent.hf load-transformers --entropy-repo facebook/blt-entropy --blt-repo facebook/blt-1b hub --prompt \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "488y1eTJqybT",
        "outputId": "74f33f60-56cc-46c9-d307-ddd5df0e97af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mhf.py\u001b[0m:\u001b[94m155\u001b[0m in \u001b[92mload_transformers\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(blt_model)                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m153 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(tok_and_patcher)                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m154 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m source == \u001b[33m\"\u001b[0m\u001b[33mhub\u001b[0m\u001b[33m\"\u001b[0m:                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m155 \u001b[2m│   │   \u001b[0mentropy_model = \u001b[1;4mLMTransformer.from_pretrained(entropy_repo)\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   \u001b[0mblt_model = ByteLatentTransformer.from_pretrained(blt_repo)    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[2m│   │   \u001b[0mtok_and_patcher = BltTokenizerAndPatcher.from_pretrained(blt_r \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m158 \u001b[0m\u001b[2m│   │   \u001b[0mtokenizer = tok_and_patcher.tokenizer_args.build()             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m─╮\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      blt_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     blt_repo = \u001b[33m'facebook/blt-1b'\u001b[0m      \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  entropy_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m entropy_repo = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       prompt = \u001b[33m'hi'\u001b[0m                   \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       source = \u001b[33m'hub'\u001b[0m                  \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────╯\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m :\u001b[94m114\u001b[0m in \u001b[92m_inner_fn\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m check_use_auth_token:                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[91m__na\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mfn(*args, **kwargs)\u001b[0m                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _inner_fn  \u001b[2m# type: ignore\u001b[0m                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             arg_name = \u001b[33m'pretrained_model_name_or_path'\u001b[0m                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            arg_value = \u001b[33m'facebook/blt-entropy'\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 args = \u001b[1m(\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33m'bytelatent.transformer.LMTransformer'\u001b[0m\u001b[1m>\u001b[0m,          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'facebook/blt-entropy'\u001b[0m                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[1m)\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m check_use_auth_token = \u001b[94mTrue\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            has_token = \u001b[94mFalse\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            signature = \u001b[1m<\u001b[0m\u001b[1;95mSignature\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mcls: Type\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39m~T\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpretrained_model_name_or_path: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpathlib.Path\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, *, force_download: bool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mresume_download: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, proxies: \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, token: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, bool, \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mNoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, cache_dir: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, \u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpathlib.Path, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, local_files_only:\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mbool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, revision: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39m**model_kwargs\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m -> ~T\u001b[0m\u001b[1m>\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mhub_mixin.py\u001b[0m:\u001b[94m566\u001b[0m in  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m563 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mcls\u001b[0m._hub_mixin_inject_config \u001b[95mand\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m model_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m564 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodel_kwargs[\u001b[33m\"\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m\"\u001b[0m] = config                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m565 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m566 \u001b[2m│   │   \u001b[0minstance = \u001b[96mcls\u001b[0m._from_pretrained(                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m567 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_id=\u001b[96mstr\u001b[0m(model_id),                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m568 \u001b[0m\u001b[2m│   │   │   \u001b[0mrevision=revision,                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m569 \u001b[0m\u001b[2m│   │   │   \u001b[0mcache_dir=cache_dir,                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     cache_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   config_file = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                force_download = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_files_only = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      model_id = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  model_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                     \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       proxies = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               resume_download = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      revision = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         token = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰────────────────────────────────────────────────────────╯\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mhub_mixin.py\u001b[0m:\u001b[94m789\u001b[0m in  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_from_pretrained\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m786 \u001b[0m\u001b[2m│   │   \u001b[0m**model_kwargs,                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m787 \u001b[0m\u001b[2m│   \u001b[0m):                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m788 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Load Pytorch pretrained weights and return the loaded model\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m789 \u001b[2m│   │   \u001b[0mmodel = \u001b[1;4;96mcls\u001b[0m\u001b[1;4m(**model_kwargs)\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m790 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m os.path.isdir(model_id):                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m791 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mLoading weights from local directory\u001b[0m\u001b[33m\"\u001b[0m)              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m792 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_file = os.path.join(model_id, constants.SAFETENSORS_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m─────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        cache_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   force_download = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m local_files_only = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     map_location = \u001b[33m'cpu'\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         model_id = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     model_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                     \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          proxies = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  resume_download = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         revision = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           strict = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            token = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────────╯\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mLMTransformer.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m missing \u001b[1;36m1\u001b[0m required positional argument: \n",
            "\u001b[32m'args'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aM1LVyqUrTtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bO8ccwkSrTqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bytelatent.transformer import LMTransformer\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.hf import BltTokenizerAndPatcher\n",
        "!huggingface-cli login --token CCCCCCCC\n",
        "entropy_repo = \"facebook/blt-entropy\"\n",
        "blt_repo = \"facebook/blt-1b\"\n",
        "entropy_model = LMTransformer.from_pretrained(entropy_repo)\n",
        "blt_model = ByteLatentTransformer.from_pretrained(blt_repo)\n",
        "tok_and_patcher = BltTokenizerAndPatcher.from_pretrained(blt_repo)\n",
        "tokenizer = tok_and_patcher.tokenizer_args.build()\n",
        "patcher = tok_and_patcher.patcher_args.build()"
      ],
      "metadata": {
        "id": "UVsymLJRrTnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "a8a55521-0c24-414c-ef55-177b8bf1561a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LMTransformer.__init__() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f2c47acbb3bf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mentropy_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"facebook/blt-entropy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mblt_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"facebook/blt-1b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mentropy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLMTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mblt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mByteLatentTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblt_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtok_and_patcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBltTokenizerAndPatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblt_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         instance = cls._from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ):\n\u001b[1;32m    788\u001b[0m         \u001b[0;34m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from local directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LMTransformer.__init__() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/blt/setup/download_prepare_hf_data.py"
      ],
      "metadata": {
        "id": "53F-KOxtxfok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "!python download_blt_weights.py\n",
        "!python demo.py \"A BLT has\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdLPHUznrZuL",
        "outputId": "66a365a6-7b0a-49d2-8ddc-e8e70a2b84be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "Fetching 9 files: 100% 9/9 [00:00<00:00, 2339.99it/s]\n",
            "Loading BLT model: blt-1b\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mmain\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0msetup_torch_distributed(distributed_args)                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mcheckpoint_path = os.path.join(\u001b[33m\"\u001b[0m\u001b[33mhf-weights\u001b[0m\u001b[33m\"\u001b[0m, model_name)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading BLT model: \u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m20 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenizer\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m \u001b[96misinstance\u001b[0m(model, ByteLatentTransformer)                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m distributed_args = \u001b[1;35mDistributedArgs\u001b[0m\u001b[1m(\u001b[0m                                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_shard\u001b[0m=\u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_replicate\u001b[0m=\u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mtp_size\u001b[0m=\u001b[94m1\u001b[0m,                                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mselective_activation_checkpointing\u001b[0m=\u001b[94mFalse\u001b[0m,    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile\u001b[0m=\u001b[94mFalse\u001b[0m,                               \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfsdp_type\u001b[0m=\u001b[33m'no_shard'\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmodel_dtype\u001b[0m=\u001b[33m'bf16'\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_recipe\u001b[0m=\u001b[94mNone\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_filter\u001b[0m=\u001b[33m'layers\\\\.\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33m0-9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m+\\\\.'\u001b[0m,          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmatmul_allow_tf32\u001b[0m=\u001b[94mFalse\u001b[0m,                     \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mallow_bf16_reduced_precision_reduction\u001b[0m=\u001b[94mTrue\u001b[0m, \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdetect_anomaly\u001b[0m=\u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile_cache_size_limit\u001b[0m=\u001b[94m8\u001b[0m,                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mspawn_method\u001b[0m=\u001b[33m'forkserver'\u001b[0m                    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       model_name = \u001b[33m'blt-1b'\u001b[0m                                         \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           prompt = \u001b[33m'A BLT has'\u001b[0m                                      \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m403\u001b[0m in                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m400 \u001b[0m\u001b[2m│   │   │   \u001b[0msetup_torch_distributed(distributed_args)                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m401 \u001b[0m\u001b[2m│   \u001b[0mtrain_args_path = os.path.join(consolidated_path, \u001b[33m\"\u001b[0m\u001b[33mparams.json\u001b[0m\u001b[33m\"\u001b[0m)   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m402 \u001b[0m\u001b[2m│   \u001b[0mfs = get_fs(train_args_path)                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m403 \u001b[2m│   \u001b[0mtrain_args = TrainArgs.model_validate_json(\u001b[1;4mfs.read_text(train_args\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m404 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m405 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m train_args.train_entropy_model:                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m406 \u001b[0m\u001b[2m│   │   \u001b[0mmodel_args = train_args.entropy_model                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m consolidated_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                fs = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[39mat \u001b[0m\u001b[94m0x7897e3a77590\u001b[0m\u001b[1m>\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   train_args_path = \u001b[33m'hf-weights/blt-1b/params.json'\u001b[0m                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/\u001b[0m\u001b[1;33mspec.py\u001b[0m:\u001b[94m721\u001b[0m in \u001b[92mread_text\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 718 \u001b[0m\u001b[2;33m│   │   │   \u001b[0m\u001b[33mURL of file on this filesystems\u001b[0m                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 719 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mencoding, errors, newline: same as `open`.\u001b[0m                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 720 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 721 \u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.open(                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 722 \u001b[0m\u001b[2m│   │   │   \u001b[0mpath,                                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 723 \u001b[0m\u001b[2m│   │   │   \u001b[0mmode=\u001b[33m\"\u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\u001b[0m,                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 724 \u001b[0m\u001b[2m│   │   │   \u001b[0mencoding=encoding,                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m encoding = \u001b[94mNone\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   errors = \u001b[94mNone\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  newline = \u001b[94mNone\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     path = \u001b[33m'hf-weights/blt-1b/params.json'\u001b[0m                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            \u001b[94m0x7897e3a77590\u001b[0m\u001b[1m>\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/\u001b[0m\u001b[1;33mspec.py\u001b[0m:\u001b[94m1298\u001b[0m in \u001b[92mopen\u001b[0m          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1295 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m k \u001b[95min\u001b[0m kwargs                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1296 \u001b[0m\u001b[2m│   │   │   \u001b[0m}                                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1297 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m io.TextIOWrapper(                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1298 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.open(                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1299 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mpath,                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1300 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mmode,                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1301 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mblock_size=block_size,                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    block_size = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m cache_options = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   compression = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            io = \u001b[1m<\u001b[0m\u001b[1;95mmodule\u001b[0m\u001b[39m \u001b[0m\u001b[33m'io'\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfrozen\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1m>\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          mode = \u001b[33m'rb'\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[94m0x7897e3a77590\u001b[0m\u001b[1m>\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   text_kwargs = \u001b[1m{\u001b[0m\u001b[33m'encoding'\u001b[0m: \u001b[94mNone\u001b[0m, \u001b[33m'errors'\u001b[0m: \u001b[94mNone\u001b[0m, \u001b[33m'newline'\u001b[0m: \u001b[94mNone\u001b[0m\u001b[1m}\u001b[0m      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/\u001b[0m\u001b[1;33mspec.py\u001b[0m:\u001b[94m1310\u001b[0m in \u001b[92mopen\u001b[0m          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1307 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1308 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1309 \u001b[0m\u001b[2m│   │   │   \u001b[0mac = kwargs.pop(\u001b[33m\"\u001b[0m\u001b[33mautocommit\u001b[0m\u001b[33m\"\u001b[0m, \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._intrans)          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1310 \u001b[2m│   │   │   \u001b[0mf = \u001b[96mself\u001b[0m._open(                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1311 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpath,                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1312 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmode=mode,                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1313 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mblock_size=block_size,                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ac = \u001b[94mTrue\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    block_size = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m cache_options = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   compression = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            io = \u001b[1m<\u001b[0m\u001b[1;95mmodule\u001b[0m\u001b[39m \u001b[0m\u001b[33m'io'\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfrozen\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1m>\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          mode = \u001b[33m'rb'\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[94m0x7897e3a77590\u001b[0m\u001b[1m>\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/\u001b[0m\u001b[1;33mlocal.py\u001b[0m:\u001b[94m201\u001b[0m  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92m_open\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0mpath = \u001b[96mself\u001b[0m._strip_protocol(path)                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.auto_mkdir \u001b[95mand\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mw\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m mode:                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.makedirs(\u001b[96mself\u001b[0m._parent(path), exist_ok=\u001b[94mTrue\u001b[0m)           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m201 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mLocalFileOpener(path, mode, fs=\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m, **kwargs)\u001b[0m          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mtouch\u001b[0m(\u001b[96mself\u001b[0m, path, truncate=\u001b[94mTrue\u001b[0m, **kwargs):                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   \u001b[0mpath = \u001b[96mself\u001b[0m._strip_protocol(path)                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m block_size = \u001b[94mNone\u001b[0m                                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     kwargs = \u001b[1m{\u001b[0m\u001b[33m'autocommit'\u001b[0m: \u001b[94mTrue\u001b[0m, \u001b[33m'cache_options'\u001b[0m: \u001b[94mNone\u001b[0m\u001b[1m}\u001b[0m                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       mode = \u001b[33m'rb'\u001b[0m                                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              \u001b[94m0x7897e3a77590\u001b[0m\u001b[1m>\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/\u001b[0m\u001b[1;33mlocal.py\u001b[0m:\u001b[94m365\u001b[0m  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92m__init__\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m362 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.autocommit = autocommit                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m363 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.compression = get_compression(path, compression)          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m364 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.blocksize = io.DEFAULT_BUFFER_SIZE                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m365 \u001b[2m│   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m._open()\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m366 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m367 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_open\u001b[0m(\u001b[96mself\u001b[0m):                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m368 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.f \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.f.closed:                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  autocommit = \u001b[94mTrue\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m compression = \u001b[94mNone\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          fs = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               \u001b[94m0x7897e3a77590\u001b[0m\u001b[1m>\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      kwargs = \u001b[1m{\u001b[0m\u001b[33m'cache_options'\u001b[0m: \u001b[94mNone\u001b[0m\u001b[1m}\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        mode = \u001b[33m'rb'\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileOpener\u001b[0m\u001b[39m object at \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               \u001b[94m0x7897e3a79930\u001b[0m\u001b[1m>\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/\u001b[0m\u001b[1;33mlocal.py\u001b[0m:\u001b[94m370\u001b[0m  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92m_open\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m367 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_open\u001b[0m(\u001b[96mself\u001b[0m):                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m368 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.f \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.f.closed:                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m369 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.autocommit \u001b[95mor\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mw\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.mode:                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m370 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.f = \u001b[1;4;96mopen\u001b[0m\u001b[1;4m(\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.path, mode=\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.mode)\u001b[0m               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m371 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.compression:                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m372 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcompress = compr[\u001b[96mself\u001b[0m.compression]                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m373 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.f = compress(\u001b[96mself\u001b[0m.f, mode=\u001b[96mself\u001b[0m.mode)          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileOpener\u001b[0m\u001b[39m object at \u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        \u001b[94m0x7897e3a79930\u001b[0m\u001b[1m>\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mFileNotFoundError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \n",
            "[rank0]: \u001b[32m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m\n",
            "[rank0]:[W508 21:21:40.197628249 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "!huggingface-cli login --token XXXXXXXXXXXXX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_oGnrzFtlhe",
        "outputId": "3394670b-1309-402f-8d32-ade3531bc47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, make sure you’re logged in:\n",
        "#!huggingface-cli login\n",
        "%cd /content/blt\n",
        "# Then run:\n",
        "!python -m bytelatent.hf load-transformers \\\n",
        "    --entropy-repo facebook/blt-entropy \\\n",
        "    --blt-repo     facebook/blt-1b \\\n",
        "    hub \\\n",
        "    --prompt      \"A BLT has\"\n"
      ],
      "metadata": {
        "id": "ofoFmVBHtl31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ec2882-6140-42a7-df51-438fa013818b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mhf.py\u001b[0m:\u001b[94m155\u001b[0m in \u001b[92mload_transformers\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(blt_model)                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m153 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(tok_and_patcher)                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m154 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m source == \u001b[33m\"\u001b[0m\u001b[33mhub\u001b[0m\u001b[33m\"\u001b[0m:                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m155 \u001b[2m│   │   \u001b[0mentropy_model = \u001b[1;4mLMTransformer.from_pretrained(entropy_repo)\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   \u001b[0mblt_model = ByteLatentTransformer.from_pretrained(blt_repo)    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[2m│   │   \u001b[0mtok_and_patcher = BltTokenizerAndPatcher.from_pretrained(blt_r \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m158 \u001b[0m\u001b[2m│   │   \u001b[0mtokenizer = tok_and_patcher.tokenizer_args.build()             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m─╮\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      blt_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     blt_repo = \u001b[33m'facebook/blt-1b'\u001b[0m      \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  entropy_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m entropy_repo = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       prompt = \u001b[33m'A BLT has'\u001b[0m            \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       source = \u001b[33m'hub'\u001b[0m                  \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────╯\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m :\u001b[94m114\u001b[0m in \u001b[92m_inner_fn\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m check_use_auth_token:                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[91m__na\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mfn(*args, **kwargs)\u001b[0m                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _inner_fn  \u001b[2m# type: ignore\u001b[0m                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             arg_name = \u001b[33m'pretrained_model_name_or_path'\u001b[0m                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            arg_value = \u001b[33m'facebook/blt-entropy'\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 args = \u001b[1m(\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33m'bytelatent.transformer.LMTransformer'\u001b[0m\u001b[1m>\u001b[0m,          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'facebook/blt-entropy'\u001b[0m                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[1m)\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m check_use_auth_token = \u001b[94mTrue\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            has_token = \u001b[94mFalse\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            signature = \u001b[1m<\u001b[0m\u001b[1;95mSignature\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mcls: Type\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39m~T\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpretrained_model_name_or_path: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpathlib.Path\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, *, force_download: bool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mresume_download: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, proxies: \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, token: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, bool, \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mNoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, cache_dir: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, \u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpathlib.Path, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, local_files_only:\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mbool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, revision: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39m**model_kwargs\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m -> ~T\u001b[0m\u001b[1m>\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mhub_mixin.py\u001b[0m:\u001b[94m566\u001b[0m in  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m563 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mcls\u001b[0m._hub_mixin_inject_config \u001b[95mand\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m model_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m564 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodel_kwargs[\u001b[33m\"\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m\"\u001b[0m] = config                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m565 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m566 \u001b[2m│   │   \u001b[0minstance = \u001b[96mcls\u001b[0m._from_pretrained(                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m567 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_id=\u001b[96mstr\u001b[0m(model_id),                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m568 \u001b[0m\u001b[2m│   │   │   \u001b[0mrevision=revision,                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m569 \u001b[0m\u001b[2m│   │   │   \u001b[0mcache_dir=cache_dir,                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     cache_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   config_file = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                force_download = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_files_only = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      model_id = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  model_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                     \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       proxies = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               resume_download = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      revision = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         token = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰────────────────────────────────────────────────────────╯\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mhub_mixin.py\u001b[0m:\u001b[94m789\u001b[0m in  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_from_pretrained\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m786 \u001b[0m\u001b[2m│   │   \u001b[0m**model_kwargs,                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m787 \u001b[0m\u001b[2m│   \u001b[0m):                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m788 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Load Pytorch pretrained weights and return the loaded model\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m789 \u001b[2m│   │   \u001b[0mmodel = \u001b[1;4;96mcls\u001b[0m\u001b[1;4m(**model_kwargs)\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m790 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m os.path.isdir(model_id):                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m791 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mLoading weights from local directory\u001b[0m\u001b[33m\"\u001b[0m)              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m792 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_file = os.path.join(model_id, constants.SAFETENSORS_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m─────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        cache_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   force_download = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m local_files_only = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     map_location = \u001b[33m'cpu'\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         model_id = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     model_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                     \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          proxies = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  resume_download = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         revision = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           strict = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            token = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────────╯\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mLMTransformer.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m missing \u001b[1;36m1\u001b[0m required positional argument: \n",
            "\u001b[32m'args'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bytelatent.transformer import LMTransformer\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.hf import BltTokenizerAndPatcher\n",
        "\n",
        "entropy_repo = \"facebook/blt-entropy\"\n",
        "blt_repo     = \"facebook/blt-1b\"\n",
        "\n",
        "# 1. Load the little language model (“entropy model”)\n",
        "entropy_model = LMTransformer.from_pretrained(entropy_repo)\n",
        "\n",
        "# 2. Load the byte-latent transformer\n",
        "blt_model     = ByteLatentTransformer.from_pretrained(blt_repo)\n",
        "\n",
        "# 3. Load & patch the tokenizer\n",
        "tok_and_patcher = BltTokenizerAndPatcher.from_pretrained(blt_repo)\n",
        "tokenizer       = tok_and_patcher.tokenizer_args.build()\n",
        "patcher         = tok_and_patcher.patcher_args.build()\n",
        "\n",
        "# 4. Run a simple generation\n",
        "inputs = tokenizer(\"A BLT has\", return_tensors=\"pt\")\n",
        "patched = patcher.apply_batch(inputs[\"input_ids\"])\n",
        "outputs = blt_model.generate(**patched)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "9kHNrF1EyMR1",
        "outputId": "1a292d11-7c9d-4e7b-8a76-36b8650e0b15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LMTransformer.__init__() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b0cd59bfa4d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 1. Load the little language model (“entropy model”)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mentropy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLMTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 2. Load the byte-latent transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         instance = cls._from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ):\n\u001b[1;32m    788\u001b[0m         \u001b[0;34m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from local directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LMTransformer.__init__() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This reads your downloaded weights + params.json,\n",
        "# writes out a `train_args.json` under output_dir/blt, and the entropy one as well.\n",
        "!python -m bytelatent.hf convert-to-transformers \\\n",
        "    hf-weights/blt-1b      output_dir\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4dwMF7Ry583",
        "outputId": "9af1101c-384f-4576-b80e-2a4221e8882c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mhf.py\u001b[0m:\u001b[94m95\u001b[0m in \u001b[92mconvert_to_transformers\u001b[0m                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 92 \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mconvert_to_transformers\u001b[0m(blt_weights_dir: \u001b[96mstr\u001b[0m, output_dir: \u001b[96mstr\u001b[0m):    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m os.path.exists(output_dir):                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   │   \u001b[0mos.makedirs(output_dir, exist_ok=\u001b[94mTrue\u001b[0m)                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 95 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenize\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 96 \u001b[0m\u001b[2m│   \u001b[0mblt_dir = os.path.join(output_dir, \u001b[33m\"\u001b[0m\u001b[33mblt\u001b[0m\u001b[33m\"\u001b[0m)                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   \u001b[0mentropy_dir = os.path.join(output_dir, \u001b[33m\"\u001b[0m\u001b[33mentropy\u001b[0m\u001b[33m\"\u001b[0m)                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   \u001b[0mmodel.save_pretrained(blt_dir, config={\u001b[33m\"\u001b[0m\u001b[33margs\u001b[0m\u001b[33m\"\u001b[0m: train_cfg.model.mod \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m─╮\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m blt_weights_dir = \u001b[33m'hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      output_dir = \u001b[33m'output_dir'\u001b[0m        \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────╯\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m403\u001b[0m in                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m400 \u001b[0m\u001b[2m│   │   │   \u001b[0msetup_torch_distributed(distributed_args)                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m401 \u001b[0m\u001b[2m│   \u001b[0mtrain_args_path = os.path.join(consolidated_path, \u001b[33m\"\u001b[0m\u001b[33mparams.json\u001b[0m\u001b[33m\"\u001b[0m)   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m402 \u001b[0m\u001b[2m│   \u001b[0mfs = get_fs(train_args_path)                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m403 \u001b[2m│   \u001b[0mtrain_args = TrainArgs.model_validate_json(\u001b[1;4mfs.read_text(train_args\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m404 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m405 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m train_args.train_entropy_model:                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m406 \u001b[0m\u001b[2m│   │   \u001b[0mmodel_args = train_args.entropy_model                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m consolidated_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                fs = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[39mat \u001b[0m\u001b[94m0x78710008b510\u001b[0m\u001b[1m>\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   train_args_path = \u001b[33m'hf-weights/blt-1b/params.json'\u001b[0m                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/\u001b[0m\u001b[1;33mspec.py\u001b[0m:\u001b[94m721\u001b[0m in \u001b[92mread_text\u001b[0m      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 718 \u001b[0m\u001b[2;33m│   │   │   \u001b[0m\u001b[33mURL of file on this filesystems\u001b[0m                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 719 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mencoding, errors, newline: same as `open`.\u001b[0m                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 720 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 721 \u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.open(                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 722 \u001b[0m\u001b[2m│   │   │   \u001b[0mpath,                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 723 \u001b[0m\u001b[2m│   │   │   \u001b[0mmode=\u001b[33m\"\u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\u001b[0m,                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 724 \u001b[0m\u001b[2m│   │   │   \u001b[0mencoding=encoding,                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m encoding = \u001b[94mNone\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   errors = \u001b[94mNone\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  newline = \u001b[94mNone\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     path = \u001b[33m'hf-weights/blt-1b/params.json'\u001b[0m                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            \u001b[94m0x78710008b510\u001b[0m\u001b[1m>\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/\u001b[0m\u001b[1;33mspec.py\u001b[0m:\u001b[94m1298\u001b[0m in \u001b[92mopen\u001b[0m          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1295 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m k \u001b[95min\u001b[0m kwargs                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1296 \u001b[0m\u001b[2m│   │   │   \u001b[0m}                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1297 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m io.TextIOWrapper(                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1298 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.open(                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1299 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mpath,                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1300 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mmode,                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1301 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mblock_size=block_size,                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    block_size = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m cache_options = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   compression = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            io = \u001b[1m<\u001b[0m\u001b[1;95mmodule\u001b[0m\u001b[39m \u001b[0m\u001b[33m'io'\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfrozen\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1m>\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          mode = \u001b[33m'rb'\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[94m0x78710008b510\u001b[0m\u001b[1m>\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   text_kwargs = \u001b[1m{\u001b[0m\u001b[33m'encoding'\u001b[0m: \u001b[94mNone\u001b[0m, \u001b[33m'errors'\u001b[0m: \u001b[94mNone\u001b[0m, \u001b[33m'newline'\u001b[0m: \u001b[94mNone\u001b[0m\u001b[1m}\u001b[0m      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/\u001b[0m\u001b[1;33mspec.py\u001b[0m:\u001b[94m1310\u001b[0m in \u001b[92mopen\u001b[0m          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1307 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1308 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1309 \u001b[0m\u001b[2m│   │   │   \u001b[0mac = kwargs.pop(\u001b[33m\"\u001b[0m\u001b[33mautocommit\u001b[0m\u001b[33m\"\u001b[0m, \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._intrans)          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1310 \u001b[2m│   │   │   \u001b[0mf = \u001b[96mself\u001b[0m._open(                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1311 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpath,                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1312 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmode=mode,                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1313 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mblock_size=block_size,                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ac = \u001b[94mTrue\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    block_size = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m cache_options = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   compression = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            io = \u001b[1m<\u001b[0m\u001b[1;95mmodule\u001b[0m\u001b[39m \u001b[0m\u001b[33m'io'\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfrozen\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1m>\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          mode = \u001b[33m'rb'\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[94m0x78710008b510\u001b[0m\u001b[1m>\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/\u001b[0m\u001b[1;33mlocal.py\u001b[0m:\u001b[94m201\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92m_open\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0mpath = \u001b[96mself\u001b[0m._strip_protocol(path)                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.auto_mkdir \u001b[95mand\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mw\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m mode:                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.makedirs(\u001b[96mself\u001b[0m._parent(path), exist_ok=\u001b[94mTrue\u001b[0m)           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m201 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mLocalFileOpener(path, mode, fs=\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m, **kwargs)\u001b[0m          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mtouch\u001b[0m(\u001b[96mself\u001b[0m, path, truncate=\u001b[94mTrue\u001b[0m, **kwargs):                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   \u001b[0mpath = \u001b[96mself\u001b[0m._strip_protocol(path)                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m block_size = \u001b[94mNone\u001b[0m                                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     kwargs = \u001b[1m{\u001b[0m\u001b[33m'autocommit'\u001b[0m: \u001b[94mTrue\u001b[0m, \u001b[33m'cache_options'\u001b[0m: \u001b[94mNone\u001b[0m\u001b[1m}\u001b[0m                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       mode = \u001b[33m'rb'\u001b[0m                                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              \u001b[94m0x78710008b510\u001b[0m\u001b[1m>\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/\u001b[0m\u001b[1;33mlocal.py\u001b[0m:\u001b[94m365\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92m__init__\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m362 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.autocommit = autocommit                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m363 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.compression = get_compression(path, compression)          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m364 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.blocksize = io.DEFAULT_BUFFER_SIZE                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m365 \u001b[2m│   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m._open()\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m366 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m367 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_open\u001b[0m(\u001b[96mself\u001b[0m):                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m368 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.f \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.f.closed:                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  autocommit = \u001b[94mTrue\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m compression = \u001b[94mNone\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          fs = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               \u001b[94m0x78710008b510\u001b[0m\u001b[1m>\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      kwargs = \u001b[1m{\u001b[0m\u001b[33m'cache_options'\u001b[0m: \u001b[94mNone\u001b[0m\u001b[1m}\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        mode = \u001b[33m'rb'\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileOpener\u001b[0m\u001b[39m object at \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               \u001b[94m0x787100091270\u001b[0m\u001b[1m>\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/\u001b[0m\u001b[1;33mlocal.py\u001b[0m:\u001b[94m370\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92m_open\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m367 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_open\u001b[0m(\u001b[96mself\u001b[0m):                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m368 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.f \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.f.closed:                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m369 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.autocommit \u001b[95mor\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mw\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.mode:                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m370 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.f = \u001b[1;4;96mopen\u001b[0m\u001b[1;4m(\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.path, mode=\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.mode)\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m371 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.compression:                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m372 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcompress = compr[\u001b[96mself\u001b[0m.compression]                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m373 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.f = compress(\u001b[96mself\u001b[0m.f, mode=\u001b[96mself\u001b[0m.mode)          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileOpener\u001b[0m\u001b[39m object at \u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        \u001b[94m0x787100091270\u001b[0m\u001b[1m>\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mFileNotFoundError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \n",
            "\u001b[32m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/facebook/blt-entropy hf-weights/blt-entropy\n",
        "!git clone https://huggingface.co/facebook/blt-1b       hf-weights/blt-1b\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMyapzwFzOs1",
        "outputId": "e9a04bbb-0ff8-4c11-a1ca-c2a3d3476334"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hf-weights/blt-entropy'...\n",
            "fatal: could not read Username for 'https://huggingface.co': No such device or address\n",
            "fatal: destination path 'hf-weights/blt-1b' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "from bytelatent.transformer import LMTransformer\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.hf import BltTokenizerAndPatcher\n",
        "\n",
        "# point at your cloned folder\n",
        "entropy_dir = \"hf-weights/blt-entropy\"\n",
        "blt_dir     = \"hf-weights/blt-1b\"\n",
        "\n",
        "entropy_model = LMTransformer.from_pretrained(\n",
        "    entropy_dir,\n",
        "    local_files_only=True\n",
        ")\n",
        "blt_model     = ByteLatentTransformer.from_pretrained(\n",
        "    blt_dir,\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "tokpatch = BltTokenizerAndPatcher.from_pretrained(blt_dir, local_files_only=True)\n",
        "tokenizer = tokpatch.tokenizer_args.build()\n",
        "patcher    = tokpatch.patcher_args.build()\n",
        "\n",
        "inputs  = tokenizer(\"A BLT has\", return_tensors=\"pt\")\n",
        "patched = patcher.apply_batch(inputs[\"input_ids\"])\n",
        "outputs = blt_model.generate(**patched)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "Qwsxjuoqzn2V",
        "outputId": "a9e4995b-5f8e-440c-99af-d1908d313c9c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LMTransformer.__init__() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-55559a1fc189>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mblt_dir\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;34m\"hf-weights/blt-1b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m entropy_model = LMTransformer.from_pretrained(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mentropy_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         instance = cls._from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ):\n\u001b[1;32m    788\u001b[0m         \u001b[0;34m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from local directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LMTransformer.__init__() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m bytelatent.hf convert-to-transformers \\\n",
        "    hf-weights/blt-1b   output/blt\n",
        "!python -m bytelatent.hf convert-to-transformers \\\n",
        "    hf-weights/blt-entropy output/entropy\n"
      ],
      "metadata": {
        "id": "vDS5UV8Ozs89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HUfBiwtDz3-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from bytelatent.transformer import LMTransformer\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.hf import BltTokenizerAndPatcher\n",
        "from bytelatent.args import ModelArgs  # Import ModelArgs\n",
        "\n",
        "# point at your cloned folder\n",
        "entropy_dir = \"hf-weights/blt-entropy\"\n",
        "blt_dir = \"hf-weights/blt-1b\"\n",
        "\n",
        "# Create ModelArgs instance\n",
        "entropy_args = ModelArgs()\n",
        "\n",
        "entropy_model = LMTransformer.from_pretrained(\n",
        "    entropy_dir,\n",
        "    local_files_only=True,\n",
        "    args=entropy_args  # Pass args to from_pretrained\n",
        ")\n",
        "blt_model = ByteLatentTransformer.from_pretrained(\n",
        "    blt_dir,\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "tokpatch = BltTokenizerAndPatcher.from_pretrained(blt_dir, local_files_only=True)\n",
        "tokenizer = tokpatch.tokenizer_args.build()\n",
        "patcher = tokpatch.patcher_args.build()\n",
        "\n",
        "inputs = tokenizer(\"A BLT has\", return_tensors=\"pt\")\n",
        "patched = patcher.apply_batch(inputs[\"input_ids\"])\n",
        "outputs = blt_model.generate(**patched)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "dH_-IW_Sz6MF",
        "outputId": "bc19e6ff-7628-4a58-af36-b2be283e1da5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bytelatent'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-426cac94935c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbytelatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLMTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbytelatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mByteLatentTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbytelatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBltTokenizerAndPatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbytelatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelArgs\u001b[0m  \u001b[0;31m# Import ModelArgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bytelatent'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "source": [
        "!mkdir -p output/blt output/entropy # Create output directories\n",
        "!python -m bytelatent.hf convert-to-transformers hf-weights/blt-1b output/blt\n",
        "!python -m bytelatent.hf convert-to-transformers hf-weights/blt-entropy output/entropy"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "oW4cdP4X0CUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!mv /content/blt/hf-weights/blt-1b/entropy_model/params.json /content/blt/hf-weights/blt-entropy/"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2f2UodKZ0Zl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!python -m bytelatent.hf convert-to-transformers \\\n",
        "    --params-file /content/blt/hf-weights/blt-1b/entropy_model/params.json \\  # إضافة مسار الملف\n",
        "    hf-weights/blt-1b output/blt\n",
        "!python -m bytelatent.hf convert-to-transformers \\\n",
        "    --params-file /content/blt/hf-weights/blt-1b/entropy_model/params.json \\  # إضافة مسار الملف\n",
        "    hf-weights/blt-entropy output/entropy"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "NTOgIP010eQ2",
        "outputId": "059a8ff9-1b99-4430-a1af-9076bd55493c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-9-7d8b37129493>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-7d8b37129493>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    hf-weights/blt-1b output/blt\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!python -m bytelatent.hf convert-to-transformers \\\n",
        "    hf-weights/blt-1b output/blt\n",
        "!python -m bytelatent.hf convert-to-transformers \\\n",
        "    hf-weights/blt-entropy output/entropy"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD8U8_Iq0kbu",
        "outputId": "dc869c52-68ce-4a96-a28e-7b0050b161b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: Error while finding module specification for 'bytelatent.hf' (ModuleNotFoundError: No module named 'bytelatent')\n",
            "/usr/bin/python3: Error while finding module specification for 'bytelatent.hf' (ModuleNotFoundError: No module named 'bytelatent')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup/download_tokenizer.py llama3 /content/blt/hf-weights/blt-1b --api_key CCCCCCCC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVe-l_Gt1Gc2",
        "outputId": "4cb7243c-447c-4388-97b3-7c49a4f9d24a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:933: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n",
            "tokenizer.model: 100% 2.18M/2.18M [00:00<00:00, 20.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo.py \"A BLT has\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW4U90bh1Ou2",
        "outputId": "c46b3d39-dbca-4308-f985-4fe72d60f809"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BLT model: blt-1b\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mmain\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0msetup_torch_distributed(distributed_args)                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mcheckpoint_path = os.path.join(\u001b[33m\"\u001b[0m\u001b[33mhf-weights\u001b[0m\u001b[33m\"\u001b[0m, model_name)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading BLT model: \u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m20 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenizer\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m \u001b[96misinstance\u001b[0m(model, ByteLatentTransformer)                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m distributed_args = \u001b[1;35mDistributedArgs\u001b[0m\u001b[1m(\u001b[0m                                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_shard\u001b[0m=\u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_replicate\u001b[0m=\u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mtp_size\u001b[0m=\u001b[94m1\u001b[0m,                                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mselective_activation_checkpointing\u001b[0m=\u001b[94mFalse\u001b[0m,    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile\u001b[0m=\u001b[94mFalse\u001b[0m,                               \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfsdp_type\u001b[0m=\u001b[33m'no_shard'\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmodel_dtype\u001b[0m=\u001b[33m'bf16'\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_recipe\u001b[0m=\u001b[94mNone\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_filter\u001b[0m=\u001b[33m'layers\\\\.\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33m0-9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m+\\\\.'\u001b[0m,          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmatmul_allow_tf32\u001b[0m=\u001b[94mFalse\u001b[0m,                     \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mallow_bf16_reduced_precision_reduction\u001b[0m=\u001b[94mTrue\u001b[0m, \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdetect_anomaly\u001b[0m=\u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile_cache_size_limit\u001b[0m=\u001b[94m8\u001b[0m,                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mspawn_method\u001b[0m=\u001b[33m'forkserver'\u001b[0m                    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       model_name = \u001b[33m'blt-1b'\u001b[0m                                         \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           prompt = \u001b[33m'A BLT has'\u001b[0m                                      \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m403\u001b[0m in                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m400 \u001b[0m\u001b[2m│   │   │   \u001b[0msetup_torch_distributed(distributed_args)                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m401 \u001b[0m\u001b[2m│   \u001b[0mtrain_args_path = os.path.join(consolidated_path, \u001b[33m\"\u001b[0m\u001b[33mparams.json\u001b[0m\u001b[33m\"\u001b[0m)   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m402 \u001b[0m\u001b[2m│   \u001b[0mfs = get_fs(train_args_path)                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m403 \u001b[2m│   \u001b[0mtrain_args = TrainArgs.model_validate_json(\u001b[1;4mfs.read_text(train_args\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m404 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m405 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m train_args.train_entropy_model:                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m406 \u001b[0m\u001b[2m│   │   \u001b[0mmodel_args = train_args.entropy_model                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m consolidated_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                fs = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[39mat \u001b[0m\u001b[94m0x10d19ab41510\u001b[0m\u001b[1m>\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   train_args_path = \u001b[33m'hf-weights/blt-1b/params.json'\u001b[0m                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/\u001b[0m\u001b[1;33mspec.py\u001b[0m:\u001b[94m721\u001b[0m in \u001b[92mread_text\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 718 \u001b[0m\u001b[2;33m│   │   │   \u001b[0m\u001b[33mURL of file on this filesystems\u001b[0m                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 719 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mencoding, errors, newline: same as `open`.\u001b[0m                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 720 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 721 \u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.open(                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 722 \u001b[0m\u001b[2m│   │   │   \u001b[0mpath,                                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 723 \u001b[0m\u001b[2m│   │   │   \u001b[0mmode=\u001b[33m\"\u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\u001b[0m,                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 724 \u001b[0m\u001b[2m│   │   │   \u001b[0mencoding=encoding,                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m encoding = \u001b[94mNone\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   errors = \u001b[94mNone\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  newline = \u001b[94mNone\u001b[0m                                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     path = \u001b[33m'hf-weights/blt-1b/params.json'\u001b[0m                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            \u001b[94m0x10d19ab41510\u001b[0m\u001b[1m>\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/\u001b[0m\u001b[1;33mspec.py\u001b[0m:\u001b[94m1298\u001b[0m in \u001b[92mopen\u001b[0m          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1295 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m k \u001b[95min\u001b[0m kwargs                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1296 \u001b[0m\u001b[2m│   │   │   \u001b[0m}                                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1297 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m io.TextIOWrapper(                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1298 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.open(                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1299 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mpath,                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1300 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mmode,                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1301 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mblock_size=block_size,                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    block_size = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m cache_options = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   compression = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            io = \u001b[1m<\u001b[0m\u001b[1;95mmodule\u001b[0m\u001b[39m \u001b[0m\u001b[33m'io'\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfrozen\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1m>\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          mode = \u001b[33m'rb'\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[94m0x10d19ab41510\u001b[0m\u001b[1m>\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   text_kwargs = \u001b[1m{\u001b[0m\u001b[33m'encoding'\u001b[0m: \u001b[94mNone\u001b[0m, \u001b[33m'errors'\u001b[0m: \u001b[94mNone\u001b[0m, \u001b[33m'newline'\u001b[0m: \u001b[94mNone\u001b[0m\u001b[1m}\u001b[0m      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/\u001b[0m\u001b[1;33mspec.py\u001b[0m:\u001b[94m1310\u001b[0m in \u001b[92mopen\u001b[0m          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1307 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1308 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1309 \u001b[0m\u001b[2m│   │   │   \u001b[0mac = kwargs.pop(\u001b[33m\"\u001b[0m\u001b[33mautocommit\u001b[0m\u001b[33m\"\u001b[0m, \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._intrans)          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1310 \u001b[2m│   │   │   \u001b[0mf = \u001b[96mself\u001b[0m._open(                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1311 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpath,                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1312 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmode=mode,                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1313 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mblock_size=block_size,                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ac = \u001b[94mTrue\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    block_size = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m cache_options = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   compression = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            io = \u001b[1m<\u001b[0m\u001b[1;95mmodule\u001b[0m\u001b[39m \u001b[0m\u001b[33m'io'\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfrozen\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1m>\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          mode = \u001b[33m'rb'\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[94m0x10d19ab41510\u001b[0m\u001b[1m>\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/\u001b[0m\u001b[1;33mlocal.py\u001b[0m:\u001b[94m201\u001b[0m  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92m_open\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0mpath = \u001b[96mself\u001b[0m._strip_protocol(path)                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.auto_mkdir \u001b[95mand\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mw\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m mode:                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.makedirs(\u001b[96mself\u001b[0m._parent(path), exist_ok=\u001b[94mTrue\u001b[0m)           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m201 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mLocalFileOpener(path, mode, fs=\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m, **kwargs)\u001b[0m          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mtouch\u001b[0m(\u001b[96mself\u001b[0m, path, truncate=\u001b[94mTrue\u001b[0m, **kwargs):                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   \u001b[0mpath = \u001b[96mself\u001b[0m._strip_protocol(path)                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m block_size = \u001b[94mNone\u001b[0m                                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     kwargs = \u001b[1m{\u001b[0m\u001b[33m'autocommit'\u001b[0m: \u001b[94mTrue\u001b[0m, \u001b[33m'cache_options'\u001b[0m: \u001b[94mNone\u001b[0m\u001b[1m}\u001b[0m                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       mode = \u001b[33m'rb'\u001b[0m                                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              \u001b[94m0x10d19ab41510\u001b[0m\u001b[1m>\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/\u001b[0m\u001b[1;33mlocal.py\u001b[0m:\u001b[94m365\u001b[0m  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92m__init__\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m362 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.autocommit = autocommit                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m363 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.compression = get_compression(path, compression)          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m364 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.blocksize = io.DEFAULT_BUFFER_SIZE                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m365 \u001b[2m│   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m._open()\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m366 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m367 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_open\u001b[0m(\u001b[96mself\u001b[0m):                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m368 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.f \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.f.closed:                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  autocommit = \u001b[94mTrue\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m compression = \u001b[94mNone\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          fs = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileSystem\u001b[0m\u001b[39m object at \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               \u001b[94m0x10d19ab41510\u001b[0m\u001b[1m>\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      kwargs = \u001b[1m{\u001b[0m\u001b[33m'cache_options'\u001b[0m: \u001b[94mNone\u001b[0m\u001b[1m}\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        mode = \u001b[33m'rb'\u001b[0m                                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        path = \u001b[33m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileOpener\u001b[0m\u001b[39m object at \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               \u001b[94m0x10d1d5e3d900\u001b[0m\u001b[1m>\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/\u001b[0m\u001b[1;33mlocal.py\u001b[0m:\u001b[94m370\u001b[0m  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92m_open\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m367 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_open\u001b[0m(\u001b[96mself\u001b[0m):                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m368 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.f \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.f.closed:                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m369 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.autocommit \u001b[95mor\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mw\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.mode:                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m370 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.f = \u001b[1;4;96mopen\u001b[0m\u001b[1;4m(\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.path, mode=\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.mode)\u001b[0m               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m371 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.compression:                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m372 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcompress = compr[\u001b[96mself\u001b[0m.compression]                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m373 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.f = compress(\u001b[96mself\u001b[0m.f, mode=\u001b[96mself\u001b[0m.mode)          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m self = \u001b[1m<\u001b[0m\u001b[1;95mfsspec.implementations.local.LocalFileOpener\u001b[0m\u001b[39m object at \u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        \u001b[94m0x10d1d5e3d900\u001b[0m\u001b[1m>\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mFileNotFoundError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \n",
            "[rank0]: \u001b[32m'/content/blt/hf-weights/blt-1b/params.json'\u001b[0m\n",
            "[rank0]:[W508 21:38:56.438464016 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python -m bytelatent.hf load-transformers \\\n",
        "    --entropy-repo facebook/blt-entropy \\\n",
        "    --blt-repo     facebook/blt-1b \\\n",
        "    hub \\\n",
        "    --prompt      \"My test prompt\"\n"
      ],
      "metadata": {
        "id": "MwqDjmdx1TPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "!python -m bytelatent.hf load-transformers \\\n",
        "    --entropy-repo facebook/blt-entropy \\\n",
        "    --blt-repo     facebook/blt-1b \\\n",
        "    hub \\\n",
        "    --prompt      \"My test prompt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syTVbS2z2dMu",
        "outputId": "49f0adb4-16ff-422c-f193-cf167ec025e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mhf.py\u001b[0m:\u001b[94m155\u001b[0m in \u001b[92mload_transformers\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(blt_model)                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m153 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(tok_and_patcher)                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m154 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m source == \u001b[33m\"\u001b[0m\u001b[33mhub\u001b[0m\u001b[33m\"\u001b[0m:                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m155 \u001b[2m│   │   \u001b[0mentropy_model = \u001b[1;4mLMTransformer.from_pretrained(entropy_repo)\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   \u001b[0mblt_model = ByteLatentTransformer.from_pretrained(blt_repo)    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[2m│   │   \u001b[0mtok_and_patcher = BltTokenizerAndPatcher.from_pretrained(blt_r \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m158 \u001b[0m\u001b[2m│   │   \u001b[0mtokenizer = tok_and_patcher.tokenizer_args.build()             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m─╮\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      blt_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     blt_repo = \u001b[33m'facebook/blt-1b'\u001b[0m      \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  entropy_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m entropy_repo = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       prompt = \u001b[33m'My test prompt'\u001b[0m       \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       source = \u001b[33m'hub'\u001b[0m                  \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────╯\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m :\u001b[94m114\u001b[0m in \u001b[92m_inner_fn\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m check_use_auth_token:                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[91m__na\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mfn(*args, **kwargs)\u001b[0m                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _inner_fn  \u001b[2m# type: ignore\u001b[0m                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             arg_name = \u001b[33m'pretrained_model_name_or_path'\u001b[0m                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            arg_value = \u001b[33m'facebook/blt-entropy'\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 args = \u001b[1m(\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33m'bytelatent.transformer.LMTransformer'\u001b[0m\u001b[1m>\u001b[0m,          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'facebook/blt-entropy'\u001b[0m                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[1m)\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m check_use_auth_token = \u001b[94mTrue\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            has_token = \u001b[94mFalse\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            signature = \u001b[1m<\u001b[0m\u001b[1;95mSignature\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mcls: Type\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39m~T\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpretrained_model_name_or_path: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpathlib.Path\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, *, force_download: bool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mresume_download: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, proxies: \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, token: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, bool, \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mNoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, cache_dir: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, \u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpathlib.Path, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, local_files_only:\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mbool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, revision: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39m**model_kwargs\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m -> ~T\u001b[0m\u001b[1m>\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mhub_mixin.py\u001b[0m:\u001b[94m566\u001b[0m in  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m563 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mcls\u001b[0m._hub_mixin_inject_config \u001b[95mand\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m model_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m564 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodel_kwargs[\u001b[33m\"\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m\"\u001b[0m] = config                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m565 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m566 \u001b[2m│   │   \u001b[0minstance = \u001b[96mcls\u001b[0m._from_pretrained(                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m567 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_id=\u001b[96mstr\u001b[0m(model_id),                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m568 \u001b[0m\u001b[2m│   │   │   \u001b[0mrevision=revision,                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m569 \u001b[0m\u001b[2m│   │   │   \u001b[0mcache_dir=cache_dir,                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     cache_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   config_file = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                force_download = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_files_only = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      model_id = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  model_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                     \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       proxies = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               resume_download = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      revision = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         token = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰────────────────────────────────────────────────────────╯\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mhub_mixin.py\u001b[0m:\u001b[94m789\u001b[0m in  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_from_pretrained\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m786 \u001b[0m\u001b[2m│   │   \u001b[0m**model_kwargs,                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m787 \u001b[0m\u001b[2m│   \u001b[0m):                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m788 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Load Pytorch pretrained weights and return the loaded model\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m789 \u001b[2m│   │   \u001b[0mmodel = \u001b[1;4;96mcls\u001b[0m\u001b[1;4m(**model_kwargs)\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m790 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m os.path.isdir(model_id):                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m791 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mLoading weights from local directory\u001b[0m\u001b[33m\"\u001b[0m)              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m792 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_file = os.path.join(model_id, constants.SAFETENSORS_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m─────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        cache_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   force_download = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m local_files_only = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     map_location = \u001b[33m'cpu'\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         model_id = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     model_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                     \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          proxies = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  resume_download = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         revision = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           strict = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            token = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────────╯\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mLMTransformer.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m missing \u001b[1;36m1\u001b[0m required positional argument: \n",
            "\u001b[32m'args'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bytelatent.transformer import LMTransformer\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.hf import BltTokenizerAndPatcher\n",
        "\n",
        "entropy_repo = \"facebook/blt-entropy\"\n",
        "blt_repo     = \"facebook/blt-1b\"\n",
        "\n",
        "# تحميل نموذج اللغة الصغير\n",
        "entropy_model = LMTransformer.from_pretrained(entropy_repo)\n",
        "\n",
        "# تحميل نموذج BLT\n",
        "blt_model     = ByteLatentTransformer.from_pretrained(blt_repo)\n",
        "\n",
        "# تحميل ومعالجة الـ tokenizer\n",
        "tok_and_patcher = BltTokenizerAndPatcher.from_pretrained(blt_repo)\n",
        "tokenizer       = tok_and_patcher.tokenizer_args.build()\n",
        "patcher         = tok_and_patcher.patcher_args.build()\n",
        "\n",
        "# توليد نص\n",
        "inputs  = tokenizer(\"required positional argument\", return_tensors=\"pt\")\n",
        "patched = patcher.apply_batch(inputs[\"input_ids\"])\n",
        "outputs = blt_model.generate(**patched)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "fKLfnysU2hgu",
        "outputId": "59a7861e-d40a-45f8-d406-ddf344b749f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LMTransformer.__init__() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1f885f0c8142>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# تحميل نموذج اللغة الصغير\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mentropy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLMTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# تحميل نموذج BLT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         instance = cls._from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ):\n\u001b[1;32m    788\u001b[0m         \u001b[0;34m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from local directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LMTransformer.__init__() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# إنشاء مجلّد جديد للـ entropy داخل hf-weights\n",
        "!mkdir -p /content/blt/hf-weights/blt-entropy\n",
        "\n",
        "# نقل نسخة من train_args.json إليه\n",
        "!cp /content/blt/hf-weights/blt-1b/train_args.json /content/blt/hf-weights/blt-entropy/\n",
        "# ثم تأكد من أن أوزان نموذج entropy (pytorch_model.bin) موجودة هناك أيضًا\n"
      ],
      "metadata": {
        "id": "YitcNgk23XWH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bytelatent.transformer import LMTransformer\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.hf import BltTokenizerAndPatcher\n",
        "\n",
        "entropy_dir = \"/content/blt/hf-weights/blt-entropy\"\n",
        "blt_dir     = \"/content/blt/hf-weights/blt-1b\"\n",
        "\n",
        "# تحميل نموذج الـ entropy مع تمرير TrainArgs من JSON\n",
        "entropy_model = LMTransformer.from_pretrained(\n",
        "    entropy_dir,\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "# تحميل نموذج BLT byte-latent transformer\n",
        "blt_model = ByteLatentTransformer.from_pretrained(\n",
        "    blt_dir,\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "# تحميل ومعالجة الـ tokenizer\n",
        "tokpatch = BltTokenizerAndPatcher.from_pretrained(\n",
        "    blt_dir,\n",
        "    local_files_only=True\n",
        ")\n",
        "tokenizer = tokpatch.tokenizer_args.build()\n",
        "patcher    = tokpatch.patcher_args.build()\n",
        "\n",
        "# توليد نص تجريبي\n",
        "inputs  = tokenizer(\"هذا اختبار\", return_tensors=\"pt\")\n",
        "patched = patcher.apply_batch(inputs[\"input_ids\"])\n",
        "outputs = blt_model.generate(**patched)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "C4RihcCo3beW",
        "outputId": "a538af8b-179c-4731-92b5-782e3eb99078"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "config.json not found in /content/blt/hf-weights/blt-entropy\n",
            "WARNING:huggingface_hub.hub_mixin:config.json not found in /content/blt/hf-weights/blt-entropy\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LMTransformer.__init__() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1990b2751497>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# تحميل نموذج الـ entropy مع تمرير TrainArgs من JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m entropy_model = LMTransformer.from_pretrained(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mentropy_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         instance = cls._from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ):\n\u001b[1;32m    788\u001b[0m         \u001b[0;34m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from local directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LMTransformer.__init__() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/blt/hf-weights/blt-1b/params.json \\\n",
        "   /content/blt/hf-weights/blt-1b/train_args.json\n",
        "!mv /content/blt/hf-weights/blt-1b/entropy_model/params.json \\\n",
        "   /content/blt/hf-weights/blt-entropy/train_args.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF6w8HCP3jKu",
        "outputId": "3954b2fa-8557-4185-c966-b46142ae7508"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/blt/hf-weights/blt-1b/params.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/blt/hf-weights/blt-1b/config.json /content/blt/hf-weights/blt-1b/entropy_model"
      ],
      "metadata": {
        "id": "qt7jSJm84Ene"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp"
      ],
      "metadata": {
        "id": "ChiTiLLr4NQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bytelatent.transformer import LMTransformer\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.hf import BltTokenizerAndPatcher\n",
        "\n",
        "entropy_repo = \"facebook/blt-entropy\"\n",
        "blt_repo = \"facebook/blt-1b\"\n",
        "entropy_model = LMTransformer.from_pretrained(entropy_repo)\n",
        "blt_model = ByteLatentTransformer.from_pretrained(blt_repo)\n",
        "tok_and_patcher = BltTokenizerAndPatcher.from_pretrained(blt_repo)\n",
        "tokenizer = tok_and_patcher.tokenizer_args.build()\n",
        "patcher = tok_and_patcher.patcher_args.build()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Sj74LZBp4zpP",
        "outputId": "e11b5cac-0239-4604-e408-cc89dbd8362f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LMTransformer.__init__() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-074b9d338a11>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mentropy_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"facebook/blt-entropy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mblt_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"facebook/blt-1b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mentropy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLMTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mblt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mByteLatentTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblt_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtok_and_patcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBltTokenizerAndPatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblt_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         instance = cls._from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ):\n\u001b[1;32m    788\u001b[0m         \u001b[0;34m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0;31m#model = cls(**model_kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from local directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LMTransformer.__init__() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_prepare_hf_data.py\n",
        "\n",
        "\n",
        "\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "import fsspec\n",
        "import requests\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "\n",
        "def run_command(command):\n",
        "    print(f\"Running: {command}\")\n",
        "    subprocess.run(command, shell=True, check=True)\n",
        "\n",
        "\n",
        "def download_dataset(repo_id, local_dir, allow_patterns):\n",
        "    print(f\"Downloading dataset from {repo_id}...\")\n",
        "    max_retries = 5\n",
        "    retry_delay = 10  # seconds\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            snapshot_download(\n",
        "                repo_id,\n",
        "                repo_type=\"dataset\",\n",
        "                local_dir=local_dir,\n",
        "                allow_patterns=allow_patterns,\n",
        "                resume_download=True,\n",
        "                max_workers=16,  # Don't hesitate to increase this number to lower the download time\n",
        "            )\n",
        "            break\n",
        "        except requests.exceptions.ReadTimeout:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"Timeout occurred. Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                raise\n",
        "    print(f\"Dataset downloaded to {local_dir}\")\n",
        "\n",
        "\n",
        "def parquet_to_jsonl(\n",
        "    dataset, work_dir, src_dir, tgt_dir, ntasks=64, s3_profile: str | None = None\n",
        "):\n",
        "    from datatrove.executor import LocalPipelineExecutor\n",
        "    from datatrove.pipeline.readers import ParquetReader\n",
        "    from datatrove.pipeline.writers import JsonlWriter\n",
        "\n",
        "    if tgt_dir.startswith(\"s3//\"):\n",
        "        if s3_profile is None:\n",
        "            out_spec = tgt_dir\n",
        "        else:\n",
        "            out_spec = (tgt_dir, fsspec.filesystem(\"s3\", profile=s3_profile))\n",
        "    else:\n",
        "        out_spec = tgt_dir\n",
        "\n",
        "    pipeline_exec = LocalPipelineExecutor(\n",
        "        pipeline=[\n",
        "            ParquetReader(\n",
        "                src_dir,\n",
        "                file_progress=True,\n",
        "                doc_progress=True,\n",
        "                glob_pattern=\"**/*.parquet\",\n",
        "            ),\n",
        "            JsonlWriter(\n",
        "                out_spec,\n",
        "                output_filename=dataset + \".chunk.${rank}.jsonl\",\n",
        "                compression=None,\n",
        "            ),\n",
        "        ],\n",
        "        tasks=ntasks,\n",
        "        logging_dir=os.path.join(work_dir, \"datatrove\"),\n",
        "    )\n",
        "    pipeline_exec.run()\n",
        "\n",
        "\n",
        "def setup_terashuf(work_dir):\n",
        "    terashuf_dir = os.path.join(work_dir, \"terashuf\")\n",
        "    terashuf_executable = os.path.join(terashuf_dir, \"terashuf\")\n",
        "\n",
        "    if os.path.exists(terashuf_executable):\n",
        "        print(\"terashuf executable already exists. Skipping setup.\")\n",
        "        return terashuf_dir\n",
        "\n",
        "    print(\"Setting up terashuf...\")\n",
        "    run_command(f\"git clone https://github.com/alexandres/terashuf {terashuf_dir}\")\n",
        "    run_command(f\"make -C {terashuf_dir}\")\n",
        "    return terashuf_dir\n",
        "\n",
        "\n",
        "def main(dataset, memory, data_dir, seed=42, nchunks=32, s3_profile: str | None = None):\n",
        "    # Configuration\n",
        "    repo_id = {\n",
        "        \"fineweb_edu\": \"HuggingFaceFW/fineweb-edu\",\n",
        "        \"fineweb_edu_10bt\": \"HuggingFaceFW/fineweb-edu\",\n",
        "        \"dclm_baseline_1.0\": \"mlfoundations/dclm-baseline-1.0\",\n",
        "        \"dclm_baseline_1.0_10prct\": \"mlfoundations/dclm-baseline-1.0\",\n",
        "    }[dataset]\n",
        "    src_dir = f\"{data_dir}/{dataset}\"\n",
        "    out_dir = f\"{src_dir}_shuffled\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    work_dir = src_dir  # Directory of this Python file\n",
        "    prefix = f\"{dataset}.chunk.\"\n",
        "    orig_extension = {\n",
        "        \"fineweb_edu\": \".jsonl\",\n",
        "        \"fineweb_edu_10bt\": \".jsonl\",\n",
        "        \"dclm_baseline_1.0\": \".jsonl.zst\",\n",
        "        \"dclm_baseline_1.0_10prct\": \".jsonl.zst\",\n",
        "    }[dataset]\n",
        "    cat_command = {\n",
        "        \"fineweb_edu\": \"cat\",\n",
        "        \"fineweb_edu_10bt\": \"cat\",\n",
        "        \"dclm_baseline_1.0\": \"zstdcat\",\n",
        "        \"dclm_baseline_1.0_10prct\": \"zstdcat\",\n",
        "    }[dataset]\n",
        "    allow_patterns = {\n",
        "        \"fineweb_edu\": None,\n",
        "        \"fineweb_edu_10bt\": \"sample/10BT/*\",\n",
        "        \"dclm_baseline_1.0\": \"*.jsonl.zst\",\n",
        "        \"dclm_baseline_1.0_10prct\": \"global-shard_01_of_10/*.jsonl.zst\",\n",
        "    }[dataset]\n",
        "    suffix = \".jsonl\"\n",
        "    k_validation = 10000  # Number of lines to take from each chunk for validation\n",
        "\n",
        "    # Setup terashuf\n",
        "    terashuf_dir = setup_terashuf(work_dir)\n",
        "\n",
        "    # Download dataset\n",
        "    download_dataset(repo_id, src_dir, allow_patterns)\n",
        "\n",
        "    if \"fineweb\" in dataset:\n",
        "        parquet_to_jsonl(dataset, work_dir, src_dir, src_dir)\n",
        "\n",
        "    # Set up environment variables\n",
        "    os.environ[\"MEMORY\"] = f\"{memory}\"\n",
        "    os.environ[\"SEED\"] = f\"{seed}\"\n",
        "\n",
        "    # Run the original shuffling and splitting command\n",
        "    terashuf_executable = os.path.join(terashuf_dir, \"terashuf\")\n",
        "    run_command(\n",
        "        f\"ulimit -n 100000 && \"\n",
        "        f\"find {src_dir} -type f -name '*{orig_extension}' -print0 | xargs -0 {cat_command} | {terashuf_executable} | \"\n",
        "        f\"split -n r/{nchunks} -d --suffix-length 2 --additional-suffix {suffix} - {out_dir}/{prefix}\"\n",
        "        \"; trap 'echo \\\"Caught signal 13, exiting with code 1\\\"; exit 1' SIGPIPE;\"\n",
        "    )\n",
        "\n",
        "    # Create validation set and remove lines from chunks\n",
        "    validation_file = f\"{out_dir}/{dataset}.val{suffix}\"\n",
        "    for i in range(nchunks):\n",
        "        chunk_file = f\"{out_dir}/{prefix}{i:02d}{suffix}\"\n",
        "        run_command(f\"head -n {k_validation} {chunk_file} >> {validation_file}\")\n",
        "        run_command(f\"sed -i '1,{k_validation}d' {chunk_file}\")\n",
        "\n",
        "    print(\"All tasks completed successfully!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"dataset\", type=str)\n",
        "    parser.add_argument(\"memory\", type=float, default=8)\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"data\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\"--nchunks\", type=int, default=1)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    main(args.dataset, args.memory, args.data_dir, args.seed, args.nchunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "T5DHhVPq5gVY",
        "outputId": "33c8aeea-c8fe-4eb9-cfa8-25b282b3c93b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'download_prepare_hf_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7892297e912a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdownload_prepare_hf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'download_prepare_hf_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PretrainedConfig\n",
        "\n",
        "# أنشئ config من dict\n",
        "cfg = PretrainedConfig.from_dict({\n",
        "    \"model_type\": \"bytelatent-lm\",\n",
        "    \"dim\": 512,\n",
        "    \"n_layers\": 8,\n",
        "    \"n_heads\": 12,\n",
        "    /* ... */\n",
        "})\n",
        "# ثم احفظه\n",
        "cfg.save_pretrained(\"/content/blt/hf-weights/blt-entropy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "MJGGR-Wi5g3X",
        "outputId": "f4c4eb30-b48b-4086-c2e4-9cc98b370274"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-19-c1d4bc30b9b4>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-c1d4bc30b9b4>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    /* ... */\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PretrainedConfig\n",
        "\n",
        "# أنشئ قاموس الإعدادات بدون تعليقات C-style\n",
        "cfg_dict = {\n",
        "    \"model_type\":   \"bytelatent-lm\",\n",
        "    \"dim\":          512,\n",
        "    \"n_layers\":     8,\n",
        "    \"n_heads\":      12,\n",
        "    # يمكنك إضافة باقي المفاتيح المطلوبة هنا:\n",
        "    # \"hidden_size\": 768,\n",
        "    # \"vocab_size\":  260,\n",
        "    # …\n",
        "}\n",
        "\n",
        "# قم بإنشاء الـ config من القاموس\n",
        "cfg = PretrainedConfig.from_dict(cfg_dict)\n",
        "\n",
        "# احفظ config.json في المسار المطلوب\n",
        "cfg.save_pretrained(\"/content/blt/hf-weights/blt-entropy\")\n"
      ],
      "metadata": {
        "id": "QJeSwmee6fEX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "from bytelatent.transformer import LMTransformer\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.hf import BltTokenizerAndPatcher\n",
        "\n",
        "entropy_dir = \"/content/blt/hf-weights/blt-entropy\"\n",
        "blt_dir     = \"/content/blt/hf-weights/blt-1b\"\n",
        "\n",
        "entropy_model = LMTransformer.from_pretrained(\n",
        "    entropy_dir,\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "blt_model = ByteLatentTransformer.from_pretrained(\n",
        "    blt_dir,\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "tokpatch  = BltTokenizerAndPatcher.from_pretrained(\n",
        "    blt_dir,\n",
        "    local_files_only=True\n",
        ")\n",
        "tokenizer = tokpatch.tokenizer_args.build()\n",
        "patcher   = tokpatch.patcher_args.build()\n",
        "\n",
        "inputs  = tokenizer(\"هذا اختبار\", return_tensors=\"pt\")\n",
        "patched = patcher.apply_batch(inputs[\"input_ids\"])\n",
        "outputs = blt_model.generate(**patched)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Ai9cK2vM6fav",
        "outputId": "1d42cc12-7e7c-4282-94ef-39122a668173"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LMTransformer.__init__() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e7c638b0c9fe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mblt_dir\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/blt/hf-weights/blt-1b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m entropy_model = LMTransformer.from_pretrained(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mentropy_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         instance = cls._from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ):\n\u001b[1;32m    788\u001b[0m         \u001b[0;34m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from local directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LMTransformer.__init__() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bytelatent.transformer import LMTransformer\n",
        "# المسار إلى مجلّد الـ entropy\n",
        "entropy_dir = \"/content/blt/hf-weights/blt-entropy\"\n",
        "\n",
        "#  تحميل النموذج مع الاعتماد على الملفات المحلية فقط\n",
        "entropy_model = LMTransformer.from_pretrained(\n",
        "    entropy_dir,\n",
        "    local_files_only=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "o9BmYMEz6qMe",
        "outputId": "21f74d51-1636-4081-ae6e-d8060c91bf94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LMTransformer.__init__() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-eac3408a68fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#  تحميل النموذج مع الاعتماد على الملفات المحلية فقط\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m entropy_model = LMTransformer.from_pretrained(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mentropy_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         instance = cls._from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ):\n\u001b[1;32m    788\u001b[0m         \u001b[0;34m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from local directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LMTransformer.__init__() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LMTransformerArgs(BaseTransformerArgs):\n",
        "    seed: int = 42\n",
        "    vocab_size: int = -1\n",
        "    weight_tying: bool = False\n",
        "    sliding_window: int | None = None\n",
        "\n",
        "class LMTransformer(BaseTransformer, PyTorchModelHubMixin, ...):\n",
        "    def __init__(self, *, args: LMTransformerArgs):\n",
        "        super().__init__(args)\n",
        "        ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "XJN88YeW65mP",
        "outputId": "3c772af2-f16b-4c66-da31-72a73e55c78f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BaseTransformerArgs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0545cbcd68cd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLMTransformerArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseTransformerArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvocab_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mweight_tying\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msliding_window\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BaseTransformerArgs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# تحميل كامل إعدادات التدريب\n",
        "train_args = json.load(open(\"hf-weights/blt-entropy/train_args.json\"))\n",
        "\n",
        "# استخرج معطيات الـ entropy model\n",
        "e = train_args[\"entropy_model\"]\n",
        "\n",
        "cfg = {\n",
        "  \"args\": {\n",
        "    \"seed\":               train_args[\"seed\"],\n",
        "    \"vocab_size\":         e[\"vocab_size\"],\n",
        "    \"weight_tying\":       False,\n",
        "    \"sliding_window\":     None,\n",
        "    \"dim\":                e[\"dim\"],\n",
        "    \"n_layers\":           e[\"n_layers\"],\n",
        "    \"n_heads\":            e[\"n_heads\"],\n",
        "    \"norm_eps\":           e[\"norm_eps\"],\n",
        "    \"attn_impl\":          e[\"attn_impl\"],\n",
        "    \"attn_bias_type\":     e[\"attn_bias_type\"],\n",
        "    \"eos_id\":             e[\"eos_id\"],\n",
        "  }\n",
        "}\n",
        "\n",
        "with open(\"hf-weights/blt-entropy/config.json\", \"w\") as f:\n",
        "    json.dump(cfg, f, indent=2)\n"
      ],
      "metadata": {
        "id": "fSgSBCCe7Ma3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bytelatent.transformer import LMTransformer\n",
        "\n",
        "entropy_model = LMTransformer.from_pretrained(\n",
        "    \"/content/blt/hf-weights/blt-1b/entropy_model\",\n",
        "    local_files_only=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iedsJKXZ7XBH",
        "outputId": "0209bd63-ec8e-4707-9d72-e5ccffddb183"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "82 validation errors for LMTransformerArgs\nalpha_depth\n  Extra inputs are not permitted [type=extra_forbidden, input_value='disabled', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\narchitecture\n  Extra inputs are not permitted [type=extra_forbidden, input_value='vanilla', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nattn_to_keep\n  Extra inputs are not permitted [type=extra_forbidden, input_value='all', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nconv_kernel_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_all_layers_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_all_layers_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_init_by_pooling\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_k\n  Extra inputs are not permitted [type=extra_forbidden, input_value=2, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_nheads\n  Extra inputs are not permitted [type=extra_forbidden, input_value=16, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_use_flex_attention\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_window_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_window_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncustom_bwd\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_global\n  Extra inputs are not permitted [type=extra_forbidden, input_value=2048, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_local_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1024, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_local_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1024, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_patch_emb\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_token\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_token_emb\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndownsampling_by_pooling\n  Extra inputs are not permitted [type=extra_forbidden, input_value='max', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndropout\n  Extra inputs are not permitted [type=extra_forbidden, input_value=0.0, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_enable_byte_group_hash\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_enable_byte_ngrams\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_hash_byte_group_nb_functions\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_hash_byte_group_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=[3, 4, 5, 6, 7, 8], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_hash_byte_group_vocab\n  Extra inputs are not permitted [type=extra_forbidden, input_value=500002, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_lm_loss\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_ngram_table_dir\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_ngram_to_size_str\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_preds_low_entropy_toks\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_preds_random_toks\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nentropy_model_checkpoint_dir\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nentropy_model_is_ngram_model\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nfull_logging_n_layers\n  Extra inputs are not permitted [type=extra_forbidden, input_value=4, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nfuse_sequence_parallel\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nglobal_local_decoder_residual_layer\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ninit_use_depth\n  Extra inputs are not permitted [type=extra_forbidden, input_value='current', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ninit_use_gaussian\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nlayer_ckpt\n  Extra inputs are not permitted [type=extra_forbidden, input_value='none', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nlocal_attention_window_len\n  Extra inputs are not permitted [type=extra_forbidden, input_value=512, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nlog_patch_lengths\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nloss_parallel\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nmax_encoder_seq_length\n  Extra inputs are not permitted [type=extra_forbidden, input_value=24576, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nmax_length\n  Extra inputs are not permitted [type=extra_forbidden, input_value=256, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nmax_patch_length\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nmonotonicity\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_heads_global\n  Extra inputs are not permitted [type=extra_forbidden, input_value=16, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_heads_local_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=16, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_heads_local_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=16, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_kv_heads_global\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_layers_global\n  Extra inputs are not permitted [type=extra_forbidden, input_value=25, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_layers_local_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=9, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_layers_local_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nngram_vocab_sizes\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nnon_linearity\n  Extra inputs are not permitted [type=extra_forbidden, input_value='swiglu', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nnorm_affine\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nnorm_type\n  Extra inputs are not permitted [type=extra_forbidden, input_value='rmsnorm', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\noutput_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=-1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npad_to_max_length\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatch_in_forward\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatch_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=4.5, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_batch_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_device\n  Extra inputs are not permitted [type=extra_forbidden, input_value='cuda', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_mode\n  Extra inputs are not permitted [type=extra_forbidden, input_value='entropy', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_threshold\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1.335442066192627, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_threshold_add\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_thresholds_str\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npm_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=0, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npre_norm\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nrecompute_attn\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nrecompute_fc1_out\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nrecompute_fc3_out\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nsequence_parallel\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nshare_encoder_decoder_emb\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ntie_local_encoder_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ntie_local_encoder_decoder_logits\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ntokenize_with_bpe_delimiter\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nuse_fsdp\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nuse_local_encoder_transformer\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nuse_rope\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ea234ba550f9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbytelatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLMTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m entropy_model = LMTransformer.from_pretrained(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"/content/blt/hf-weights/blt-1b/entropy_model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m                     \u001b[0mexpected_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hub_mixin_init_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mexpected_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                         \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;31m# Populate model_kwargs from config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_decode_arg\u001b[0;34m(cls, expected_type, value)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtype_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hub_mixin_coders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;31m# Otherwise => don't decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/blt/bytelatent/transformer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     74\u001b[0m         LMTransformerArgs: (\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mlambda\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLMTransformerArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     },\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 82 validation errors for LMTransformerArgs\nalpha_depth\n  Extra inputs are not permitted [type=extra_forbidden, input_value='disabled', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\narchitecture\n  Extra inputs are not permitted [type=extra_forbidden, input_value='vanilla', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nattn_to_keep\n  Extra inputs are not permitted [type=extra_forbidden, input_value='all', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nconv_kernel_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_all_layers_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_all_layers_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_init_by_pooling\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_k\n  Extra inputs are not permitted [type=extra_forbidden, input_value=2, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_nheads\n  Extra inputs are not permitted [type=extra_forbidden, input_value=16, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_use_flex_attention\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_window_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncross_attn_window_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ncustom_bwd\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_global\n  Extra inputs are not permitted [type=extra_forbidden, input_value=2048, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_local_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1024, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_local_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1024, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_patch_emb\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_token\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndim_token_emb\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndownsampling_by_pooling\n  Extra inputs are not permitted [type=extra_forbidden, input_value='max', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ndropout\n  Extra inputs are not permitted [type=extra_forbidden, input_value=0.0, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_enable_byte_group_hash\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_enable_byte_ngrams\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_hash_byte_group_nb_functions\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_hash_byte_group_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=[3, 4, 5, 6, 7, 8], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_hash_byte_group_vocab\n  Extra inputs are not permitted [type=extra_forbidden, input_value=500002, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_lm_loss\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_ngram_table_dir\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_ngram_to_size_str\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_preds_low_entropy_toks\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nencoder_preds_random_toks\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nentropy_model_checkpoint_dir\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nentropy_model_is_ngram_model\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nfull_logging_n_layers\n  Extra inputs are not permitted [type=extra_forbidden, input_value=4, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nfuse_sequence_parallel\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nglobal_local_decoder_residual_layer\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ninit_use_depth\n  Extra inputs are not permitted [type=extra_forbidden, input_value='current', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ninit_use_gaussian\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nlayer_ckpt\n  Extra inputs are not permitted [type=extra_forbidden, input_value='none', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nlocal_attention_window_len\n  Extra inputs are not permitted [type=extra_forbidden, input_value=512, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nlog_patch_lengths\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nloss_parallel\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nmax_encoder_seq_length\n  Extra inputs are not permitted [type=extra_forbidden, input_value=24576, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nmax_length\n  Extra inputs are not permitted [type=extra_forbidden, input_value=256, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nmax_patch_length\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nmonotonicity\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_heads_global\n  Extra inputs are not permitted [type=extra_forbidden, input_value=16, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_heads_local_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=16, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_heads_local_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=16, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_kv_heads_global\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_layers_global\n  Extra inputs are not permitted [type=extra_forbidden, input_value=25, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_layers_local_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=9, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nn_layers_local_encoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nngram_vocab_sizes\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nnon_linearity\n  Extra inputs are not permitted [type=extra_forbidden, input_value='swiglu', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nnorm_affine\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nnorm_type\n  Extra inputs are not permitted [type=extra_forbidden, input_value='rmsnorm', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\noutput_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=-1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npad_to_max_length\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatch_in_forward\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatch_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=4.5, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_batch_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_device\n  Extra inputs are not permitted [type=extra_forbidden, input_value='cuda', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_mode\n  Extra inputs are not permitted [type=extra_forbidden, input_value='entropy', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_threshold\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1.335442066192627, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_threshold_add\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npatching_thresholds_str\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npm_size\n  Extra inputs are not permitted [type=extra_forbidden, input_value=0, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npre_norm\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nrecompute_attn\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nrecompute_fc1_out\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nrecompute_fc3_out\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nsequence_parallel\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nshare_encoder_decoder_emb\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ntie_local_encoder_decoder\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ntie_local_encoder_decoder_logits\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\ntokenize_with_bpe_delimiter\n  Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nuse_fsdp\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nuse_local_encoder_transformer\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nuse_rope\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "import torch\n",
        "import transformers\n",
        "blt_model = ByteLatentTransformer.from_pretrained(\n",
        "    \"/content/blt/hf-weights/blt-1b\",\n",
        "    local_files_only=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb3upt-N7XR3",
        "outputId": "e187bee5-aeca-4cd6-c74c-35548fba3ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "!python -m bytelatent.hf load-transformers --entropy-repo facebook/blt-entropy --blt-repo facebook/blt-1b hub --prompt \"My test prompt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8a_qPSM7mWo",
        "outputId": "07670ea0-a4b2-4a05-87ef-0b11b736f762"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mhf.py\u001b[0m:\u001b[94m155\u001b[0m in \u001b[92mload_transformers\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(blt_model)                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m153 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(tok_and_patcher)                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m154 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m source == \u001b[33m\"\u001b[0m\u001b[33mhub\u001b[0m\u001b[33m\"\u001b[0m:                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m155 \u001b[2m│   │   \u001b[0mentropy_model = \u001b[1;4mLMTransformer.from_pretrained(entropy_repo)\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   \u001b[0mblt_model = ByteLatentTransformer.from_pretrained(blt_repo)    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[2m│   │   \u001b[0mtok_and_patcher = BltTokenizerAndPatcher.from_pretrained(blt_r \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m158 \u001b[0m\u001b[2m│   │   \u001b[0mtokenizer = tok_and_patcher.tokenizer_args.build()             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m─╮\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      blt_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     blt_repo = \u001b[33m'facebook/blt-1b'\u001b[0m      \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  entropy_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m entropy_repo = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       prompt = \u001b[33m'My test prompt'\u001b[0m       \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       source = \u001b[33m'hub'\u001b[0m                  \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────╯\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m :\u001b[94m114\u001b[0m in \u001b[92m_inner_fn\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m check_use_auth_token:                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[91m__na\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mfn(*args, **kwargs)\u001b[0m                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _inner_fn  \u001b[2m# type: ignore\u001b[0m                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             arg_name = \u001b[33m'pretrained_model_name_or_path'\u001b[0m                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            arg_value = \u001b[33m'facebook/blt-entropy'\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 args = \u001b[1m(\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33m'bytelatent.transformer.LMTransformer'\u001b[0m\u001b[1m>\u001b[0m,          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'facebook/blt-entropy'\u001b[0m                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[1m)\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m check_use_auth_token = \u001b[94mTrue\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            has_token = \u001b[94mFalse\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            signature = \u001b[1m<\u001b[0m\u001b[1;95mSignature\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mcls: Type\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39m~T\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpretrained_model_name_or_path: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpathlib.Path\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, *, force_download: bool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mresume_download: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, proxies: \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, token: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, bool, \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mNoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, cache_dir: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, \u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mpathlib.Path, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, local_files_only:\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mbool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, revision: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39m**model_kwargs\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m -> ~T\u001b[0m\u001b[1m>\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mhub_mixin.py\u001b[0m:\u001b[94m566\u001b[0m in  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m563 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mcls\u001b[0m._hub_mixin_inject_config \u001b[95mand\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m model_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m564 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodel_kwargs[\u001b[33m\"\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m\"\u001b[0m] = config                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m565 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m566 \u001b[2m│   │   \u001b[0minstance = \u001b[96mcls\u001b[0m._from_pretrained(                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m567 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_id=\u001b[96mstr\u001b[0m(model_id),                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m568 \u001b[0m\u001b[2m│   │   │   \u001b[0mrevision=revision,                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m569 \u001b[0m\u001b[2m│   │   │   \u001b[0mcache_dir=cache_dir,                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     cache_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   config_file = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                force_download = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_files_only = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      model_id = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  model_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                     \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       proxies = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               resume_download = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      revision = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         token = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰────────────────────────────────────────────────────────╯\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mhub_mixin.py\u001b[0m:\u001b[94m789\u001b[0m in  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_from_pretrained\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m786 \u001b[0m\u001b[2m│   │   \u001b[0m**model_kwargs,                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m787 \u001b[0m\u001b[2m│   \u001b[0m):                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m788 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Load Pytorch pretrained weights and return the loaded model\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m789 \u001b[2m│   │   \u001b[0mmodel = \u001b[1;4;96mcls\u001b[0m\u001b[1;4m(**model_kwargs)\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m790 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m os.path.isdir(model_id):                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m791 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mLoading weights from local directory\u001b[0m\u001b[33m\"\u001b[0m)              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m792 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_file = os.path.join(model_id, constants.SAFETENSORS_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m─────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        cache_dir = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   force_download = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m local_files_only = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     map_location = \u001b[33m'cpu'\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         model_id = \u001b[33m'facebook/blt-entropy'\u001b[0m \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     model_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                     \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          proxies = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  resume_download = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         revision = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           strict = \u001b[94mFalse\u001b[0m                  \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            token = \u001b[94mNone\u001b[0m                   \u001b[33m│\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────────╯\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mLMTransformer.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m missing \u001b[1;36m1\u001b[0m required positional argument: \n",
            "\u001b[32m'args'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bytelatent.transformer import LMTransformer\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.hf import BltTokenizerAndPatcher\n",
        "\n",
        "entropy_repo = \"facebook/blt-entropy\"\n",
        "blt_repo = \"facebook/blt-1b\"\n",
        "entropy_model = LMTransformer.from_pretrained(entropy_repo)\n",
        "blt_model = ByteLatentTransformer.from_pretrained(blt_repo)\n",
        "tok_and_patcher = BltTokenizerAndPatcher.from_pretrained(blt_repo)\n",
        "tokenizer = tok_and_patcher.tokenizer_args.build()\n",
        "patcher = tok_and_patcher.patcher_args.build()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "C8UoZnlR8Fj_",
        "outputId": "4457bf27-6c09-4652-8d19-8afd3c48a64c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LMTransformer.__init__() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-074b9d338a11>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mentropy_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"facebook/blt-entropy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mblt_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"facebook/blt-1b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mentropy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLMTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mblt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mByteLatentTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblt_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtok_and_patcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBltTokenizerAndPatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblt_repo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         instance = cls._from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ):\n\u001b[1;32m    788\u001b[0m         \u001b[0;34m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from local directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LMTransformer.__init__() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    },
    {
      "source": [
        "#!pip install transformers  # Install the \"transformers\" package\n",
        "%cd /content/blt\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "import torch\n",
        "#import transformer # No need to import \"transformer\" if you meant \"transformers\"\n",
        "from transformers import AutoModelForCausalLM # Import AutoModelForCausalLM for loading causal language models\n",
        "\n",
        "blt_model = ByteLatentTransformer.from_pretrained(\n",
        "    \"/content/blt/hf-weights/blt-1b\",\n",
        "    local_files_only=True,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "S9_46z3Q9Lbw",
        "outputId": "83a25c6d-c2db-4ddf-dd7d-c805f444de68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ByteLatentTransformer.__init__() got an unexpected keyword argument 'device_map'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c1dd20d4b18d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m \u001b[0;31m# Import AutoModelForCausalLM for loading causal language models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m blt_model = ByteLatentTransformer.from_pretrained(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"/content/blt/hf-weights/blt-1b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         instance = cls._from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ):\n\u001b[1;32m    788\u001b[0m         \u001b[0;34m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from local directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ByteLatentTransformer.__init__() got an unexpected keyword argument 'device_map'"
          ]
        }
      ]
    }
  ]
}